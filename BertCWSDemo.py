#!/anaconda3/envs/haiqin370/bin/ python3
# -*- coding: utf-8 -*-
"""
Created on at 09:28 2019-02-21 
@author: haiqinyang

Feature: 

Scenario: 
"""
import os

from src.config import args
from src.preprocess import CWS_BMEO # dataset_to_dataloader, randomly_mask_input, OntoNotesDataset
import torch
import time

from src.BERT.modeling import BertConfig
from src.customize_modeling import BertCRFCWS
from src.utilis import save_model

import logging
logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.INFO)
logger = logging.getLogger(__name__)

CONFIG_NAME = 'bert_config.json'
WEIGHTS_NAME = 'pytorch_model.bin'


def load_model(label_list, args):
    if args.visible_device is not None:
        if isinstance(args.visible_device, int):
            args.visible_device = str(args.visible_device)
        elif isinstance(args.visible_device, (tuple, list)):
            args.visible_device = ','.join([str(_) for _ in args.visible_device])
        os.environ["CUDA_VISIBLE_DEVICES"] = args.visible_device

    if args.local_rank == -1 or args.no_cuda:
        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
        n_gpu = torch.cuda.device_count()
    else:
        device = torch.device("cuda", args.local_rank)
        n_gpu = 1
        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
        torch.distributed.init_process_group(backend='nccl')
        if args.fp16:
            logger.info("16-bits training currently not supported in distributed training")
            args.fp16 = False # (see https://github.com/pytorch/pytorch/pull/13496)
    logger.info("device %s n_gpu %d distributed training %r", device, n_gpu, bool(args.local_rank != -1))

    if n_gpu > 0:
        torch.cuda.manual_seed_all(args.seed)

    if args.bert_model_dir is not None:
        config_file = os.path.join(args.bert_model_dir, CONFIG_NAME)
        bert_config = BertConfig.from_json_file(config_file)
    else:
        bert_config = BertConfig.from_json_file(args.bert_config_file)

    if args.num_hidden_layers>0 and args.num_hidden_layers<bert_config.num_hidden_layers:
        bert_config.num_hidden_layers = args.num_hidden_layers

    if args.max_seq_length > bert_config.max_position_embeddings:
        raise ValueError(
            "Cannot use sequence length {} because the BERT model was only trained up to sequence length {}".format(
            args.max_seq_length, bert_config.max_position_embeddings))

    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):
        if not args.override_output:
            raise ValueError("Output directory ({}) already exists and is not empty.".format(args.output_dir))
        else:
            os.system("rm %s" % os.path.join(args.output_dir, '*'))

    model = BertCRFCWS(device, bert_config, args.vocab_file, args.max_seq_length, len(label_list))

    if args.init_checkpoint is None:
        raise RuntimeError('Evaluating a random initialized model is not supported...!')
    #elif os.path.isdir(args.init_checkpoint):
    #    raise ValueError("init_checkpoint is not a file")
    else:
        weights_path = os.path.join(args.init_checkpoint, WEIGHTS_NAME)

        # main code copy from modeling.py line after 506
        state_dict = torch.load(weights_path)

        missing_keys = []
        unexpected_keys = []
        error_msgs = []
        # copy state_dict so _load_from_state_dict can modify it
        metadata = getattr(state_dict, '_metadata', None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata

        def load(module, prefix=''):
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            module._load_from_state_dict(
                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
            for name, child in module._modules.items():
                if child is not None:
                    load(child, prefix + name + '.')
        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')
        if len(missing_keys) > 0:
            logger.info("Weights of {} not initialized from pretrained model: {}".format(
                model.__class__.__name__, missing_keys))
        if len(unexpected_keys) > 0:
            logger.info("Weights from pretrained model not used in {}: {}".format(
                model.__class__.__name__, unexpected_keys))

    model.to(device)
    if args.local_rank != -1:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],
                                                          output_device=args.local_rank)
    elif n_gpu > 1 and not args.no_cuda:
        model = torch.nn.DataParallel(model)

    return model, device

def preload(args):
    processors = {
        "ontonotes_cws": lambda: CWS_BMEO(nopunc=args.nopunc),
    }

    task_name = args.task_name.lower()
    if task_name not in processors:
        raise ValueError("Task not found: %s" % (task_name))

    # Prepare model
    processor = processors[task_name]()

    label_list = processor.get_labels()
    model, device = load_model(label_list, args)

    if args.bert_model is not None:
        weights = torch.load(args.bert_model, map_location='cpu')

        try:
            model.load_state_dict(weights)
        except RuntimeError:
            model.module.load_state_dict(weights)

    model.eval()
    save_model(model, args.output_dir + 'model_eval.tsv')

    return model

def set_local_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/4ner_data/',
            'vocab_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/vocab.txt',
            'bert_config_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/bert_config.json',
            'output_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_23/rs/nhl3/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 128,
            'num_hidden_layers': 3,
            'init_checkpoint': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/',
            'bert_model': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_23/models/nhl3/weights_epoch03.pt',
            'override_output': True,
            'tensorboardWriter': False
            }

def set_server_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '../data/ontonotes5/4ner_data/',
            'vocab_file': '../models/bert-base-chinese/vocab.txt',
            'bert_config_file': '../models/bert-base-chinese/bert_config.json',
            'output_dir': './tmp_2019_3_22/out/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 128,
            'num_hidden_layers': 3,
            'init_checkpoint': '../models/bert-base-chinese/',
            'bert_model': './tmp_2019_3_23/ontonotes/nhl3_nte15_nbs64/weights_epoch03.pt',
            'no_cuda': True,
            'override_output': True,
            'tensorboardWriter': False
            }

LOCAL_FLAG = False
LOCAL_FLAG = True

if __name__=='__main__':
    if LOCAL_FLAG:
        kwargs = set_local_eval_param()
    else:
        kwargs = set_server_eval_param()

    args._parse(kwargs)
    model = preload(args)

    text0 = '''
        å•æªåŒ¹é©¬é€›è‹±å›½â€”â€”ä¼¦æ•¦ç¯‡ã€‚ä¼¦æ•¦å°±æ˜¯è¿™ä¸ªæ ·å­åˆæ¬¡æ¥åˆ°è¿™ä¸ªâ€œè€ç‰Œèµ„æœ¬ä¸»ä¹‰â€çš„â€œé›¾éƒ½â€œï¼Œ
        å°±åƒå›åˆ°äº†ä¸Šæµ·ï¼Œä¸€å¹¢å¹¢ä¸é«˜çš„å°æ¥¼ï¼Œæ˜¾å¾—å¼‚å¸¸é™ˆæ—§ï¼Œå¾ˆå¤šæ¥¼æˆ¿è¢«æ•°ç™¾å¹´çƒŸå°˜ç†çš„å°±åƒè¢«åˆ·äº†ä¸€å±‚é»‘è‰²çš„æ²¹æ¼†ï¼Œ
        æ²¹å…‰é”ƒäº®ï¼Œå¦‚æœä¸æ˜¯æ—è¾¹çš„æ¥¼æˆ¿æ­£åœ¨æ¸…æ´—ï¼Œå¾ˆéš¾è®©äººç›¸ä¿¡å¦‚ä»Šçš„ä¼¦æ•¦æ˜¯é¥±ç»æ±¡æŸ“æ²§æ¡‘ååŠæ—¶åˆ¹è½¦çš„é«˜æ‰‹ï¼Œ
        å› ä¸ºä¸€åº§ç°ä»£åŒ–çš„å›½é™…å¤§éƒ½å¸‚ä¹Ÿæ˜¯æœ‰ä¸å°‘æ¥¼æˆ¿æ˜¯é»‘è‰²çš„å‘¢ï¼Œé»‘è‰²æ˜¾å¾—å‡é‡ã€é«˜é›…ï¼Œä½†æ˜¯ç»å¯¹ä¸èƒ½é æ²¹çƒŸå»ç†â€¦â€¦å µè½¦ï¼Œ
        æ˜¯æ‰€æœ‰å¤§éƒ½å¸‚çš„é€šç—…ï¼Œè™½ç„¶ä¸è¶³ä¸ºæ€ªï¼Œä½†æ˜¯ï¼Œ1988å¹´çš„åŒ—äº¬è¿˜æ²¡æœ‰é‚£ä¹ˆå¤šçš„è½¦ï¼Œä¹Ÿæ²¡æœ‰å…¨åŸå¤§å µè½¦çš„ç°è±¡ï¼Œ
        æœ‰çš„æ˜¯åˆšåˆšå¼€å§‹çš„â€œé æ²¹çƒŸå’Œæ±½è½¦çš„å°¾æ°”çƒŸç†ç«ç‡ç¾ä¸½çš„å¤åŸâ€ï¼Œæœ‰è°èƒ½å¤Ÿæƒ³åˆ°ï¼ŒçŸ­çŸ­çš„åå¹´ï¼ŒåŒ—äº¬å°±æ°”å–˜ååçš„è¿½èµ¶ä¸Šäº†ä¼¦æ•¦ï¼Œ
        æ²¡æœ‰ä¸€æ¡æ´å‡€çš„æ²³æµï¼Œæ²¡æœ‰æ¸…æ–°çš„ç©ºæ°”ï¼Œæœ‰çš„æ˜¯è®©äººçª’æ¯çš„ç©ºæ°”æ±¡æŸ“â€¦â€¦.ä»¥åŠï¼Œè®©äººå§‹æ–™æœªåŠçš„å…¨åŸå¤§å µè½¦ã€‚
        å¦‚æœï¼Œæˆ‘ä»¬é‚£äº›è´Ÿè´£åŸå¸‚å»ºè®¾è§„åˆ’çš„å…ˆç”Ÿä»¬ï¼Œåœ¨å›½å¤–ï¼Œä¸åªåªæ˜¯æ¸¸å±±ç©æ°´çš„è¯ï¼Œå¸¦å›åˆ«äººçš„æ•™è®­ã€æ€»ç»“åˆ«äººçš„ç»éªŒçš„è¯ï¼Œ
        æˆ‘ä»¬è¿™ä¸ªè¢«ç©·ç¥–å…ˆæ¯çš„â€œä¸€å¡Œç³Šæ¶‚â€çš„è„†å¼±çš„ç”Ÿæ€ç¯å¢ƒä¹Ÿä¸ä¼šå†ç»å—20ä¸–çºª90å¹´ä»£çš„ç°ä»£åŒ–çš„å¤§æ±¡æŸ“äº†ã€‚ä½†æ˜¯ï¼Œ
        ä¼¦æ•¦æ˜¯ä¸€åº§æ”¹è¿‡è‡ªæ–°çš„åŸå¸‚ï¼Œäººå®¶ç—›å®šæ€ç—›ï¼Œç´§æ€¥åˆ¹è½¦ï¼ŒåŠæ—¶çš„æ²»ç†äº†æ±¡æŸ“ï¼Œæˆ‘ä»¬åœ¨æ³°å¾å£«æ²³é‡Œå¯ä»¥çœ‹åˆ°é±¼å„¿åœ¨è‡ªç”±çš„ç¿»æ»šï¼Œ
        å¤©ç©ºæ¹›è“ï¼Œç¿ ç»¿çš„è‰åœ°ä¸å…°å¤©è¾‰æ˜ ç€ï¼Œä¸€ç‰‡â€œæ±¡æŸ“å¤§æˆ˜â€åçš„å’Œå¹³æ™¯è±¡    
        '''
    t0 = time.time()
    outputT0 = model.cutlist_noUNK([text0])
    output0 = [' '.join(lst) for lst in outputT0]
    o0 = ''
    for x in output0: o0 += x + '\t'
    print(o0+'\n')
    print('Processing time: ' + str(time.time()-t0))

    '''
        å•æª åŒ¹é©¬ é€› è‹±å›½ â€”â€” ä¼¦æ•¦ ç¯‡ ã€‚ ä¼¦æ•¦ å°± æ˜¯ è¿™ä¸ª æ ·å­ åˆæ¬¡ æ¥åˆ° è¿™ä¸ª â€œ è€ç‰Œ èµ„æœ¬ä¸»ä¹‰ â€ çš„ â€œ é›¾éƒ½ â€œ ï¼Œ 
        å°± åƒ å›åˆ° äº† ä¸Šæµ· ï¼Œ ä¸€ å¹¢ å¹¢ ä¸ é«˜ çš„ å° æ¥¼ ï¼Œ æ˜¾å¾— å¼‚å¸¸ é™ˆæ—§ ï¼Œ å¾ˆå¤š æ¥¼æˆ¿ è¢« æ•°ç™¾ å¹´ çƒŸå°˜ç† çš„ 
        å°± åƒ è¢« åˆ· äº† ä¸€ å±‚ é»‘è‰² çš„ æ²¹æ¼† ï¼Œ æ²¹å…‰ é”ƒäº® ï¼Œ å¦‚æœ ä¸ æ˜¯ æ—è¾¹ çš„ æ¥¼æˆ¿ æ­£åœ¨ æ¸…æ´— ï¼Œ å¾ˆ éš¾ è®© äºº 
        ç›¸ä¿¡ å¦‚ä»Š çš„ ä¼¦æ•¦ æ˜¯ é¥±ç» æ±¡æŸ“ æ²§æ¡‘ å åŠæ—¶ åˆ¹è½¦ çš„ é«˜æ‰‹ ï¼Œ å› ä¸º ä¸€ åº§ ç°ä»£åŒ– çš„ å›½é™… å¤§éƒ½å¸‚ ä¹Ÿ æ˜¯ æœ‰ 
        ä¸å°‘ æ¥¼æˆ¿ æ˜¯ é»‘è‰² çš„ å‘¢ ï¼Œ é»‘è‰² æ˜¾å¾— å‡é‡ ã€ é«˜é›… ï¼Œ ä½†æ˜¯ ç»å¯¹ ä¸ èƒ½ é  æ²¹çƒŸ å» ç† â€¦â€¦ å µè½¦ ï¼Œ æ˜¯ æ‰€æœ‰ 
        å¤§éƒ½å¸‚ çš„ é€šç—… ï¼Œ è™½ç„¶ ä¸è¶³ä¸ºæ€ª ï¼Œ ä½†æ˜¯ ï¼Œ 1988å¹´ çš„ åŒ—äº¬ è¿˜ æ²¡æœ‰ é‚£ä¹ˆ å¤š çš„ è½¦ ï¼Œ ä¹Ÿ æ²¡æœ‰ å…¨ åŸ å¤§ 
        å µè½¦ çš„ ç°è±¡ ï¼Œ æœ‰çš„ æ˜¯ åˆšåˆš å¼€å§‹ çš„ â€œ é  æ²¹çƒŸ å’Œ æ±½è½¦ çš„ å°¾æ°” çƒŸç†ç«ç‡ ç¾ä¸½ çš„ å¤åŸ â€ ï¼Œ æœ‰ è° èƒ½å¤Ÿ æƒ³åˆ° ï¼Œ 
        çŸ­çŸ­ çš„ å å¹´ ï¼Œ åŒ—äº¬ å°± æ°”å–˜åå çš„ è¿½èµ¶ ä¸Š äº† ä¼¦æ•¦ ï¼Œ æ²¡æœ‰ ä¸€ æ¡ æ´å‡€ çš„ æ²³æµ ï¼Œ æ²¡æœ‰ æ¸…æ–° çš„ ç©ºæ°” ï¼Œ 
        æœ‰çš„ æ˜¯ è®© äºº çª’æ¯ çš„ ç©ºæ°” æ±¡ æŸ“ â€¦â€¦. ä»¥åŠ ï¼Œ è®© äºº å§‹æ–™ æœªåŠ çš„ å…¨ åŸ å¤§ å µè½¦ ã€‚ å¦‚æœ ï¼Œ æˆ‘ä»¬ é‚£äº› è´Ÿè´£ 
        åŸå¸‚ å»ºè®¾ è§„åˆ’ çš„ å…ˆç”Ÿä»¬ ï¼Œ åœ¨ å›½å¤– ï¼Œ ä¸ åª åª æ˜¯ æ¸¸å±±ç©æ°´ çš„è¯ ï¼Œ å¸¦å› åˆ«äºº çš„ æ•™è®­ ã€ æ€»ç»“ åˆ«äºº çš„ ç»éªŒ çš„è¯ ï¼Œ 
        æˆ‘ä»¬ è¿™ä¸ª è¢« ç©·ç¥–å…ˆ æ¯ çš„ â€œ ä¸€å¡Œç³Šæ¶‚ â€ çš„ è„†å¼± çš„ ç”Ÿæ€ ç¯å¢ƒ ä¹Ÿ ä¸ ä¼š å† ç»å— 20ä¸–çºª 90å¹´ä»£ çš„ ç°ä»£åŒ– çš„ å¤§ æ±¡æŸ“ äº† ã€‚ 
        ä½†æ˜¯ ï¼Œ ä¼¦æ•¦ æ˜¯ ä¸€ åº§ æ”¹ è¿‡ è‡ª æ–° çš„ åŸå¸‚ ï¼Œ äººå®¶ ç—›å®šæ€ç—› ï¼Œ ç´§æ€¥ åˆ¹è½¦ ï¼Œ åŠæ—¶ çš„ æ²»ç† äº† æ±¡æŸ“ ï¼Œ æˆ‘ä»¬ åœ¨ æ³°å¾å£«æ²³ é‡Œ å¯ä»¥ 
        çœ‹åˆ° é±¼å„¿ åœ¨ è‡ªç”± çš„ ç¿»æ»š ï¼Œ å¤©ç©º æ¹›è“ ï¼Œ ç¿ ç»¿ çš„ è‰åœ° ä¸ å…°å¤© è¾‰æ˜  ç€ ï¼Œ ä¸€ ç‰‡ â€œ æ±¡æŸ“ å¤§æˆ˜ â€ å çš„ å’Œå¹³ æ™¯è±¡	    
    '''

    text1 = '''
        å…°å¿ƒé¤å…\nä½œä¸ºä¸€ä¸ªæ— è¾£ä¸æ¬¢çš„å¦¹å­ï¼Œå¯¹ä¸Šæµ·èœçš„åæ¸…æ·¡åç”œçœŸçš„æ˜¯å„ç§åƒä¸æƒ¯ã€‚
        æ¯æ¬¡å‡ºé—¨å’Œé—ºèœœè¶Šé¥­å±€éƒ½æ˜¯é¿å¼€æœ¬å¸®èœã€‚åæ¥å¬å¾ˆå¤šæœ‹å‹è¯´ä¸Šæµ·æœ‰å‡ å®¶ç‰¹åˆ«æ­£å®—å‘³é“åš
        çš„å¾ˆå¥½çš„é¤å…äºæ˜¯è¿™å‘¨æœ«å’Œé—ºèœœä»¬å‡†å¤‡ä¸€èµ·å»å°ä¸€å°æ­£å®—çš„æœ¬å¸®èœã€‚\nè¿›è´¤è·¯æ˜¯æˆ‘åœ¨ä¸Š
        æµ·æ¯”è¾ƒå–œæ¬¢çš„ä¸€æ¡è¡—å•¦ï¼Œè¿™å®¶é¤å…å°±å¼€åœ¨è¿™æ¡è·¯ä¸Šã€‚å·²ç»å¼€äº†ä¸‰åå¤šå¹´çš„è€é¤å…äº†ï¼Œåœ°
        æ–¹å¾ˆå°ï¼Œå°±äº”å…­å¼ æ¡Œå­ã€‚ä½†æ˜¯ç¿»æ¡Œç‡æ¯”è¾ƒå¿«ã€‚äºŒæ¥¼ä¹‹å‰çš„å±…æ°‘é—´ä¹Ÿæ”¹æˆäº†é¤å…ï¼Œä½†æ˜¯åœ¨
        ä¸Šæµ·çš„åæ°”å´éå¸¸å¤§ã€‚çƒ§çš„å°±æ˜¯å®¶å¸¸èœï¼Œæ™®é€šåˆ°å’Œå®¶é‡Œçƒ§çš„ä¸€æ ·ï¼Œç”Ÿæ„éå¸¸å¥½ï¼Œå¤–é¢æ’
        é˜Ÿçš„æ¯”é‡Œé¢åƒçš„äººè¿˜è¦å¤šã€‚
    '''
    t0 = time.time()
    outputT1 = model.cutlist_noUNK([text1])
    output1 = [' '.join(lst) for lst in outputT1]
    o1 = ''
    for x in output1: o1 += x + '\t'
    print(text1)
    print(o1+'\n')
    print('Processing time: ' + str(time.time()-t0))
    '''
        å…°å¿ƒ é¤å… ä½œä¸º ä¸€ ä¸ª æ— è¾£ä¸æ¬¢ çš„ å¦¹å­ ï¼Œ å¯¹ ä¸Šæµ· èœ çš„ å æ¸…æ·¡ åç”œ çœŸçš„ æ˜¯ å„ ç§ åƒ ä¸æƒ¯ ã€‚ 
        æ¯æ¬¡ å‡ºé—¨ å’Œ é—ºèœœ è¶Š é¥­å±€ éƒ½ æ˜¯ é¿å¼€ æœ¬å¸® èœ ã€‚ åæ¥ å¬ å¾ˆå¤š æœ‹å‹ è¯´ ä¸Šæµ· æœ‰ å‡  å®¶ ç‰¹åˆ« æ­£å®— 
        å‘³é“ åš çš„ å¾ˆ å¥½ çš„ é¤å… äºæ˜¯ è¿™ å‘¨æœ« å’Œ é—ºèœœä»¬ å‡†å¤‡ ä¸€èµ· å» å°ä¸€å° æ­£å®— çš„ æœ¬å¸®èœ ã€‚ è¿›è´¤è·¯ 
        æ˜¯ æˆ‘ åœ¨ ä¸Šæµ· æ¯”è¾ƒ å–œæ¬¢ çš„ ä¸€ æ¡ è¡— å•¦ ï¼Œ è¿™ å®¶ é¤å… å°± å¼€ åœ¨ è¿™ æ¡ è·¯ ä¸Š ã€‚ å·²ç» å¼€ äº† ä¸‰åå¤š 
        å¹´ çš„ è€ é¤å… äº† ï¼Œ åœ°æ–¹ å¾ˆ å° ï¼Œ å°± äº”å…­ å¼  æ¡Œå­ ã€‚ ä½†æ˜¯ ç¿»æ¡Œç‡ æ¯”è¾ƒ å¿« ã€‚ äºŒ æ¥¼ ä¹‹å‰ çš„ å±…æ°‘é—´ 
        ä¹Ÿ æ”¹æˆ äº† é¤å… ï¼Œ ä½†æ˜¯ åœ¨ ä¸Šæµ· çš„ åæ°” å´ éå¸¸ å¤§ ã€‚ çƒ§ çš„ å°± æ˜¯ å®¶å¸¸èœ ï¼Œ æ™®é€š åˆ° å’Œ å®¶é‡Œ çƒ§ çš„ 
        ä¸€æ · ï¼Œ ç”Ÿæ„ éå¸¸ å¥½ ï¼Œ å¤–é¢ æ’é˜Ÿ çš„ æ¯” é‡Œé¢ åƒ çš„ äºº è¿˜ è¦ å¤š ã€‚	
    '''

    text2 = '''
        æ¬¾æ¬¾å¥½çœ‹çš„ç¾ç”²ï¼Œç®€ç›´èƒ½æç–¯â€œé€‰æ‹©ç»¼åˆç—‡â€è¯¶ï¼ã€‚è¿™æ˜¯ä¸€ç»„è¶…çº§æ¸©æŸ”åˆå¸¦ç‚¹è®¾è®¡æ„Ÿçš„ç¾ç”²ğŸ’…ã€‚
        æ˜¥å¤©æ¥äº†ğŸŒºã€‚ç¾ç”²ä¹Ÿä»æ·±è‰²ç³»è½¬å˜ä¸ºæ·¡æ·¡çš„æµ…è‰²ç³»äº†ğŸ’ã€‚ä»Šå¤©ç»™å¤§å®¶æ¨èæœ€é€‚åˆæ˜¥å¤©çš„ç¾ç”²ğŸ’…ã€‚
        å¸Œæœ›ä½ ä»¬ä¼šå–œæ¬¢~ğŸ˜@MTå°ç¾é…± @MTæƒ…æŠ¥å±€ @ç¾å›¾ç§€ç§€ #æ˜¥å­£ç¾ç”²##æ˜¾ç™½ç¾ç”²##æ¸…æ–°ç¾ç”²##insç¾ç”²#
        '''
    t0 = time.time()
    outputT2 = model.cutlist_noUNK([text2])
    output2 = [' '.join(lst) for lst in outputT2]
    o2 = ''
    for x in output2: o2 += x + '\t'
    print(text2)
    print(o2+'\n')
    print('Processing time: ' + str(time.time()-t0))
    '''
        æ¬¾æ¬¾ å¥½çœ‹ çš„ ç¾ç”² ï¼Œ ç®€ç›´ èƒ½ æç–¯ â€œ é€‰æ‹© ç»¼åˆç—‡ â€ è¯¶ ï¼ ã€‚ è¿™ æ˜¯ ä¸€ ç»„ è¶…çº§ æ¸©æŸ” åˆ å¸¦ ç‚¹ è®¾è®¡æ„Ÿ çš„ 
        ç¾ç”² ğŸ’… ã€‚ æ˜¥å¤© æ¥ äº† ğŸŒº ã€‚ ç¾ç”² ä¹Ÿ ä» æ·± è‰²ç³» è½¬å˜ ä¸º æ·¡æ·¡ çš„ æµ… è‰²ç³» äº† ğŸ’ ã€‚ ä»Šå¤© ç»™ å¤§å®¶ æ¨è æœ€ 
        é€‚åˆ æ˜¥å¤© çš„ ç¾ç”² ğŸ’… ã€‚ å¸Œæœ› ä½ ä»¬ ä¼š å–œæ¬¢ ~ ğŸ˜ @ MT å°ç¾é…± @ MT æƒ…æŠ¥å±€ @ ç¾å›¾ ç§€ç§€ # æ˜¥å­£ ç¾ç”² # # 
        æ˜¾ç™½ ç¾ç”² # # æ¸…æ–° ç¾ç”² # # ins ç¾ç”² #	
    '''


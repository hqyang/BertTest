#!/anaconda3/envs/haiqin370/bin/ python3
# -*- coding: utf-8 -*-
"""
Created on at 09:28 2019-02-21 
@author: haiqinyang

Feature: 

Scenario: 
"""
import sys
sys.path.append('./src')

import os

from src.config import args
from src.preprocess import CWS_BMEO # dataset_to_dataloader, randomly_mask_input, OntoNotesDataset
import torch
import time
from tqdm import tqdm

from src.BERT.modeling import BertConfig
from src.customize_modeling import BertCRFCWS
from src.utilis import save_model
import pdb

import logging
logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.INFO)
logger = logging.getLogger(__name__)

CONFIG_NAME = 'bert_config.json'
WEIGHTS_NAME = 'pytorch_model.bin'


def load_model(label_list, args):
    if args.visible_device is not None:
        if isinstance(args.visible_device, int):
            args.visible_device = str(args.visible_device)
        elif isinstance(args.visible_device, (tuple, list)):
            args.visible_device = ','.join([str(_) for _ in args.visible_device])
        os.environ["CUDA_VISIBLE_DEVICES"] = args.visible_device

    if args.local_rank == -1 or args.no_cuda:
        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
        n_gpu = torch.cuda.device_count()
    else:
        device = torch.device("cuda", args.local_rank)
        n_gpu = 1
        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
        torch.distributed.init_process_group(backend='nccl')
        if args.fp16:
            logger.info("16-bits training currently not supported in distributed training")
            args.fp16 = False # (see https://github.com/pytorch/pytorch/pull/13496)
    logger.info("device %s n_gpu %d distributed training %r", device, n_gpu, bool(args.local_rank != -1))

    if n_gpu > 0:
        torch.cuda.manual_seed_all(args.seed)

    if args.bert_model_dir is not None:
        config_file = os.path.join(args.bert_model_dir, CONFIG_NAME)
        bert_config = BertConfig.from_json_file(config_file)
    else:
        bert_config = BertConfig.from_json_file(args.bert_config_file)

    if args.num_hidden_layers>0 and args.num_hidden_layers<bert_config.num_hidden_layers:
        bert_config.num_hidden_layers = args.num_hidden_layers

    if args.max_seq_length > bert_config.max_position_embeddings:
        raise ValueError(
            "Cannot use sequence length {} because the BERT model was only trained up to sequence length {}".format(
            args.max_seq_length, bert_config.max_position_embeddings))

    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):
        if not args.override_output:
            raise ValueError("Output directory ({}) already exists and is not empty.".format(args.output_dir))
        else:
            os.system("rm %s" % os.path.join(args.output_dir, '*'))

    model = BertCRFCWS(device, bert_config, args.vocab_file, args.max_seq_length, len(label_list))

    if args.init_checkpoint is None:
        raise RuntimeError('Evaluating a random initialized model is not supported...!')
    #elif os.path.isdir(args.init_checkpoint):
    #    raise ValueError("init_checkpoint is not a file")
    else:
        weights_path = os.path.join(args.init_checkpoint, WEIGHTS_NAME)

        # main code copy from modeling.py line after 506
        state_dict = torch.load(weights_path)

        missing_keys = []
        unexpected_keys = []
        error_msgs = []
        # copy state_dict so _load_from_state_dict can modify it
        metadata = getattr(state_dict, '_metadata', None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata

        def load(module, prefix=''):
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            module._load_from_state_dict(
                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
            for name, child in module._modules.items():
                if child is not None:
                    load(child, prefix + name + '.')
        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')
        if len(missing_keys) > 0:
            logger.info("Weights of {} not initialized from pretrained model: {}".format(
                model.__class__.__name__, missing_keys))
        if len(unexpected_keys) > 0:
            logger.info("Weights from pretrained model not used in {}: {}".format(
                model.__class__.__name__, unexpected_keys))

    model.to(device)
    if args.local_rank != -1:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],
                                                          output_device=args.local_rank)
    elif n_gpu > 1 and not args.no_cuda:
        model = torch.nn.DataParallel(model)

    return model, device

def preload(args):
    processors = {
        "ontonotes_cws": lambda: CWS_BMEO(nopunc=args.nopunc),
    }

    task_name = args.task_name.lower()
    if task_name not in processors:
        raise ValueError("Task not found: %s" % (task_name))

    # Prepare model
    processor = processors[task_name]()

    label_list = processor.get_labels()
    model, device = load_model(label_list, args)

    if args.bert_model is not None:
        weights = torch.load(args.bert_model, map_location='cpu')

        try:
            model.load_state_dict(weights)
        except RuntimeError:
            model.module.load_state_dict(weights)

    model.eval()
    save_model(model, args.output_dir + 'model_eval.tsv')

    return model

def set_local_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/4ner_data/',
            'vocab_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/vocab.txt',
            'bert_config_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/bert_config.json',
            'output_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_23/rs/nhl3/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 128,
            'num_hidden_layers': 3,
            'init_checkpoint': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/',
            'bert_model': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_23/models/nhl3/weights_epoch03.pt',
            'override_output': True,
            'tensorboardWriter': False
            }

def set_server_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'data_dir': '../data/ontonotes5/4ner_data/',
            'vocab_file': '../models/bert-base-chinese/vocab.txt',
            'bert_config_file': '../models/bert-base-chinese/bert_config.json',
            'output_dir': './tmp_2019_3_22/out/ontonotes_eval/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 128,
            'num_hidden_layers': 3,
            'init_checkpoint': '../models/bert-base-chinese/',
            'bert_model': './tmp_2019_3_23/ontonotes/nhl3_nte15_nbs64/weights_epoch03.pt',
            'visible_device': 0,
            }


def test_cases(model):
    tt00 = '''
        âœ¨ä»Šæ—¥ä»½ç‰›ä»”å¤–å¥—ç©¿æ­æ‰“å¡|åˆæ˜¥ä¸€å®šè¦æœ‰ä¸€ä»¶ä¸‡èƒ½ç‰›ä»”å¤–å¥—é¸­ğŸ’¯ã€‚-æˆ‘ä»Šå¤©åˆåŒå’å•æ²¡åŒ–å¦†å‡ºé—¨é€›è¡—äº†ã€æ‡’ç™Œæ™šæœŸé—´æ­‡æ€§å‘ä½œå“ˆå“ˆå“ˆå“ˆã€ã€‚
        -è½è‚©è¢–ã€ä¸ä¼šæ˜¾è‚©å®½/åèƒŒæœ‰æ¶‚é¸¦å’Œè•¾ä¸æ‹¼æ¥ã€è§å›¾å…­/ã€‚-Look1:æ­é…äº†è¡¬è¡«å’Œé»‘ç°è‰²ç‰›ä»”è£¤/ã€‚-Look2ï¼šæ­é…äº†ç™½è‰²çŸ­Tå’Œç‰›ä»”è£¤/ã€‚
        ç‰›ä»”è£¤æˆ‘å°è¯•äº†ä¸¤ç§é¢œè‰²ã€æµ…è‰²ç³»è“è‰²ç‰›ä»”è£¤æ•´ä½“å°±åå¤å¤é£ä¸€ç‚¹ã€é…æ·±è‰²ç³»å°±æ›´æ—¥å¸¸æ´»åŠ›ä¸€äº›ã€ã€‚#æ˜¥å¤©èŠ±ä¼šå¼€##æ¯æ—¥ç©¿æ­##æ—¥å¸¸ç©¿æ­#
    '''
    print(tt00)
    t0 = time.time()
    outputT0 = model.cutlist_noUNK([tt00])
    output0 = [' '.join(lst)+' ' for lst in outputT0]
    o0 = '\t'
    for x in output0: o0 += x + '\t'
    print(o0+'\n')
    print('Processing time: ' + str(time.time()-t0))

    tt00 = '''
        #å¤§é±¼æµ·æ£ # #å¤§é±¼æµ·æ£ å£çº¸# å¾ˆæ„Ÿäººçš„ä¸€éƒ¨ç”µå½±ã€Šå¤§é±¼æµ·æ£ ã€‹ï¼Œæ¤¿ä¸ºäº†æ•‘é²²ï¼Œä¸æƒœç‰ºç‰²è‡ªå·±çš„ä¸€åŠå¯¿å‘½ï¼Œæ¹«å–œæ¬¢æ¤¿ï¼Œ
        å´æŠŠè‡ªå·±çš„ä¸€åŠå¯¿å‘½ç»™äº†æ¤¿â€¦â€¦äººä¸€ä½†æ­»åéƒ½ä¼šåŒ–æˆä¸€æ¡å¤§é±¼ï¼Œæ¤¿å¬æˆ‘çš„ï¼Œæ•°åˆ°3ï¼Œ2ï¼Œ1ï¼Œæˆ‘ä»¬ä¸€èµ·è·³ä¸‹å»ï¼Œ3.2.1è·³ï¼Œ
        æˆ‘ä¼šåŒ–æˆ******é™ªç€ä½ ã€‚ã€‚æ¤¿ï¼Œæˆ‘å–œæ¬¢ä½ ï¼ï¼ã€‚â€œåŒ—å†¥æœ‰é±¼ï¼Œå…¶åä¸ºé²²ã€‚â€ã€‚â€œé²²ä¹‹å¤§ã€‚â€ã€‚â€œä¸€é”…ç‚–ä¸ä¸‹ã€‚ã€‚â€œåŒ–è€Œä¸ºé¸Ÿã€‚â€ã€‚
        â€œå…¶åä¸ºé¹ã€‚â€ã€‚â€œé¹ä¹‹å¤§ã€‚â€ã€‚â€œéœ€è¦ä¸¤ä¸ªçƒ§çƒ¤æ¶ã€‚â€ã€‚â€œä¸€ä¸ªç§˜åˆ¶ã€‚â€ã€‚â€œä¸€ä¸ªéº»è¾£ã€‚â€ã€‚â€œæ¥ç“¶é›ªèŠ±ï¼ï¼ï¼â€ã€‚â€œå¸¦ä½ å‹‡é—¯å¤©æ¶¯
    '''
    print(tt00)
    t0 = time.time()
    outputT0 = model.cutlist_noUNK([tt00])
    output0 = [' '.join(lst)+' ' for lst in outputT0]
    o0 = '\t'
    for x in output0: o0 += x + '\t'
    print(o0+'\n')
    print('Processing time: ' + str(time.time()-t0))

    '''
    	# å¤§ é±¼ æµ·æ£  # # å¤§ é±¼ æµ·æ£  å£çº¸ # å¾ˆ æ„Ÿäºº çš„ ä¸€ éƒ¨ ç”µå½± ã€Š å¤§ é±¼ æµ·æ£  ã€‹ ï¼Œ æ¤¿ ä¸ºäº† æ•‘ é²² ï¼Œ ä¸æƒœ ç‰ºç‰² 
    	è‡ªå·± çš„ ä¸€åŠ å¯¿å‘½ ï¼Œ æ¹« å–œæ¬¢æ¤¿ ï¼Œ å´ æŠŠ è‡ªå·± çš„ ä¸€åŠ å¯¿å‘½ ç»™ äº† æ¤¿ â€¦â€¦ äºº ä¸€ä½† æ­» å éƒ½ ä¼š åŒ–æˆ ä¸€ æ¡ å¤§ é±¼ ï¼Œ 
    	æ¤¿ å¬ æˆ‘ çš„ ï¼Œ æ•° åˆ° 3 ï¼Œ 2 ï¼Œ 1 ï¼Œ æˆ‘ä»¬ ä¸€èµ· è·³ ä¸‹å» ï¼Œ 3.2.1 è·³ ï¼Œ æˆ‘ ä¼š åŒ– æˆ ****** é™ª ç€ ä½  ã€‚ ã€‚ æ¤¿ ï¼Œ 
    	æˆ‘ å–œæ¬¢ ä½  ï¼ ï¼ ã€‚ â€œ åŒ—å†¥ æœ‰ é±¼ ï¼Œ å…¶ å ä¸º é²² ã€‚ â€ã€‚â€œ é²² ä¹‹ å¤§ ã€‚ â€ã€‚â€œ ä¸€ é”… ç‚– ä¸ ä¸‹ ã€‚ ã€‚â€œ åŒ– è€Œ ä¸º é¸Ÿ ã€‚ â€ã€‚
    	â€œ å…¶ å ä¸º é¹ ã€‚ â€ã€‚â€œ é¹ ä¹‹ å¤§ ã€‚ â€ã€‚â€œ éœ€è¦ ä¸¤ ä¸ª çƒ§çƒ¤ æ¶ ã€‚ â€ã€‚â€œ ä¸€ ä¸ª ç§˜åˆ¶ ã€‚ â€ã€‚â€œ ä¸€ ä¸ª éº»è¾£ ã€‚ â€ã€‚â€œ æ¥ ç“¶ 
    	é›ªèŠ± ï¼ ï¼ï¼â€ ã€‚ â€œ å¸¦ ä½  å‹‡é—¯ å¤©æ¶¯ 	
    '''


def test_ontonotes_file(model, args):
    parts = ['dev', 'train', 'test']

    for part in tqdm(parts):
        text_file = os.path.join(args.data_dir, 'eval_data/ontonotes_'+part+'.txt') #  data in text
        output_file = os.path.join(args.output_dir, 'ontonotes_'+part+'.txt') #  data in text
        st_read = time.time()
        with open(text_file, 'r') as f:
            sents = f.readlines()
        end_read = time.time()
        print('reading time: {:.3f} seconds'.format(end_read-st_read))

        outputT = model.cutlist_noUNK(sents)
        end_decode = time.time()
        print('decode time: {:.3f} seconds'.format(end_decode-end_read))

        with open(output_file, 'a+') as f:
            for lst in outputT:
                outstr = ' '.join(lst)
                f.writelines(outstr + '\n')
        end_write = time.time()
        print('write time: {:.3f} seconds'.format(end_write-end_decode))

        print(part + ' done!')

LOCAL_FLAG = False
#LOCAL_FLAG = True

if __name__=='__main__':
    if LOCAL_FLAG:
        kwargs = set_local_eval_param()
    else:
        kwargs = set_server_eval_param()

    args._parse(kwargs)
    print(args.data_dir+' '+args.output_dir)

    model = preload(args)

    test_cases(model)
    test_ontonotes_file(model, args)


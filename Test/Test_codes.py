#!/anaconda3/envs/haiqin370/bin/ python3
# -*- coding: utf-8 -*-
"""
Created on at 11:21 2019-01-30 
@author: haiqinyang

Feature: 

Scenario: 
"""
from sklearn.preprocessing import LabelEncoder
import sys
sys.path.append('../src')
from utilis import save_model
from config import args
from utilis import get_dataset_and_dataloader, get_eval_dataloaders
from preprocess import CWS_BMEO
from tqdm import tqdm
import time
import torch

def test_BertCRF_constructor():
    from src.BERT.modeling import BertCRF
    from collections import namedtuple

    test_input_args = namedtuple("test_input_args", "bert_model cache_dir")
    test_input_args.bert_model = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese.tar.gz'
    test_input_args.cache_dir = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/'

    model = BertCRF(test_input_args, 4)


def test_BasicTokenizer():
    from src.tokenization import BasicTokenizer
    # prove processing English and Chinese characters
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    text = 'beauty‰∏ÄÁôæÂàÜ\n Beauty ‰∏ÄÁôæÂàÜ!!'
    print(basic_tokenizer.tokenize(text))


def test_FullTokenizer():
    from src.tokenization import FullTokenizer, BasicTokenizer

    vocab_file = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/vocab.txt'
    full_tokenizer = FullTokenizer(vocab_file, do_lower_case=True)

    text = '‰ªªÂ§©Â†ÇÊ∏∏ÊàèÂïÜÂ∫óÁöÑÂä†ÂÖ•Ë¢´‰∏öÁïåËßÜ‰∏∫androidÁöÑÈù©ÂëΩ„ÄÇ'
    print(full_tokenizer.tokenize(text))

    text = 'Âè∞Êπæ!!. ÊØîËµõ„ÄÇ‰ªäÂ§©ÔºåÂºÄÂßãÂêóÔºü  ÔºüÔºüÔºÅÂí≥Âí≥Ôø£ Ôø£)œÉÁ¨¨‰∏ÄÊ¨°Á©øÊ±âÊúçÂá∫Èó®üéÄüíûÂºÄÂøÉLaughing'
    print(full_tokenizer.tokenize(text))

    text = 'Âè∞ÊπæÁöÑÂÖ¨ËßÜ‰ªäÂ§©‰∏ªÂäûÁöÑÂè∞ÂåóÂ∏ÇÈïø Candidate  Defence  Ôºå'
    print(full_tokenizer.tokenize(text))
    #['Âè∞', 'Êπæ', 'ÁöÑ', 'ÂÖ¨', 'ËßÜ', '‰ªä', 'Â§©', '‰∏ª', 'Âäû', 'ÁöÑ', 'Âè∞', 'Âåó', 'Â∏Ç', 'Èïø', 'can', '##di', '##da', '##te', 'de', '##fe', '##nce', 'Ôºå']

    text = 'Candidate'
    print(full_tokenizer.tokenize(text))
    # ['can', '##di', '##da', '##te']

    text = '  Defence  Ôºå'
    print(full_tokenizer.tokenize(text))

    text1 = '''Ê§çÁâ©Á†îÁ©∂ÊâÄÊâÄÈï∑Âë®ÊòåÂºòÂÖàÁîüÁï∂ÈÅ∏Á¨¨‰∏â‰∏ñÁïåÁßëÂ≠∏Èô¢ÔºàÔº¥ÔΩàÔΩÖ„ÄÄÔº¥ÔΩàÔΩâÔΩíÔΩÑ„ÄÄÔº∑ÔΩèÔΩíÔΩåÔΩÑ„ÄÄÔº°ÔΩÉÔΩÅÔΩÑÔΩÖÔΩçÔΩô„ÄÄÔΩèÔΩÜ„ÄÄÔº≥ÔΩÉÔΩâÔΩÖÔΩéÔΩÉÔΩÖÔΩìÔºåÁ∞°Á®±Ôº¥Ôº∑Ôº°Ôº≥Ôºâ
    Èô¢Â£´„ÄÇÔº¥Ôº∑Ôº°Ôº≥‰øÇ‰∏Ä‰πùÂÖ´‰∏âÂπ¥Áî±Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄÔº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄÔº≥ÔΩÅÔΩåÔΩÅÔΩçÔºàÂ∑¥Âü∫ÊñØÂù¶Á±çÔºåÊõæÁç≤Ë´æË≤ùÁàæÁçéÔºâÁôºËµ∑ÊàêÁ´ãÔºåÊúÉÂì°ÈÅç‰ΩàÔºñÔºìÂÄãÂúãÂÆ∂ÔºåÁõÆÂâçÁî±ÔºíÔºìÔºí‰ΩçÈô¢Â£´
    ÔºàÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÂèäÔº¶ÔΩèÔΩïÔΩéÔΩÑÔΩâÔΩéÔΩá„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâÔºåÔºñÔºñ‰ΩçÂçîÈô¢Â£´ÔºàÔº°ÔΩìÔΩìÔΩèÔΩÉÔΩâÔΩÅÔΩîÔΩÖ„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâÔºíÔºî‰ΩçÈÄö‰ø°Èô¢Â£´ÔºàÔº£ÔΩèÔΩíÔΩíÔΩÖÔΩìÔΩêÔΩèÔΩéÔΩÑÔΩâÔΩéÔΩá„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâ
    „ÄÄÂèäÔºí‰ΩçÈÄö‰ø°ÂçîÈô¢Â£´ÔºàÔº£ÔΩèÔΩíÔΩíÔΩÖÔΩìÔΩêÔΩèÔΩéÔΩÑÔΩâÔΩéÔΩá„ÄÄÔº°ÔΩìÔΩìÔΩèÔΩÉÔΩâÔΩÅÔΩîÔΩÖ„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâÁµÑÊàêÔºà‰∏çÂåÖÊã¨‰∏Ä‰πù‰πùÂõõÂπ¥Áï∂ÈÅ∏ËÄÖÔºâÔºåÊùéÊîøÈÅì„ÄÅÊ•äÊåØÂØß„ÄÅ‰∏ÅËÇá‰∏≠„ÄÅ
    ÊùéÈÅ†Âì≤„ÄÅÈô≥ÁúÅË∫´„ÄÅÂê≥ÂÅ•ÈõÑ„ÄÅÊú±Á∂ìÊ≠¶„ÄÅËî°ÂçóÊµ∑Á≠âÈô¢Â£´ÂùáÁÇ∫Ë©≤Èô¢Ôº°ÔΩìÔΩìÔΩèÔΩÉÔΩâÔΩÅÔΩîÔΩÖ„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩó„ÄÇÊú¨Èô¢Êï∏ÁêÜÁµÑÈô¢Â£´„ÄÅÂìà‰ΩõÂ§ßÂ≠∏Êï∏Â≠∏Á≥ªÊïôÊéà‰∏òÊàêÊ°êÔºåÁ∂ìÁëûÂÖ∏ÁöáÂÆ∂ÁßëÂ≠∏Èô¢Ë©ïÂÆöÁÇ∫
    ‰∏Ä‰πù‰πùÂõõÂπ¥ÂÖãÂàó‰ΩõÔºàÔº£ÔΩíÔΩÅÔΩÜÔΩèÔΩè„ÄÄÔº∞ÔΩíÔΩâÔΩöÔΩÖÔºâÁçéÂæó‰∏ªÔºåËóâ‰ª•Ë°®ÂΩ∞ÂÖ∂Âú®ÂæÆÂàÜÂπæ‰ΩïÈ†òÂüüÂΩ±ÈüøÊ∑±ÈÅ†‰πãË≤¢Áçª„ÄÇ'''
    print(full_tokenizer.tokenize(text1))

    text = 'Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄÔº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄÔº≥ÔΩÅÔΩåÔΩÅÔΩç'
    print(full_tokenizer.tokenize(text))

    text = ' Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄÔº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄÔº≥ÔΩÅÔΩåÔΩÅÔΩç '
    print(full_tokenizer.tokenize(text))

    text = '( Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄ Ôº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄ Ôº≥ÔΩÅÔΩåÔΩÅÔΩç  ) '
    print(full_tokenizer.tokenize(text))

    text = 'ÔºñÔºìÂÄãÂúãÂÆ∂'
    print(full_tokenizer.tokenize(text))

    text = 'ÔºñÔºì ÂÄãÂúãÂÆ∂'
    print(full_tokenizer.tokenize(text))

    text = 'ÔºíÔºîÔºíÔºî‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = 'ÔºíÔºîÔºíÔºî ‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = '2424 ‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = '2424‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = '‰∏ã‰∏ÄÊ≥¢DVDÂèäÁî®‰∫éÂèØÊê∫ÂºèÂ∞èÂûãËµÑËÆØÁî®ÂìÅÁöÑÂæÆÂûãÂÖâÁ¢üÔºàMinidiskÔºâÔºå‰πüÂ∑≤Ëø´‰∏çÂèäÂæÖÂú∞Á≠âÁùÄÊï≤ÂºÄÊ∂àË¥πËÄÖÁöÑËç∑ÂåÖ„ÄÇ'
    print(full_tokenizer.tokenize(text))

    test_text = 'Á¨¨‰∏â‰∏ñÁïåÁßëÂ≠∏Èô¢ÔºàÔº¥ÔΩàÔΩÖ„ÄÄÔº¥ÔΩàÔΩâÔΩíÔΩÑ„ÄÄÔº∑ÔΩèÔΩíÔΩåÔΩÑ„ÄÄÔº°ÔΩÉÔΩÅÔΩÑÔΩÖÔΩçÔΩô„ÄÄÔΩèÔΩÜ„ÄÄÔº≥ÔΩÉÔΩâÔΩÖÔΩéÔΩÉÔΩÖÔΩìÔºåÁ∞°Á®±Ôº¥Ôº∑Ôº°Ôº≥Ôºâ'
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    sep_tokens = basic_tokenizer.tokenize(test_text)
    print('Basic:')
    for tt in sep_tokens:
        if basic_tokenizer._is_chinese_char(ord(tt[0])):
            wt = 'C'
        elif basic_tokenizer._is_english_char(ord(tt[0])):
            wt = 'E'
        else:
            wt = 'O'

        print(tt+' '+wt)

def check_english(w):
    import re

    english_check = re.compile(r'[a-z]')

    if english_check.match(w):
        print("english", w)
    else:
        print("other:", w)

def test_pandas_drop():
    import pandas as pd
    import os

    data_dir = '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data'
    df = pd.read_csv(os.path.join(data_dir, "test_code.tsv"), sep='\t')

    # full_pos (chunk), ner, seg, text
    df.drop(['full_pos', 'ner'], axis=1)

    # change name to tag for consistently processing
    df.rename(columns={'seg': 'label'}, inplace=True)

    print(len(df.full_pos))

def test_pandas_drop_syn():
    import pandas as pd
    import numpy as np

    df = pd.DataFrame(np.arange(12).reshape(3,4), columns=['A', 'B', 'C', 'D'])
    print(df)

    # need parameter inplace=True
    df.drop(['B', 'C'], axis=1, inplace=True)
    print(df)

def test_metrics():
    from src.metrics import get_ner_BMES, get_ner_BIO, get_ner_fmeasure, reverse_style
    label_list_BMES = "O O O O O O O O O O O O B-PER E-PER O O O O O O O O O O O O O O O O O O O O O O O"
    label_list_BIO = "O B-PER I-PER O O O B-PER O O B-ORG I-ORG I-ORG"

    label_list_BMES = label_list_BMES.split()
    #get_ner_BMES(label_list_BMES)

    #label_list_BIO = label_list_BIO.split()
    ll_BIO1 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG']
    ll_BIO2 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']

    print(get_ner_BIO(ll_BIO1))
    print(get_ner_BIO(ll_BIO2))

def test_CWS_Dict():
    from src.utilis import CWS_Dict
    cws_dict = CWS_Dict()

    sent = 'ËøàÂêë  ÂÖÖÊª°  Â∏åÊúõ  ÁöÑ  Êñ∞  ‰∏ñÁ∫™  ‚Äî‚Äî  ‰∏Ä‰πù‰πùÂÖ´Âπ¥  Êñ∞Âπ¥  ËÆ≤ËØù  Ôºà  ÈôÑ  ÂõæÁâá  Ôºë  Âº†  Ôºâ\n  ' \
           '‰∏≠ÂÖ±‰∏≠Â§Æ  ÊÄª‰π¶ËÆ∞  „ÄÅ  ÂõΩÂÆ∂  ‰∏ªÂ∏≠  Ê±ü  Ê≥ΩÊ∞ë\n' \
           'Ôºà  ‰∏Ä‰πù‰πù‰∏ÉÂπ¥  ÂçÅ‰∫åÊúà  ‰∏âÂçÅ‰∏ÄÊó•  Ôºâ \n ' \
           'ÔºëÔºíÊúà  ÔºìÔºëÊó•  Ôºå  ‰∏≠ÂÖ±‰∏≠Â§Æ  ÊÄª‰π¶ËÆ∞  „ÄÅ  ÂõΩÂÆ∂  ‰∏ªÂ∏≠  Ê±ü  Ê≥ΩÊ∞ë  ÂèëË°®  ÔºëÔºôÔºôÔºòÂπ¥  Êñ∞Âπ¥  ËÆ≤ËØù  ' \
           '„Ää  ËøàÂêë  ÂÖÖÊª°  Â∏åÊúõ  ÁöÑ  Êñ∞  ‰∏ñÁ∫™  „Äã  „ÄÇ  Ôºà  Êñ∞ÂçéÁ§æ  ËÆ∞ËÄÖ  ÂÖ∞  Á∫¢ÂÖâ  ÊëÑ  Ôºâ\n' \
           'ÂêåËÉû  ‰ª¨  „ÄÅ  ÊúãÂèã  ‰ª¨  „ÄÅ  Â•≥Â£´  ‰ª¨  „ÄÅ  ÂÖàÁîü  ‰ª¨  Ôºö\n ' \
           'Âú®  ÔºëÔºôÔºôÔºòÂπ¥  Êù•‰∏¥  ‰πãÈôÖ  Ôºå  Êàë  ÂçÅÂàÜ  È´òÂÖ¥  Âú∞  ÈÄöËøá  ‰∏≠Â§Æ  ‰∫∫Ê∞ë  ÂπøÊí≠  ÁîµÂè∞  „ÄÅ' \
           '  ‰∏≠ÂõΩ  ÂõΩÈôÖ  ÂπøÊí≠  ÁîµÂè∞  Âíå  ‰∏≠Â§Æ  ÁîµËßÜÂè∞  Ôºå  Âêë  ÂÖ®ÂõΩ  ÂêÑÊóè  ‰∫∫Ê∞ë  Ôºå  Âêë  È¶ôÊ∏Ø  ÁâπÂà´  Ë°åÊîøÂå∫  ' \
           'ÂêåËÉû  „ÄÅ  Êæ≥Èó®  Âíå  Âè∞Êπæ  ÂêåËÉû  „ÄÅ  Êµ∑Â§ñ  ‰æ®ËÉû  Ôºå  Âêë  ‰∏ñÁïå  ÂêÑÂõΩ  ÁöÑ  ÊúãÂèã  ‰ª¨  Ôºå  Ëá¥‰ª•  ËØöÊåö  ÁöÑ  ' \
           'ÈóÆÂÄô  Âíå  ËâØÂ•Ω  ÁöÑ  Á•ùÊÑø  ÔºÅ  '

    q_num = cws_dict._findNum(sent)
    print(list(q_num.queue))

    sent = '‰∫∫ÊÇ¨ÊåÇÂú®ÂçäÁ©∫‰∏≠Â≠§Á´ãÊó†Êè¥ÔºåËÄå‰ªñÁöÑËÑö‰∏ãÂ∞±ÊòØ‰∏á‰∏àÊ∑±Ê∏ä„ÄÇ\n' \
           '‰ΩÜÁî±‰∫éÊãÖÂøÉÂç∑Ëµ∑ÁöÑÊ∞îÊµ™‰ºöÊääÈ©¨‰øÆËøû‰∫∫Â∏¶‰ºûÂêπËêΩÊÇ¨Â¥ñÔºåÁõ¥ÂçáÊú∫Êó†Ê≥ïÁõ¥Êé•ÂÆûÊñΩÁ©∫‰∏≠ÊïëÊè¥„ÄÇ\n' \
           'ÊïëÊè¥‰∫∫ÂëòÂè™ËÉΩÂÄüÂä©Áõ¥ÂçáÊú∫Áôª‰∏äÊÇ¨Â¥ñÈ°∂Á´ØÔºåÂ∞ÜÁª≥Á¥¢ÊâîÁªôÈ©¨‰øÆËøõË°åËê•Êïë„ÄÇ\n' \
           'Âú®ÁªèÂéÜ‰∫Ü‰∏ÄÁï™Âë®Êäò‰πãÂêéÔºåÈ©¨‰øÆÁªà‰∫éË¢´ÊïëÊè¥‰∫∫ÂëòÊãâ‰∏ä‰∫ÜÊÇ¨Â¥ñÔºå' \
           'Âπ∏ËøêÁöÑÊòØÁî±‰∫éËê•ÊïëÂèäÊó∂ÔºåÈ©¨‰øÆÊú¨‰∫∫Âπ∂Êó†Â§ßÁ¢ç„ÄÇ‰∏≠Â§ÆÂè∞ÁºñËØëÊä•ÈÅì„ÄÇ\n' \
           'Â•ΩÔºåËøôÊ¨°‰∏≠ÂõΩÊñ∞ÈóªËäÇÁõÆÂ∞±Âà∞ËøôÔºåÊàëÊòØÂæê‰øêÔºåË∞¢Ë∞¢Â§ßÂÆ∂„ÄÇ\n' \
           'Êé•‰∏ãÊù•ÊòØÁî±Áéã‰∏ñÊûó‰∏ªÊåÅÁöÑ‰ªäÊó•ÂÖ≥Ê≥®ËäÇÁõÆ„ÄÇ\n' \
           'ÂêÑ‰ΩçËßÇ‰ºóÔºåÂÜçËßÅ„ÄÇ\n' \
           ' Ôº•Ôº≠Ôº∞Ôº¥Ôºπ'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))

    # some problems are here
    sent = 'Ôº•Ôº≠Ôº∞Ôº¥Ôºπ'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))


def test_pkuseg():
    from src.metrics import getChunks, getFscore
    tag_list = ['BBIBBIIBIIIB', 'BBBBIBBBIIIB']
    #tag_list = ['S,BI,S,BII,BIII,S,S,S,S,BI,S']

    tmp_list = [','.join(tag_list[i]) for i in range(len(tag_list))]
    print(getChunks(tmp_list)) # ['B*1*0,B*2*1,B*1*3,B*3*4,B*4*7,B*1*11,', 'B*1*0,B*1*1,B*1*2,B*2*3,B*1*5,B*1*6,B*4*7,B*1*11,']


    tag_to_idx = {'B': 0, 'I': 1, 'O': 2}
    idx_to_chunk_tag = {}

    '''
    for tag, idx in tag_to_idx.items():
        if tag.startswith("I"):
            tag = "I"
        if tag.startswith("O"):
            tag = "O"
        idx_to_chunk_tag[idx] = tag
    '''

    idx_to_chunk_tag = {}
    tag_to_idx = {'B': 0, 'M': 1, 'E': 2, 'S': 3, '[START]': 4, '[END]': 5}
    BIO_tag_to_idx = {'B': 0, 'I': 1, 'O': 2, '[START]': 3, '[END]': 4}
    token_list = [''.join(str(BIO_tag_to_idx[item])+',') for i in range(len(tag_list)) for item in tag_list[i] ]

    idx_to_chunk_tag = idx_to_tag(BIO_tag_to_idx)

    token_list = []
    for i in range(len(tag_list)):
        t = ''
        for item in tag_list[i]:
            t += ''.join(str(BIO_tag_to_idx[item])+',')
        token_list.append(t)


    goldTagList = [token_list[0]]
    resTagList = [token_list[1]]

    scoreList, infoList = getFscore(goldTagList, resTagList, idx_to_chunk_tag)
    # ValueError: invalid literal for int() with base 10: 'B'
    print(scoreList)
    print(infoList)


def calSize(H, vs, mpe, L):
    for l in L:
        # embedding: (vs+mpe)*H; # Query, Key, value: 3*H*H; Intermediate: 4*H *H; Pooler: H*H
        sz = (vs+mpe)*H + ((3+4)*H*H)*l + H*H
        print('# layer: '+str(l)+', #para: '+str(sz))


def verifyModelSize():
    H = 768
    vs = 21128
    mpe = 512
    L = [3, 6, 12]

    num_model_para = calSize(H, vs, mpe, L)

'''
def test_parse_one2BERTformat():
    from OntoNotes.f6_generate_training_data import parse_one2BERT2Dict
    s = '(NP (CP (IP (NP (DNP (NER-GPE (NR Taiwan)) (DEG ÁöÑ)) (NER-ORG (NR ÂÖ¨ËßÜ))) (VP (NT ‰ªäÂ§©) (VV ‰∏ªÂäû))) (DEC ÁöÑ)) (NP-m (NP (NR Âè∞Âåó) (NN Â∏ÇÈïø)) (NP-m (NP (NN candidate) (NN defence)) (PU Ôºå))))'
    out_dict = parse_one2BERT2Dict(s)
    print('src_seg:'+out_dict['src_seg'])
    print('src_ner:'+out_dict['src_ner'])
    print('full_pos:'+out_dict['full_pos'])
    print('text:'+out_dict['text'])
    print('text_seg:'+out_dict['text_seg'])
    print('bert_seg:'+out_dict['bert_seg'])
    print('bert_ner:'+out_dict['bert_ner'])
'''

def set_local_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/4ner_data/',
            #'bert_model_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data/eval/2019_3_12/models/',
            'vocab_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/vocab.txt',
            'bert_config_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/bert_config.json',
            'output_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/rs/nhl3/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 64,
            'num_hidden_layers': 3,
            'init_checkpoint': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/',
            'bert_model': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/models/nhl3/weights_epoch03.pt',
            'override_output': True,
            'tensorboardWriter': False
            }


def test_load_model():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    label_list = ['B', 'M', 'E', 'S', '[START]', '[END]']
    model, device = load_model(label_list, args)

    save_model(model, args.output_dir + 'tmp.tsv')


def test_dataloader():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    processors = {
        "ontonotes_cws": lambda: CWS_BMEO(nopunc=args.nopunc),
    }

    task_name = args.task_name.lower()

    # Prepare model
    processor = processors[task_name]()
    train_dataset, train_dataloader = get_dataset_and_dataloader(processor, args, training=False, type = 'tmp_test')

    eval_dataloaders = get_ontonotes_eval_dataloaders(processor, args)

    for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
        input_ids, segment_ids, input_mask = batch[:3]
        label_ids = batch[3:] if len(batch[3:])>1 else batch[3]


def decode_iter(logits, attention_mask):
    mask = attention_mask.byte()
    batch_size, seq_length = mask.shape

    best_tags_list = []
    for idx in range(batch_size):
        # Find the tag which maximizes the score at the last timestep; this is our best tag
        # for the last timestep
        best_tags = []
        for iseq in range(seq_length):
            if mask[idx, iseq]:
                _, best_selected_tag = logits[idx, iseq].max(dim=0)
                best_tags.append(best_selected_tag.item())

        best_tags_list.append(best_tags)
    return best_tags_list


def decode_batch(logits, attention_mask):
    mask = attention_mask.byte()
    batch_size, seq_length = mask.shape

    _, best_selected_tag = logits.max(dim=2)

    best_tags_list = []
    for n in range(batch_size):
        selected_tag = torch.masked_select(best_selected_tag[n, :], mask[n, :])
        best_tags_list.append(selected_tag.tolist())

    return best_tags_list


def test_decode():
    n_sample = 32
    n_len = 8
    n_tag = 6

    logits = torch.rand((n_sample, n_len, n_tag))

    mask = [1]*(n_len//2) + [0]*(n_len//2)
    attention_mask = torch.ByteTensor([mask]*n_sample)

    tm = time.time()
    lit = decode_iter(logits, attention_mask)
    print('time: ' + str(time.time()-tm))
    print(lit)

    tm = time.time()
    lbt = decode_batch(logits, attention_mask)
    print('time: ' + str(time.time()-tm))
    print(lbt)


if __name__ == '__main__':
    #test_BertCRF_constructor()
    #test_BasicTokenizer()
    #test_pandas_drop()
    #test_pandas_drop_syn()
    #test_metrics()
    #test_CWS_Dict()

    #test_pkuseg()
    #test_FullTokenizer()
    #check_english('candidate defence')
    #check_english('Âè∞Âåócandidate defence')

    #test_parse_one2BERTformat()

    #test_load_model()

    #test_dataloader()

    test_decode()

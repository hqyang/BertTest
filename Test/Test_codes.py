#!/anaconda3/envs/haiqin370/bin/ python3
# -*- coding: utf-8 -*-
"""
Created on at 11:21 2019-01-30 
@author: haiqinyang

Feature: 

Scenario: 
"""
from sklearn.preprocessing import LabelEncoder
from src.config import args, segType
from src.utilis import save_model, get_dataset_and_dataloader, restore_unknown_tokens_without_unused_with_pos
from src.preprocess import CWS_BMEO, tokenize_list_with_cand_indexes, make_dict_feature_vec, read_dict
from src.BERT import BertTokenizer
from tqdm import tqdm
import time
import torch
import numpy as np


def test_BertCRF_constructor():
    from src.BERT.modeling import BertCRF
    from collections import namedtuple

    test_input_args = namedtuple("test_input_args", "bert_model cache_dir")
    test_input_args.bert_model = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese.tar.gz'
    test_input_args.cache_dir = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/'

    model = BertCRF(test_input_args, 4)


def test_BasicTokenizer():
    from src.tokenization import BasicTokenizer
    # prove processing English and Chinese characters
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    text = 'beautyä¸€ç™¾åˆ†\n Beauty ä¸€ç™¾åˆ†!!'
    print(basic_tokenizer.tokenize(text))


def test_FullTokenizer():
    from src.tokenization import FullTokenizer, BasicTokenizer

    vocab_file = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/models.txt'
    full_tokenizer = FullTokenizer(vocab_file, do_lower_case=True)

    text = 'ä»»å¤©å ‚æ¸¸æˆå•†åº—çš„åŠ å…¥è¢«ä¸šç•Œè§†ä¸ºandroidçš„é©å‘½ã€‚'
    print(full_tokenizer.tokenize(text))

    text = 'å°æ¹¾!!. æ¯”èµ›ã€‚ä»Šå¤©ï¼Œå¼€å§‹å—ï¼Ÿ  ï¼Ÿï¼Ÿï¼å’³å’³ï¿£ ï¿£)Ïƒç¬¬ä¸€æ¬¡ç©¿æ±‰æœå‡ºé—¨ğŸ€ğŸ’å¼€å¿ƒLaughing'
    print(full_tokenizer.tokenize(text))

    text = 'å°æ¹¾çš„å…¬è§†ä»Šå¤©ä¸»åŠçš„å°åŒ—å¸‚é•¿ Candidate  Defence  ï¼Œ'
    print(full_tokenizer.tokenize(text))
    #['å°', 'æ¹¾', 'çš„', 'å…¬', 'è§†', 'ä»Š', 'å¤©', 'ä¸»', 'åŠ', 'çš„', 'å°', 'åŒ—', 'å¸‚', 'é•¿', 'can', '##di', '##da', '##te', 'de', '##fe', '##nce', 'ï¼Œ']

    text = 'Candidate'
    print(full_tokenizer.tokenize(text))
    # ['can', '##di', '##da', '##te']

    text = '  Defence  ï¼Œ'
    print(full_tokenizer.tokenize(text))

    text1 = '''æ¤ç‰©ç ”ç©¶æ‰€æ‰€é•·å‘¨æ˜Œå¼˜å…ˆç”Ÿç•¶é¸ç¬¬ä¸‰ä¸–ç•Œç§‘å­¸é™¢ï¼ˆï¼´ï½ˆï½…ã€€ï¼´ï½ˆï½‰ï½’ï½„ã€€ï¼·ï½ï½’ï½Œï½„ã€€ï¼¡ï½ƒï½ï½„ï½…ï½ï½™ã€€ï½ï½†ã€€ï¼³ï½ƒï½‰ï½…ï½ï½ƒï½…ï½“ï¼Œç°¡ç¨±ï¼´ï¼·ï¼¡ï¼³ï¼‰
    é™¢å£«ã€‚ï¼´ï¼·ï¼¡ï¼³ä¿‚ä¸€ä¹å…«ä¸‰å¹´ç”±ï¼°ï½’ï½ï½†ã€€ï¼¡ï½„ï½‚ï½•ï½“ã€€ï¼³ï½ï½Œï½ï½ï¼ˆå·´åŸºæ–¯å¦ç±ï¼Œæ›¾ç²è«¾è²çˆ¾çï¼‰ç™¼èµ·æˆç«‹ï¼Œæœƒå“¡éä½ˆï¼–ï¼“å€‹åœ‹å®¶ï¼Œç›®å‰ç”±ï¼’ï¼“ï¼’ä½é™¢å£«
    ï¼ˆï¼¦ï½…ï½Œï½Œï½ï½—åŠï¼¦ï½ï½•ï½ï½„ï½‰ï½ï½‡ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰ï¼Œï¼–ï¼–ä½å”é™¢å£«ï¼ˆï¼¡ï½“ï½“ï½ï½ƒï½‰ï½ï½”ï½…ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰ï¼’ï¼”ä½é€šä¿¡é™¢å£«ï¼ˆï¼£ï½ï½’ï½’ï½…ï½“ï½ï½ï½ï½„ï½‰ï½ï½‡ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰
    ã€€åŠï¼’ä½é€šä¿¡å”é™¢å£«ï¼ˆï¼£ï½ï½’ï½’ï½…ï½“ï½ï½ï½ï½„ï½‰ï½ï½‡ã€€ï¼¡ï½“ï½“ï½ï½ƒï½‰ï½ï½”ï½…ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰çµ„æˆï¼ˆä¸åŒ…æ‹¬ä¸€ä¹ä¹å››å¹´ç•¶é¸è€…ï¼‰ï¼Œææ”¿é“ã€æ¥ŠæŒ¯å¯§ã€ä¸è‚‡ä¸­ã€
    æé å“²ã€é™³çœèº«ã€å³å¥é›„ã€æœ±ç¶“æ­¦ã€è”¡å—æµ·ç­‰é™¢å£«å‡ç‚ºè©²é™¢ï¼¡ï½“ï½“ï½ï½ƒï½‰ï½ï½”ï½…ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ã€‚æœ¬é™¢æ•¸ç†çµ„é™¢å£«ã€å“ˆä½›å¤§å­¸æ•¸å­¸ç³»æ•™æˆä¸˜æˆæ¡ï¼Œç¶“ç‘å…¸çš‡å®¶ç§‘å­¸é™¢è©•å®šç‚º
    ä¸€ä¹ä¹å››å¹´å…‹åˆ—ä½›ï¼ˆï¼£ï½’ï½ï½†ï½ï½ã€€ï¼°ï½’ï½‰ï½šï½…ï¼‰çå¾—ä¸»ï¼Œè—‰ä»¥è¡¨å½°å…¶åœ¨å¾®åˆ†å¹¾ä½•é ˜åŸŸå½±éŸ¿æ·±é ä¹‹è²¢ç»ã€‚'''
    print(full_tokenizer.tokenize(text1))

    text = 'ï¼°ï½’ï½ï½†ã€€ï¼¡ï½„ï½‚ï½•ï½“ã€€ï¼³ï½ï½Œï½ï½'
    print(full_tokenizer.tokenize(text))

    text = ' ï¼°ï½’ï½ï½†ã€€ï¼¡ï½„ï½‚ï½•ï½“ã€€ï¼³ï½ï½Œï½ï½ '
    print(full_tokenizer.tokenize(text))

    text = '( ï¼°ï½’ï½ï½†ã€€ ï¼¡ï½„ï½‚ï½•ï½“ã€€ ï¼³ï½ï½Œï½ï½  ) '
    print(full_tokenizer.tokenize(text))

    text = 'ï¼–ï¼“å€‹åœ‹å®¶'
    print(full_tokenizer.tokenize(text))

    text = 'ï¼–ï¼“ å€‹åœ‹å®¶'
    print(full_tokenizer.tokenize(text))

    text = 'ï¼’ï¼”ï¼’ï¼”ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = 'ï¼’ï¼”ï¼’ï¼” ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = '2424 ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = '2424ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = 'ä¸‹ä¸€æ³¢DVDåŠç”¨äºå¯æºå¼å°å‹èµ„è®¯ç”¨å“çš„å¾®å‹å…‰ç¢Ÿï¼ˆMinidiskï¼‰ï¼Œä¹Ÿå·²è¿«ä¸åŠå¾…åœ°ç­‰ç€æ•²å¼€æ¶ˆè´¹è€…çš„è·åŒ…ã€‚'
    print(full_tokenizer.tokenize(text))

    test_text = 'ç¬¬ä¸‰ä¸–ç•Œç§‘å­¸é™¢ï¼ˆï¼´ï½ˆï½…ã€€ï¼´ï½ˆï½‰ï½’ï½„ã€€ï¼·ï½ï½’ï½Œï½„ã€€ï¼¡ï½ƒï½ï½„ï½…ï½ï½™ã€€ï½ï½†ã€€ï¼³ï½ƒï½‰ï½…ï½ï½ƒï½…ï½“ï¼Œç°¡ç¨±ï¼´ï¼·ï¼¡ï¼³ï¼‰'
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    sep_tokens = basic_tokenizer.tokenize(test_text)
    print('Basic:')
    for tt in sep_tokens:
        if basic_tokenizer._is_chinese_char(ord(tt[0])):
            wt = 'C'
        elif basic_tokenizer._is_english_char(ord(tt[0])):
            wt = 'E'
        else:
            wt = 'O'

        print(tt+' '+wt)

def check_english(w):
    import re

    english_check = re.compile(r'[a-z]')

    if english_check.match(w):
        print("english", w)
    else:
        print("other:", w)

def test_pandas_drop():
    import pandas as pd
    import os

    data_dir = '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data'
    df = pd.read_csv(os.path.join(data_dir, "test_code.tsv"), sep='\t')

    # full_pos (chunk), ner, seg, text
    df.drop(['full_pos', 'ner'], axis=1)

    # change name to tag for consistently processing
    df.rename(columns={'seg': 'label'}, inplace=True)

    print(len(df.full_pos))

def test_pandas_drop_syn():
    import pandas as pd
    import numpy as np

    df = pd.DataFrame(np.arange(12).reshape(3,4), columns=['A', 'B', 'C', 'D'])
    print(df)

    # need parameter inplace=True
    df.drop(['B', 'C'], axis=1, inplace=True)
    print(df)

def test_metrics():
    from src.metrics import get_ner_BMES, get_ner_BIO, get_ner_fmeasure, reverse_style
    label_list_BMES = "O O O O O O O O O O O O B-PER E-PER O O O O O O O O O O O O O O O O O O O O O O O"
    label_list_BIO = "O B-PER I-PER O O O B-PER O O B-ORG I-ORG I-ORG"

    label_list_BMES = label_list_BMES.split()
    #get_ner_BMES(label_list_BMES)

    #label_list_BIO = label_list_BIO.split()
    ll_BIO1 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG']
    ll_BIO2 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']

    print(get_ner_BIO(ll_BIO1))
    print(get_ner_BIO(ll_BIO2))

def test_CWS_Dict():
    from src.utilis import CWS_Dict
    cws_dict = CWS_Dict()

    sent = 'è¿ˆå‘  å……æ»¡  å¸Œæœ›  çš„  æ–°  ä¸–çºª  â€”â€”  ä¸€ä¹ä¹å…«å¹´  æ–°å¹´  è®²è¯  ï¼ˆ  é™„  å›¾ç‰‡  ï¼‘  å¼   ï¼‰\n  ' \
           'ä¸­å…±ä¸­å¤®  æ€»ä¹¦è®°  ã€  å›½å®¶  ä¸»å¸­  æ±Ÿ  æ³½æ°‘\n' \
           'ï¼ˆ  ä¸€ä¹ä¹ä¸ƒå¹´  åäºŒæœˆ  ä¸‰åä¸€æ—¥  ï¼‰ \n ' \
           'ï¼‘ï¼’æœˆ  ï¼“ï¼‘æ—¥  ï¼Œ  ä¸­å…±ä¸­å¤®  æ€»ä¹¦è®°  ã€  å›½å®¶  ä¸»å¸­  æ±Ÿ  æ³½æ°‘  å‘è¡¨  ï¼‘ï¼™ï¼™ï¼˜å¹´  æ–°å¹´  è®²è¯  ' \
           'ã€Š  è¿ˆå‘  å……æ»¡  å¸Œæœ›  çš„  æ–°  ä¸–çºª  ã€‹  ã€‚  ï¼ˆ  æ–°åç¤¾  è®°è€…  å…°  çº¢å…‰  æ‘„  ï¼‰\n' \
           'åŒèƒ  ä»¬  ã€  æœ‹å‹  ä»¬  ã€  å¥³å£«  ä»¬  ã€  å…ˆç”Ÿ  ä»¬  ï¼š\n ' \
           'åœ¨  ï¼‘ï¼™ï¼™ï¼˜å¹´  æ¥ä¸´  ä¹‹é™…  ï¼Œ  æˆ‘  ååˆ†  é«˜å…´  åœ°  é€šè¿‡  ä¸­å¤®  äººæ°‘  å¹¿æ’­  ç”µå°  ã€' \
           '  ä¸­å›½  å›½é™…  å¹¿æ’­  ç”µå°  å’Œ  ä¸­å¤®  ç”µè§†å°  ï¼Œ  å‘  å…¨å›½  å„æ—  äººæ°‘  ï¼Œ  å‘  é¦™æ¸¯  ç‰¹åˆ«  è¡Œæ”¿åŒº  ' \
           'åŒèƒ  ã€  æ¾³é—¨  å’Œ  å°æ¹¾  åŒèƒ  ã€  æµ·å¤–  ä¾¨èƒ  ï¼Œ  å‘  ä¸–ç•Œ  å„å›½  çš„  æœ‹å‹  ä»¬  ï¼Œ  è‡´ä»¥  è¯šæŒš  çš„  ' \
           'é—®å€™  å’Œ  è‰¯å¥½  çš„  ç¥æ„¿  ï¼  '

    q_num = cws_dict._findNum(sent)
    print(list(q_num.queue))

    sent = 'äººæ‚¬æŒ‚åœ¨åŠç©ºä¸­å­¤ç«‹æ— æ´ï¼Œè€Œä»–çš„è„šä¸‹å°±æ˜¯ä¸‡ä¸ˆæ·±æ¸Šã€‚\n' \
           'ä½†ç”±äºæ‹…å¿ƒå·èµ·çš„æ°”æµªä¼šæŠŠé©¬ä¿®è¿äººå¸¦ä¼å¹è½æ‚¬å´–ï¼Œç›´å‡æœºæ— æ³•ç›´æ¥å®æ–½ç©ºä¸­æ•‘æ´ã€‚\n' \
           'æ•‘æ´äººå‘˜åªèƒ½å€ŸåŠ©ç›´å‡æœºç™»ä¸Šæ‚¬å´–é¡¶ç«¯ï¼Œå°†ç»³ç´¢æ‰”ç»™é©¬ä¿®è¿›è¡Œè¥æ•‘ã€‚\n' \
           'åœ¨ç»å†äº†ä¸€ç•ªå‘¨æŠ˜ä¹‹åï¼Œé©¬ä¿®ç»ˆäºè¢«æ•‘æ´äººå‘˜æ‹‰ä¸Šäº†æ‚¬å´–ï¼Œ' \
           'å¹¸è¿çš„æ˜¯ç”±äºè¥æ•‘åŠæ—¶ï¼Œé©¬ä¿®æœ¬äººå¹¶æ— å¤§ç¢ã€‚ä¸­å¤®å°ç¼–è¯‘æŠ¥é“ã€‚\n' \
           'å¥½ï¼Œè¿™æ¬¡ä¸­å›½æ–°é—»èŠ‚ç›®å°±åˆ°è¿™ï¼Œæˆ‘æ˜¯å¾ä¿ï¼Œè°¢è°¢å¤§å®¶ã€‚\n' \
           'æ¥ä¸‹æ¥æ˜¯ç”±ç‹ä¸–æ—ä¸»æŒçš„ä»Šæ—¥å…³æ³¨èŠ‚ç›®ã€‚\n' \
           'å„ä½è§‚ä¼—ï¼Œå†è§ã€‚\n' \
           ' ï¼¥ï¼­ï¼°ï¼´ï¼¹'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))

    # some problems are here
    sent = 'ï¼¥ï¼­ï¼°ï¼´ï¼¹'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))


def test_pkuseg():
    from src.metrics import getChunks, getFscore
    tag_list = ['BBIBBIIBIIIB', 'BBBBIBBBIIIB']
    #tag_list = ['S,BI,S,BII,BIII,S,S,S,S,BI,S']

    tmp_list = [','.join(tag_list[i]) for i in range(len(tag_list))]
    print(getChunks(tmp_list)) # ['B*1*0,B*2*1,B*1*3,B*3*4,B*4*7,B*1*11,', 'B*1*0,B*1*1,B*1*2,B*2*3,B*1*5,B*1*6,B*4*7,B*1*11,']


    tag_to_idx = {'B': 0, 'I': 1, 'O': 2}
    idx_to_chunk_tag = {}

    '''
    for tag, idx in tag_to_idx.items():
        if tag.startswith("I"):
            tag = "I"
        if tag.startswith("O"):
            tag = "O"
        idx_to_chunk_tag[idx] = tag
    '''

    idx_to_chunk_tag = {}
    tag_to_idx = {'B': 0, 'M': 1, 'E': 2, 'S': 3, '[START]': 4, '[END]': 5}
    BIO_tag_to_idx = {'B': 0, 'I': 1, 'O': 2, '[START]': 3, '[END]': 4}
    token_list = [''.join(str(BIO_tag_to_idx[item])+',') for i in range(len(tag_list)) for item in tag_list[i] ]

    idx_to_chunk_tag = idx_to_tag(BIO_tag_to_idx)

    token_list = []
    for i in range(len(tag_list)):
        t = ''
        for item in tag_list[i]:
            t += ''.join(str(BIO_tag_to_idx[item])+',')
        token_list.append(t)


    goldTagList = [token_list[0]]
    resTagList = [token_list[1]]

    scoreList, infoList = getFscore(goldTagList, resTagList, idx_to_chunk_tag)
    # ValueError: invalid literal for int() with base 10: 'B'
    print(scoreList)
    print(infoList)


def calSize(H, vs, mpe, L):
    for l in L:
        # embedding: (vs+mpe)*H; # Query, Key, value: 3*H*H; Intermediate: 4*H *H; Pooler: H*H
        sz = (vs+mpe)*H + ((3+4)*H*H)*l + H*H
        print('# layer: '+str(l)+', #para: '+str(sz))


def verifyModelSize():
    H = 768
    vs = 21128
    mpe = 512
    L = [3, 6, 12]

    num_model_para = calSize(H, vs, mpe, L)

'''
def test_parse_one2BERTformat():
    from OntoNotes.f6_generate_training_data import parse_one2BERT2Dict
    s = '(NP (CP (IP (NP (DNP (NER-GPE (NR Taiwan)) (DEG çš„)) (NER-ORG (NR å…¬è§†))) (VP (NT ä»Šå¤©) (VV ä¸»åŠ))) (DEC çš„)) (NP-m (NP (NR å°åŒ—) (NN å¸‚é•¿)) (NP-m (NP (NN candidate) (NN defence)) (PU ï¼Œ))))'
    out_dict = parse_one2BERT2Dict(s)
    print('src_seg:'+out_dict['src_seg'])
    print('src_ner:'+out_dict['src_ner'])
    print('full_pos:'+out_dict['full_pos'])
    print('text:'+out_dict['text'])
    print('text_seg:'+out_dict['text_seg'])
    print('bert_seg:'+out_dict['bert_seg'])
    print('bert_ner:'+out_dict['bert_ner'])
'''

def set_local_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/4ner_data/',
            #'bert_model_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data/eval/2019_3_12/models/',
            'vocab_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/models.txt',
            'bert_config_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/bert_config.json',
            'output_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/rs/nhl3/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 64,
            'num_hidden_layers': 3,
            'init_checkpoint': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/',
            'bert_model': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/models/nhl3/weights_epoch03.pt',
            'override_output': True,
            'tensorboardWriter': False
            }


def test_load_model():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    label_list = ['B', 'M', 'E', 'S', '[START]', '[END]']
    model, device = load_model(label_list, args)

    save_model(model, args.output_dir + 'tmp.tsv')


def test_dataloader():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    processors = {
        "ontonotes_cws": lambda: CWS_BMEO(nopunc=args.nopunc),
    }

    task_name = args.task_name.lower()

    # Prepare models
    processor = processors[task_name]()
    train_dataset, train_dataloader = get_dataset_and_dataloader(processor, args, training=False, type = 'tmp_test')

    eval_dataloaders = get_ontonotes_eval_dataloaders(processor, args)

    for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
        input_ids, segment_ids, input_mask = batch[:3]
        label_ids = batch[3:] if len(batch[3:])>1 else batch[3]


def decode_iter(logits, attention_mask):
    mask = attention_mask.byte()
    batch_size, seq_length = mask.shape

    best_tags_list = []
    for idx in range(batch_size):
        # Find the tag which maximizes the score at the last timestep; this is our best tag
        # for the last timestep
        best_tags = []
        for iseq in range(seq_length):
            if mask[idx, iseq]:
                _, best_selected_tag = logits[idx, iseq].max(dim=0)
                best_tags.append(best_selected_tag.item())

        best_tags_list.append(best_tags)
    return best_tags_list


def decode_batch(logits, attention_mask):
    mask = attention_mask.byte()
    batch_size, seq_length = mask.shape

    _, best_selected_tag = logits.max(dim=2)

    best_tags_list = []
    for n in range(batch_size):
        selected_tag = torch.masked_select(best_selected_tag[n, :], mask[n, :])
        best_tags_list.append(selected_tag.tolist())

    return best_tags_list


def test_decode():
    n_sample = 32
    n_len = 8
    n_tag = 6

    logits = torch.rand((n_sample, n_len, n_tag))

    mask = [1]*(n_len//2) + [0]*(n_len//2)
    attention_mask = torch.ByteTensor([mask]*n_sample)

    tm = time.time()
    lit = decode_iter(logits, attention_mask)
    print('time: ' + str(time.time()-tm))
    print(lit)

    tm = time.time()
    lbt = decode_batch(logits, attention_mask)
    print('time: ' + str(time.time()-tm))
    print(lbt)


def test_split():
    import re

    text = 'ä½†æ˜¯ï¼Œè§„æ¨¡å¤§ä¸ç­‰äºè§„æ¨¡ç»æµã€‚è¿™å¯ä»¥ä»ä¸‰ä¸ªæ–¹é¢è€ƒå¯Ÿï¼šï¼ˆï¼‘ï¼‰ç”Ÿäº§èƒ½åŠ›çš„é™åº¦ã€‚æŠ•å…¥å¢åŠ è¶…è¿‡ä¸€å®šç‚¹ï¼Œäº§å‡ºçš„å¢é‡æˆ–è¾¹é™…äº§å‡ºå°†ä¼šå‡å°‘ï¼Œå‡ºç°è§„æ¨¡æŠ¥é…¬é€’å‡ç°è±¡ã€‚ï¼ˆï¼’ï¼‰äº¤æ˜“æˆæœ¬çš„é™åº¦ï¼Œä¸»è¦æ˜¯ä¼ä¸šå†…éƒ¨äº¤æ˜“æˆæœ¬â€”â€”â€”é€šå¸¸ç§°ä¸ºç®¡ç†æˆæœ¬â€”â€”â€”é™åˆ¶ã€‚ä¼ä¸šä¹‹æ‰€ä»¥æ›¿ä»£å¸‚åœºå­˜åœ¨ï¼Œæ˜¯å› ä¸ºé€šè¿‡å¸‚åœºäº¤æ˜“æ˜¯éœ€è¦æˆæœ¬çš„ï¼Œå¦‚æœå¯»åˆé€‚äº§å“ã€è°ˆåˆ¤ã€ç­¾çº¦ã€ç›‘ç£æ‰§è¡Œç­‰ï¼Œéƒ½éœ€è¦èŠ±è´¹æˆæœ¬ï¼Œåœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œä¼ä¸šå°†ä¸€äº›ç»æµæ´»åŠ¨å†…éƒ¨åŒ–ï¼Œé€šè¿‡è¡Œæ”¿æƒå¨åŠ ä»¥ç»„ç»‡ï¼Œèƒ½å¤ŸèŠ‚çº¦å¸‚åœºä¸Šçš„äº¤æ˜“æˆæœ¬ã€‚ä¼ä¸šå†…éƒ¨åè°ƒä¸€èˆ¬é€šè¿‡å±‚çº§åˆ¶ç»“æ„è¿›è¡Œï¼Œä¹Ÿéœ€è¦ä¸€å®šçš„è´¹ç”¨ï¼Œè¿™ç§è´¹ç”¨ä¹ƒæ˜¯ä¼ä¸šå†…éƒ¨å‘ç”Ÿçš„äº¤æ˜“æˆæœ¬ã€‚å¦‚æœç®¡ç†å¹…åº¦è¿‡å¤§ï¼Œæˆ–è€…å±‚æ¬¡å¤ªå¤šï¼Œä»åŸºå±‚åˆ°ä¸­å¿ƒå†³ç­–è€…çš„ä¿¡æ¯ä¼ é€’é€Ÿåº¦å°±ä¼šå˜æ…¢ï¼Œç”šè‡³ä¿¡å·å¤±çœŸï¼Œè‡´ä½¿ä¼ä¸šæ•ˆç‡é™ä½ï¼Œå‡ºç°è§„æ¨¡ä¸ç»æµã€‚ç»„ç»‡ç®¡ç†å½¢å¼çš„å˜åŠ¨ï¼Œå¦‚å®è¡Œäº‹ä¸šéƒ¨åˆ¶ç­‰ï¼Œèƒ½å¤Ÿæ”¹å˜ä¿¡æ¯ä¼ é€’çš„é€Ÿåº¦å’Œä¿¡æ¯è´¨é‡ï¼Œæ”¹å–„å†³ç­–æ°´å¹³ï¼Œä»è€Œæ‹‰é•¿è§„æ¨¡ç»æµå­˜åœ¨çš„æ—¶é—´è·¨åº¦ã€‚ä½†è¿™ä¸æ˜¯æ²¡æœ‰é™åº¦çš„ã€‚ï¼ˆï¼“ï¼‰å¯¹æŠ€æœ¯è¿›æ­¥çš„é™åˆ¶ï¼Œè¿™åœ¨å‡ºç°å„æ–­æƒ…å½¢æ—¶å°¤å…¶å¦‚æ­¤ã€‚éšç€ä¼ä¸šè§„æ¨¡æ‰©å¤§ï¼Œåœ¨å¸‚åœºä¸­çš„å„æ–­åŠ›é‡çš„å¢å¼ºï¼Œå¸‚åœºå°†åç¦»å……åˆ†ç«äº‰æ—¶çš„å‡è¡¡ï¼Œå„æ–­è€…å°†é€šè¿‡å„æ–­å®šä»·å’Œè¿›å…¥å£å’é™åˆ¶ç«äº‰è€…ï¼Œèµšå–å„æ–­åˆ©æ¶¦ã€‚æ­¤æ—¶ä¼ä¸šè¿½æ±‚åˆ›é€ ã€è¿½æ±‚æŠ€æœ¯è¿›æ­¥çš„å‹åŠ›å’ŒåŠ¨åŠ›å°†ä¼šå‡å¼±ã€‚è¿™åœ¨ä¸€ä¸ªè¡Œä¸šåªæœ‰ä¸€ä¸ªä¼ä¸šçš„å®Œå…¨å„æ–­ï¼ˆç‹¬å ï¼‰æƒ…å½¢ä¸­æœ€ä¸ºæ˜æ˜¾ã€‚ä¹Ÿæ­£å› ä¸ºè¿™ä¸€ç‚¹ï¼Œä¸»è¦å¸‚åœºç»æµå›½å®¶çš„åå„æ–­æ³•éƒ½æåŠ›é™åˆ¶å„æ–­ç¨‹åº¦ï¼Œä¸å…è®¸ä¸€ä¸ªè¡Œä¸šåªæœ‰ä¸€ä¸ªä¼ä¸šï¼ˆå‚å•†ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨æ–°çš„ç§‘æŠ€é©å‘½é¢å‰ï¼Œå°ä¼ä¸šä¹Ÿå› å…¶èƒ½å¤Ÿçµæ´»åœ°é¢å¯¹å¸‚åœºã€å¯Œæœ‰åˆ›é€ åŠ›è€Œæ˜¾ç¤ºå‡ºç”Ÿå‘½åŠ›ï¼Œå¤§ä¼ä¸šåè€Œå¯èƒ½å¯¹å¸‚åœºå˜åŒ–ååº”è¿Ÿç¼“è€Œå¤„äºç«äº‰åŠ£åŠ¿ã€‚æ€»ä¹‹ï¼Œè§„æ¨¡ç»æµåŒ…å«çš„æ˜¯ä¸€ä¸ªé€‚åº¦è§„æ¨¡ã€æœ‰æ•ˆè§„æ¨¡ï¼Œæ—¢ä¸æ˜¯è¶Šå¤§è¶Šç»æµï¼Œä¹Ÿä¸æ˜¯è¶Šå°è¶Šç»æµã€‚'
    at = re.split('(ã€‚|ï¼Œ|ï¼š|\n|#)', text)
    print(at)


def test_write():
    with open('tmp.txt', 'w+') as f:
        f.write('abc\n')
        f.write('dd')
        f.write(' ee')
        f.write('\n abc')

    with open('tmp.txt', 'r') as f:
        line = f.read()
        while line != '':
            print(line)
            line = f.read()


def test_construct_pos_tags():
    from src.preprocess import construct_pos_tags

    pos_tags_file = '../resource/pos_tags.txt'
    pos_label_list, pos_label_map, pos_idx_to_label_map = \
        construct_pos_tags(pos_tags_file, mode = 'BIO')

    print(pos_label_list)
    print(pos_label_map)
    print(pos_idx_to_label_map)


def test_outputPOSFscoreUsedBIO():
    goldTagList = [[0, 62, 63, 62, 63, 104, 105, 85, 85, 104, 105, 65, 66, 85, 104, 105, 105, 105, 1, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 85, 65, 66, 66, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 79, 62, 63, 62, 63, 79, 62, 63, 62, 63, 104, 105, 104, 105, 53, 54, 85, 104, 105,
                    79, 68, 69, 62, 63, 22, 65, 66, 65, 66, 62, 63, 62, 63, 62, 63, 85, 4, 79, 68, 69, 55,
                    4, 104, 105, 47, 48, 62, 63, 85, 2, 3, 2, 3, 65, 66, 62, 63, 62, 63, 62, 63, 4, 106,
                    4, 106, 85, 104, 105, 65, 66, 62, 63, 25, 62, 63, 85, 4, 4, 106, 62, 63, 10, 62, 63,
                    106, 79, 47, 48, 25, 62, 63, 62, 63, 55, 85, 47, 48, 48, 25, 62, 63, 62, 63, 13, 62,
                    63, 62, 63, 4, 104, 105, 62, 63, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 65, 66, 65, 66, 62, 63, 63, 62, 63, 62, 63, 71, 72, 62, 63, 62, 63, 62, 63, 85, 79,
                    68, 69, 68, 69, 69, 104, 105, 105, 105, 85, 79, 62, 63, 104, 105, 14, 15, 15, 56, 57,
                    104, 105, 55, 85, 104, 105, 7, 14, 15, 15, 58, 62, 63, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0]]

    input_mask = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]

    preTagList = [[0, 62, 63, 62, 63, 63, 63, 85, 85, 62, 63, 65, 66, 85, 105, 4, 105, 104, 1],
                  [0, 85, 65, 66, 63, 85, 1],
                  [0, 79, 66, 66, 34, 63, 79, 63, 16, 62, 63, 104, 105, 4, 63, 62, 63, 85, 104, 105, 79,
                   16, 69, 65, 69, 25, 62, 66, 66, 63, 62, 63, 62, 63, 62, 63, 85, 104, 105, 68, 69, 63,
                   4, 104, 105, 104, 105, 62, 105, 85, 104, 55, 104, 105, 65, 66, 62, 63, 66, 66, 62, 63,
                   104, 105, 104, 105, 85, 104, 105, 65, 66, 62, 63, 22, 62, 63, 85, 4, 4, 63, 62, 81, 4,
                   62, 63, 105, 79, 2, 16, 22, 104, 104, 62, 63, 55, 85, 62, 63, 63, 25, 62, 63, 104, 105,
                   13, 62, 105, 104, 105, 4, 104, 105, 104, 105, 85, 1],
                  [0, 65, 66, 66, 63, 62, 63, 63, 62, 63, 62, 63, 71, 16, 62, 63, 104, 105, 62, 63, 85, 79,
                   68, 69, 69, 15, 69, 105, 16, 63, 105, 85, 79, 66, 63, 104, 105, 15, 15, 15, 62, 58, 62,
                   63, 105, 85, 104, 105, 7, 14, 15, 15, 63, 63, 63, 85, 1]]

    from src.metrics import outputPOSFscoreUsedBIO
    scoreList, infoList = outputPOSFscoreUsedBIO(goldTagList, preTagList, input_mask)
    print(scoreList)
    print(infoList)


def compare_string_directly(text, tag):
    result_str = ''
    for idx in range(len(tag)):
        tt = text[idx]
        tt = tt.replace('##', '')
        ti = tag[idx]

        if int(ti) == 2:  # 'B'
            result_str += ' ' + tt
        elif int(ti) > 3:  # and (cur_word_is_english)
            # int(ti)>1: tokens of 'E' and 'S'
            # current word is english
            result_str += tt + ' '
        else:
            result_str += tt

    return result_str


def compare_string_reference(text, tag):
    result_str = ''
    for idx in range(len(tag)):
        tt = text[idx]
        tt = tt.replace('##', '')
        ti = tag[idx]

        if int(ti) == segType.BMES_label_map['B']:  # 'B'
            result_str += ' ' + tt
        elif int(ti) > segType.BMES_label_map['M']:  # and (cur_word_is_english)
            # int(ti)>1: tokens of 'E' and 'S'
            # current word is english
            result_str += tt + ' '
        else:
            result_str += tt

    return result_str

def check_time():
    import time

    count = 100000
    text_ele = ['ç”µå½±é¦–å‘ã€‚']*count
    text = ''.join(text_ele)

    tag_ele = ['24245']*count
    tag = ''.join(tag_ele)

    st = time.time()
    print(compare_string_directly(text, tag))
    print(time.time()-st)

    st = time.time()
    print(compare_string_reference(text, tag))
    print(time.time()-st)


def test_restore_unknown():
    original_str = 'ä¸åªçˆ±ç¿¡ç¿   ä¸€åˆ‡ç å® æ–‡ç©éƒ½æ˜¯æˆ‘çš„æœ€çˆ±   ä¸€ä¸ªå¥³ç”Ÿæˆ´ç€å¤§é‡‘åˆšå•¥çš„ç¡®å®å¾ˆéœ¸æ°”#ç”¨ç å®è®²æ•…äº‹# äº†ğŸ˜‚ğŸ˜‚  æ”¹å¤©æ‹¿å‡ºæ¥æ™’æ™’å¤ªé˜³  ğŸ˜#æˆ‘è¦ä¸Šå¢™# #ç²¾å“ç¿¡ç¿ #'
    result_str = 'ä¸ åª çˆ±  ç¿¡ç¿   ä¸€åˆ‡  ç å® æ–‡ç© éƒ½ æ˜¯ æˆ‘ çš„ æœ€ çˆ± ä¸€ ä¸ª  å¥³ç”Ÿ æˆ´ ç€  å¤§é‡‘åˆš [UNK] çš„  ç¡®å® å¾ˆ  éœ¸æ°” # ç”¨  ç å® è®²  æ•…äº‹ # äº† [UNK]  æ”¹å¤© æ‹¿  å‡ºæ¥  æ™’æ™’  å¤ªé˜³ [UNK] # æˆ‘ è¦ ä¸Š å¢™ # #  ç²¾å“  ç¿¡ç¿  # '
    result_pos = 'AD AD VV NR DT NN NN AD VC PN DEG AD NN CD M NN VV AS NN PU X X AD VA PU P NN VV NN PU AS PU VV VV VV VV NN PU PU PN VV VV NN PU PU NN NN PU '

    seg_ls, pos_ls = restore_unknown_tokens_without_unused_with_pos(original_str, result_str, result_pos)
    print(seg_ls)
    print(pos_ls)


def test_tokenize_list_with_cand_indexes():
    max_length = 128

    vocab_file = '../src/BERT/models/multi_cased_L-12_H-768_A-12/vocab.txt'
    do_lower_case = True
    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)
    lword = [['ä¸', 'åª', 'çˆ±', 'ç¿¡', 'ç¿ ', 'ä¸€', 'åˆ‡', 'ç ', 'å®', 'æ–‡', 'ç©', 'éƒ½', 'æ˜¯', 'æˆ‘', 'çš„', 'æœ€', 'çˆ±', 'ä¸€', 'ä¸ª', 'å¥³', 'ç”Ÿ', 'æˆ´', 'ç€', 'å¤§', 'é‡‘', 'åˆš', '[UNK]', 'çš„', 'ç¡®', 'å®', 'å¾ˆ', 'éœ¸', 'æ°”', '#', 'ç”¨', 'ç ', 'å®', 'è®²', 'æ•…', 'äº‹', '#', 'äº†', '[UNK]', 'æ”¹', 'å¤©', 'æ‹¿', 'å‡º', 'æ¥', 'æ™’', 'æ™’', 'å¤ª', 'é˜³', '[UNK]', '#', 'æˆ‘', 'è¦', 'ä¸Š', 'å¢™', '#', '#', 'ç²¾', 'å“', 'ç¿¡', 'ç¿ ', '#'], ['#', 'ç»˜', 'ç”»', 'æœº', 'å™¨', 'äºº', '#', 'âŠ™', '##âˆ€', '##âŠ™', 'ï¼', 'å“¦', 'åŸ', 'æ¥', 'è‡ª', 'å·±', 'é•¿', 'æˆ', 'è¿™', 'æ ·', 'å­', 'çš„', 'å•¦', 'ï¼'], ['#', 'ç¾', 'é£Ÿ', '#', 'ä¹‹', 'æ¥', 'è‡ª', 'ç§‘', 'å', 'è·¯', 'çš„', 'ä¸­', 'ä¸œ', 'æ–™', 'ç†', 'ï¼Œ', 'è¿ª', 'æ‹œ', 'æ–™', 'ç†', 'å‘³', 'é“', 'çœŸ', 'å¿ƒ', 'å¾ˆ', 'è®©', 'äºº', 'å¤±', 'æœ›', 'å“ˆ', '[UNK]', 'å‘³', 'é“', 'å¤ª', 'ä¸€', 'èˆ¬', 'ï¼Œ', 'æœ', 'ç„¶', 'åª', 'æ˜¯', 'ç¾', 'è€Œ', 'å·²', 'ã€‚', 'è¿˜', 'æ˜¯', 'åœŸ', 'è€³', 'å…¶', 'æ–™', 'ç†', 'å¥½', 'åƒ'], ['é€‚', 'åˆ', 'å¤', 'å¤©', 'çš„', 'å°‘', 'å¥³', 'æ„Ÿ', 'é¦™', 'æ°›', '[UNK]', 'è¶…', 'å¹³', 'ä»·', '[UNK]', 'è¶…', 'æ—¥', 'å¸¸', '[UNK]', 'ã€‚', '[UNK]', 'å¾ˆ', 'å¤š', 'å®', 'å®', 'å¹³', 'æ—¶', 'å‘¢', 'å’Œ', 'é‡', 'è¦', 'çš„', 'äºº', 'çº¦', 'ä¼š', 'éƒ½', 'ä¼š', 'æœ‰', 'ç§', 'å–·', 'é¦™', 'æ°´', 'çš„', 'ä¹ ', 'æƒ¯', 'ï¼Œ', 'ä½†', 'æ˜¯', 'å‘¢', 'ï¼Œ', 'å¯¹', 'äº', 'å­¦', 'ç”Ÿ', 'å…š', 'æ¥', 'è¯´', 'ï¼Œ', 'ä¸€', 'ç“¶', 'é¦™', 'æ°´', 'éƒ½', 'å¾ˆ', 'è´µ', 'ï¼Œ', 'å¾ˆ', 'éš¾', 'éš', 'ç€', 'å¿ƒ', 'æƒ…', 'æ¢', 'ä¸ª', 'å‘³', 'é“', 'ï¼Œ', 'è€Œ', 'ä¸”', 'æœ‰', 'äº›', 'é¦™', 'æ°´', 'å‘³', 'é“', 'ç‰¹', 'åˆ«', 'æµ“', 'ï¼Œ', 'å¾ˆ', 'å¤š', 'ç›´', 'ç”·', 'ç™Œ', 'ä»¬', 'è§‰', 'å¾—', 'ä½ ', 'èº«', 'ä¸Š', 'çš„', 'å‘³', 'é“', 'ç‰¹', 'åˆ«', 'åˆº', 'é¼»', '[UNK]', 'ã€‚', '[UNK]', 'æ‰€', 'ä»¥', 'æˆ‘', 'ç°', 'åœ¨', 'å–œ', 'æ¬¢', 'ç”¨', 'ä½“', 'é¦™', 'åŸ', 'æ¶²', 'ï¼Œ', 'æ·¡', 'æ·¡', 'çš„'], ['æ¸…', 'é¦™', 'ï¼Œ', 'è¿', 'å‡º', 'æ±—', 'éƒ½', 'æ˜¯', 'é', 'å¸¸', 'è‡ª', 'ç„¶', 'çš„', 'ä½“', 'é¦™', 'ï¼Œ', 'æ ¹', 'æœ¬', 'é—»', 'ä¸', 'åˆ°', 'ç‹', 'è‡­', 'æ±—', 'è‡­', 'å•Š', 'ï¼Œ', 'æ¯', 'æ¬¡', 'å‡º', 'é—¨', 'éƒ½', 'æº', 'å¸¦', 'åœ¨', 'èº«', 'ä¸Š', 'å¯', 'ä»¥', 'éš', 'æ—¶', 'æ¶‚', 'æŠ¹', 'ï¼Œ', 'æ¯', 'æ¬¡', 'æ¶‚', 'æŠ¹', 'éƒ½', 'è¢«', 'é—®', 'æˆ‘', 'æŠ¹', 'äº†', 'ä»€', 'ä¹ˆ', 'é‚£', 'ä¹ˆ', 'é¦™', 'ï¼Œ', 'å“ˆ', 'å“ˆ', 'å“ˆ', 'å“ˆ', 'ä¸€', 'ç›´', 'è¢«', 'å¤¸', 'é¦™', 'é¦™', 'çš„', '[UNK]', '#', 'ç‹', 'è‡­', '#', '#', 'ä½“', 'é¦™', '#'], ['æ¯', 'ä¸ª', 'å¥³', 'å­©', 'å­', 'éƒ½', 'å¯', 'ä»¥', 'æ˜¯', 'æ³¢', 'å¦', 'ï¼Œ', 'ä½†', 'é‡', 'åˆ°', 'çš„', 'ä¸', 'ä¸€', 'å®š', 'æ˜¯', 'å®—', 'ä»‹'], ['æ³¡', 'ä¸€', 'æ³¡', 'ï¼Œ', 'æ¢', 'æ–°', 'è½¦', 'å•¦', '#', 'æˆ‘', 'çˆ±', 'å®˜', 'æ–¹', 'æˆ‘', 'çˆ±', 'çƒ­', 'é—¨', '#', '#', 'é€†', 'è¢­', 'å°', 'ä»™', 'å¥³', '#', '[UNK]'], ['ç§€', 'æ©', 'çˆ±', 'å¿…', 'å¤‡', 'è¶…', 'å¯', 'çˆ±', 'çš„', 'æƒ…', 'ä¾£', 'å£', 'çº¸', 'å¤´', 'åƒ', '[UNK]', 'å›¾', 'ç´ ', 'æ', 'ã€‚', 'ç»™', 'å¤§', 'å®¶', 'åˆ†', 'äº«', 'ä¸€', 'äº›', 'è‡ª', 'å·±', 'å¾ˆ', 'å–œ', 'æ¬¢', 'çš„', 'æƒ…', 'ä¾£', 'å£', 'çº¸', 'åŸ', 'å›¾', 'å¯', 'ä»¥', 'æ€', 'æˆ‘', '[UNK]', 'ã€‚', '[UNK]', '[UNK]', 'å›¾', 'ä¿®', 'å›¾', '[UNK]', 'ã€‚', 'æ‰“', 'å¼€', 'pi', '##cs', '##art', 'å°±', 'å¯', 'ä»¥', 'ä»', 'ç›¸', 'å†Œ', 'ç…§', 'ç‰‡', 'é‡Œ', '[UNK]', 'å‡º', 'ä¸€', 'å¼ ', 'å¤§', 'å¤´', 'è´´', 'ï¼Œ', 'ç„¶', 'å', 'åœ¨', 'è¿™', 'äº›', 'å¡', 'é€š', 'å£', 'çº¸', 'çš„', 'ç©º', 'ç™½', 'å¤„', 'å†', 'è°ƒ', 'æ•´', 'ä¸€', 'ä¸‹', 'äº®', 'åº¦', 'å¯¹', 'æ¯”', 'åº¦', 'è¾¹', 'æ¡†', 'ä»€', 'ä¹ˆ', 'çš„', 'ä¸€', 'å¼ ', 'è¶…', 'ç”œ', 'è¶…', 'å°‘', 'å¥³', 'å¿ƒ', 'çš„', 'æƒ…', 'ä¾£', 'å¤´', 'åƒ', 'å£', 'çº¸', 'èƒŒ', 'æ™¯'], ['å›¾', 'å°±', '[UNK]', 'ç‚‰', 'å•¦', '~', 'ã€‚', 'èµ¶', 'ç´§', 'å’Œ', 'ç”·', 'æœ‹', 'å‹', 'ä¸€', 'èµ·', 'ç”¨', 'èµ·', 'æ¥', 'å§', '~', '[UNK]', '#', 'æœ‰', 'ä¸ª', 'æ‹', 'çˆ±', 'æƒ³', 'å’Œ', 'ä½ ', 'è°ˆ', '#', '#', 'å£', 'çº¸', '#', '#', 'æƒ…', 'ä¾£', '[UNK]', 'å›¾', 'ç´ ', 'æ', '#', '#', 'æƒ…', 'ä¾£', 'å¤´', 'åƒ', '#'], ['#', 'é­”', 'æ³•', 'ç…§', 'ç‰‡', '#', '#', 'é­”', 'æ³•', 'æ˜Ÿ', 'äº‘', 'ç…§', '#', 'ã€‚', 'å°', 'æ—¶', 'å€™', 'ï¼Œ', 'æ€»', 'æ˜¯', 'æ»¡', 'å¿ƒ', 'å¹»', 'æƒ³', 'ç€', 'æ‹¥', 'æœ‰', 'åƒ', 'å“ˆ', 'åˆ©', 'æ³¢', 'ç‰¹', 'ä¸€', 'æ ·', 'çš„', 'é­”', 'æ³•', 'ï¼Œ', 'å°', 'æ£’', 'ä¸€', 'æŒ¥', 'ï¼Œ', 'è®©', 'è‡ª', 'å·±', 'æ‹¥', 'æœ‰', 'å¾ˆ', 'å¤š', 'ä¸œ', 'è¥¿', 'ï¼›', 'é•¿', 'å¤§', 'å', 'ï¼Œ', 'æ‰', 'å‘', 'ç°', 'åŸ', 'æ¥', 'è‡ª', 'å·±', 'æ‰', 'æ˜¯', 'æœ€', 'å€¼', 'å¾—', 'æ‹¥', 'æœ‰', 'çš„', '[UNK]'], ['ä½ ', 'è¿˜', 'è®°', 'å¾—', 'æˆ‘', 'ä¹ˆ', 'ï½', 'æˆ‘', 'å¯', 'æ˜¯', 'æ·±', 'æ·±', 'çš„', 'è®°', 'å¾—', 'ä½ ', '[UNK]', '[UNK]', 'åœˆ', '#', '90', 'å', 'å…»', 'ç”Ÿ', 'æ—¥', 'å¸¸', '#', '#', 'ç«¥', 'å¹´', 'å„¿', 'æ—¶', 'çš„', 'å›', 'å¿†', '#', 'åœˆ', 'å‡º', 'ä½ ', 'çš„', 'å„¿', 'æ—¶', 'å°', 'é›¶', 'é£Ÿ', 'ã€', 'å›', 'å¿†', 'æ€', '[UNK]'], ['Iris', 'ç½‘', 'æ‹', 'æ¨¡', 'ç‰¹', 'å', 'é¦ˆ', '[', 'å˜¿', 'å“ˆ', ']', 'ã€‚', 'ç½‘', 'æ‹', 'å°±', 'æ˜¯', 'é€', 'ç¦', 'åˆ©', 'çš„', 'åœ°', 'æ–¹', 'å‘€', 'ï¼', 'å®ƒ', 'ä¸', 'æ˜¯', 'å¾®', 'å•†', 'è¦', 'ä½ ', 'å–', 'è´§', 'ï¼Œ', 'è€Œ', 'æ˜¯', 'ç®€', 'å•', 'æ‹', 'ç…§', 'é€', 'ä½ ', 'ä¸€', 'å †', 'è´§', 'å’Œ', '[UNK]', 'ã€‚', 'å®ƒ', 'ç¨³', 'å®š', 'å®‰', 'é€¸', 'ï¼Œ', 'ç”¨', 'é—²', 'æ—¶', 'å°±', 'å¯', 'ä»¥', 'èµš', '#', 'l', '##ris', 'ç½‘', 'æ‹', '#'], ['#', 'running', '##man'], ['å¤', 'æ—¥', 'æ˜¾', 'ç™½', 'é£', 'ã€‚', 'çº¢', 'è‰²', 'æ˜¾', 'ç™½', 'ï¼Œ', 'æ­', 'é…', 'ä¸Š', 'æœ€', 'è¿‘', 'æµ', 'è¡Œ', 'çš„', 'æ™•', 'æŸ“', 'ã€‚', 'å¥½', 'çœ‹', 'åˆ', 'æœ‰', 'å¤', 'æ—¥', 'æ°”', 'æ¯'], ['é—²', 'çš„', 'æ²¡', 'äº‹', 'ï¼Œ', 'æ”¾', 'æ¾', 'ä¸‹', 'å¿ƒ', 'æƒ…', 'å§', 'ï¼'], ['æœ¨', 'ç³ ', 'å¸ƒ', 'ä¸', '[UNK]', 'ä¸Š', 'æ¬¡', 'åœ¨', 'æ¾³', 'é—¨', 'åƒ', 'äº†', 'å¿µ', 'å¿µ', 'ä¸', 'å¿˜', 'ï¼Œ', 'ä»Š', 'å¤©', 'è‡ª', 'å·±', 'åŠ¨', 'æ‰‹', 'åš', 'äº†', 'ã€‚', 'è«‹', 'çµ¦', 'ä¸ª', 'èµ', 'ã€‚', '#', 'è‡ª', 'åˆ¶', 'ç¾', 'é£Ÿ', '#', '#', 'çƒ˜', 'ç„™', 'é£Ÿ', 'è°±', '#', '#', 'çƒ˜', 'ç„™', '#', '#', 'è‡ª', 'å·±', 'åŠ¨', 'æ‰‹', 'ç¾', 'é£Ÿ', '#', '#', 'è‡ª', 'å·±', 'åŠ¨', 'æ‰‹', 'ä¸°', 'è¡£', 'è¶³', 'é£Ÿ', '#', '#', 'å¤', 'æ—¥', 'ç”œ', 'é»', 'è‡ª', 'å·±', 'å‹•', 'æ‰‹', 'ä½œ', '#'], ['æ——', 'è¢', '_', '216', 'æ——', 'è¢', 'çš„', 'å‘', 'å±•', 'å²', '-', 'å¤', 'å…¸', 'æ——', 'è¢', 'ã€‚', 'æ——', 'è¢', ':', 'ç›¸', 'çº¦', 'æ¸©', 'å©‰', 'è£', 'å', 'ã€‚', 'è®©', 'æ—¶', 'å…‰', 'æŠŠ', 'ç²—', '[UNK]', 'è¤ª', 'å»', 'ã€‚', '#', 'æ——', 'è¢', '#', '#', 'å¤', 'é£', '#', '#', 'å†™', 'çœŸ', '#', '#', 'å', 'æœŸ', 'åˆ¶', 'ä½œ', '#', '#', 'æ‘„', 'å½±', '#', 'ã€‚', 'è®©', 'å²', 'æœˆ', 'æŠŠ', 'æ²§', 'æ¡‘', 'è', 'åŒ–', 'ã€‚', 'ç©¿', 'è¿‡', 'ä¸€', 'ä¸ª', 'ä¸–', 'çºª', 'çš„', 'é¥', 'è¿œ', 'è·', 'ç¦»', 'ã€‚', 'é¥±', 'å«', 'å¤©', 'ç”Ÿ', 'çš„', '[UNK]', 'åªš', 'ç«¯', 'åº„', 'ã€‚', 'å’Œ', 'ä½ ', 'ç›¸', 'çº¦', 'æ¸©', 'å©‰', 'è£', 'å', 'ã€‚', 'çº¤', 'å°˜', 'æŠ–', 'è½', 'åœ¨', 'å†', 'å²', 'è§’', 'è½', 'ä¸­', 'ã€‚', 'å¯‚', 'é™', 'åœ°', 'è®©', 'å‡º', 'å…‰', 'å½©', 'çš„'], ['é“', 'è·¯', 'ã€‚', 'è®©', 'æˆ‘', 'è„‰', 'è„‰', 'å«', 'æƒ…', 'è€Œ', 'æ¥', 'ã€‚', 'åœ¨', 'ç™¾', 'èŠ±', 'å¼€', 'æ”¾', 'çš„', 'æ˜¥', 'å¤©', 'ã€‚', 'åœ¨', 'çƒ­', 'æƒ…', 'ä¼¼', 'ç«', 'çš„', 'å¤', 'æ—¥', 'ã€‚', 'åœ¨', 'æµ', 'æ°´', '[UNK]', '[UNK]', 'çš„', 'é‡‘', 'ç§‹', 'ã€‚', 'åœ¨', 'ç™½', 'é›ª', 'é£˜', 'é£', 'çš„', 'å†¬', 'å­£', 'ã€‚', 'ä¸€', 'å¦‚', 'æ—¢', 'å¾€', 'ã€‚', 'ä¸', 'ä½ ', 'ç›¸', 'çº¦', 'æ¸©', 'å©‰', 'è£', 'å', 'ã€‚', '#', 'æ——', 'è¢', '#', '#', 'å¤', 'é£', '#', '#', 'å†™', 'çœŸ', '#', '#', 'å', 'æœŸ', 'åˆ¶', 'ä½œ', '#', '#', 'æ‘„', 'å½±', '#'], ['æ˜Ÿ', 'æ˜Ÿ', 'ç¯', 'ï¼ƒ', 'å½“', 'æˆ‘', 'çœ‹', 'ç€', 'ä½ ', 'çš„', 'æ—¶', 'å€™', 'çœ¼', 'é‡Œ', 'ä¹Ÿ', 'æœ‰', 'æ˜Ÿ', 'æ˜Ÿ', '[UNK]'], ['æ²¡', 'æœ‰', 'é‚£', 'æŠŠ', 'å‰‘', 'ï¼Œ', 'æˆ‘', 'ç…§', 'æ ·', 'å¯', 'ä»¥', 'æ­¼', 'ç­', 'æ•Œ', 'å†›'], ['æ‹', 'å‡º', 'æ™®', 'é€š', 'äºº', 'æœ€', 'ç¾', 'çš„', 'ä¸€', 'é¢', 'ï½'], ['å°', 'å§', 'å§', 'åˆ', 'åœ¨', 'æ‹', 'æ‹', 'æ‹', 'å•¦', '[UNK]', '#', 'é€†', 'è¢­', 'å°', 'ä»™', 'å¥³', '#'], ['ç¬¬', 'ä¸€', 'æ¬¡', 'åš', 'çš„', 'å›¾', 'æ¡ˆ', 'å‹‰', 'å¼º', 'èƒ½', 'çœ‹', '#', '520', 'é™ª', 'ä½ ', 'æ’’', 'ç³–', '#'], ['#', 'åœ°', 'ç‹±', 'å°‘', 'å¥³', '#', '#', 'é˜', 'é­”', 'çˆ±', '#', '[UNK]'], ['æœ¬', 'äºº', 'å› ', 'å·¥', 'ä½œ', 'åŸ', 'å› ', 'ï¼Œ', 'ç»', 'å¸¸', 'å‡º', 'å·®', 'ï¼Œ', 'å®¶', 'é‡Œ', 'æœ‰', 'åª', 'ç‹—', 'ç‹—', 'ä¸', 'èƒ½', 'ç…§', 'é¡¾', 'ï¼Œ', 'æ³', 'è¯·', 'æœ‰', 'çˆ±', 'å¿ƒ', 'çš„', 'å¥³', 'æ€§', 'å±±', 'ä¸œ', 'è', 'æ³½', 'æœ¬', 'åœ°', 'æœ‹', 'å‹', 'å…', 'è´¹', 'é¢†', 'å…»', ':', 'V', '##X', ':', 'j', '##sam', '##ine', '##12', '##12'], ['å‘', 'ä¸€', 'äº›', 'è‰º', 'æœ¯', 'æ°›', 'å›´', 'çš„', 'åˆ›', 'ä½œ', 'ï¼Œ', 'ç¾', 'å›¾', 'ä¸Š', '99', '%', 'æ˜¯', 'åƒ', 'ç¯‡', 'ä¸€', 'å¾‹', 'çš„', 'ç³–', 'æ°´', 'ç‰‡', 'ï¼Œ', 'ä¸', 'è€', 'çœ‹', 'ã€‚'], ['é»‘', 'é¾™', 'æ±Ÿ', 'æ—…', 'æ¸¸', 'æ”»', 'ç•¥', 'ä¹‹', 'äº”', 'å¤§', 'è¿', 'æ± ', 'ç«', 'å±±', 'æ¢', 'ç´¢', 'ä¹‹', 'æ—…', 'ã€‚', '[UNK]', 'è¡Œ', 'ç¨‹', 'è®¡', 'åˆ’', 'ã€‚', 'Day', '##1', ':', 'ã€‚', 'å…¥', 'ä½', 'äº”', 'å¤§', 'è¿', 'æ± ', 'é£', 'æ™¯', 'åŒº', 'ä¸‡', 'è±ª', 'å', 'è‹‘', 'å•†', 'åŠ¡', 'é…’', 'åº—', '[UNK]', '[UNK]', 'æ™š', 'é¤', 'çŸ¿', 'æ³‰', 'å†·', 'æ°´', 'é±¼', 'é±¼', 'å®´', 'ã€‚', 'Day', '##2', ':', 'ã€‚', 'æ–°', 'æœŸ', 'ç«', 'å±±', 'è§‚', 'å…‰', 'åŒº', 'è€', 'é»‘', 'å±±', 'ã€', 'ç«', 'çƒ§', 'å±±', '[UNK]', '[UNK]', 'åˆ', 'é¤', 'çŸ¿', 'æ³‰', 'è±†', 'è…', 'å®´', '[UNK]', '[UNK]', 'æ¸©', 'æ³Š', '[UNK]', '[UNK]', 'æ°´', 'æ™¶', 'å®«', 'æ™¯', 'åŒº', '[UNK]', '[UNK]', 'é¾™', 'é—¨', 'çŸ³', 'å¯¨', 'æ™¯', 'åŒº', 'ã€‚', 'Day', '##3', 'ï¼š', 'ã€‚', 'åœ£', 'æ°´', 'ç¥­', 'å…¸', '[UNK]', '[UNK]', 'èŠ±'], ['è½¦', 'å·¡', 'æ¸¸', '[UNK]', '[UNK]', 'æ³¥', 'æµ†', 'å¤§', 'æˆ˜', '[UNK]', '[UNK]', 'ç«', 'å±±', 'æ¸©', 'æ³‰', 'ä½“', 'éªŒ', '[UNK]', '[UNK]', 'å¼€', 'å¹•', 'å¼', 'åŠ', 'æ­Œ', 'èˆ', 'æ™š', 'ä¼š', '[UNK]', '[UNK]', 'ç”µ', 'éŸ³', 'ä¹‹', 'å¤œ', 'ã€‚', 'Day', '##4', ':', 'ã€‚', 'è¿”', 'ç¨‹', 'ã€‚', '#', 'é•¿', 'é£', 'è®¡', 'åˆ’', '#', '@', 'MT', 'æƒ…', 'æŠ¥', 'å±€', '@', 'MT', 'å°', 'åŠ©', 'æ‰‹', '@', 'MT', 'å±…', 'å§”', 'ä¼š', '@', 'MT', 'ç©', 'å›¾', 'å›'], ['#', 'å¤©', 'æ´¥', 'æ¢', 'åº—', '#', '#', 'å¤©', 'æ´¥', 'ç¾', 'é£Ÿ', '#', 'ã€‚', 'åº—', 'å', ':', 'ç¾½', 'æ·±', 'ã€‚', 'å›¾', 'ä¸€', 'ç„¦', 'ç³–', 'å†°', 'æ·‡', 'æ·‹', 'æˆ‘', 'çˆ±', 'äº†', '[UNK]', 'ã€‚', 'é¤', 'å…', 'ç¯', 'å¢ƒ', 'ä¹Ÿ', 'æŒº', 'å¥½', 'çš„', 'ã€‚', 'æ¨', 'è', 'ç‚¹', 'ç‰›', 'æ’', '[UNK]', 'ï¼Œ', 'è¿˜', 'æœ‰', 'ç•ª', 'èŒ„', 'ç‰›', '[UNK]', 'ã€‚', 'éœ€', 'è¦', 'æ', 'å‰', 'è®¢', 'ä¸€', 'ä¸‹'], ['[UNK]', 'è¶…', 'ç¾', 'èƒŒ', 'æ™¯', 'å¢™', 'é€', 'ç»™', 'ä½ ', '[UNK]'], ['é¼“', 'æ¥¼', 'åŒº', 'è€', 'å¹´', 'å¤§', 'å­¦', 'æ±‡', 'æŠ¥', 'æ¼”', 'å‡º', 'åœ†', 'æ»¡', 'ç»“', 'æŸ'], ['[', 'å¥¸', 'ç¬‘', ']', '[', 'å¥¸', 'ç¬‘', ']', 'è…®', 'çº¢', 'ç»§', 'ç»­', 'ã€‚', 'ä»¥', 'å‰', 'æˆ‘', 'æ€»', 'è§‰', 'å¾—', 'è…®', 'çº¢', 'ç”¨', 'ä¸', 'ç”¨', 'æ— ', 'æ‰€', 'è°“', 'ã€‚', 'ä½†', 'æ˜¯', 'å', 'æ¥', 'æˆ‘', 'å‘', 'ç°', 'ï¼Œ', 'çœ¼', 'å½±', 'æ²¡', 'æ—¶', 'é—´', 'ç»†', 'æ', 'æ…¢', 'åŒ–', 'ã€‚', 'ç›´', 'æ¥', 'è…®', 'çº¢', '[UNK]', 'å£', 'çº¢', 'ã€‚', 'æ°”', 'è‰²', 'ç›´', 'æ¥', 'ç¿»', 'å€', 'ï¼Œ', 'å¹´', 'è½»', '3', '-', '5', 'å²', '[', 'æœº', 'æ™º', ']'], ['[UNK]', '[', 'Mi', '##cist', '##y', 'å¯†', 'æ±', '[UNK]', 'è¿ª', 'æŸ', 'è…°', 'å¸¦', ']', 'ã€‚', 'è¶Š', 'ç¾', 'çš„', 'äºº', 'å¯¹', 'è‡ª', 'å·±', 'çš„', 'è¦', 'æ±‚', 'æ€»', 'æ˜¯', 'è¶Š', 'é«˜', 'ã€‚', 'ä¸', 'ä»…', 'å¯', 'ä»¥', 'è¾…', 'åŠ©', 'ç‡ƒ', 'è„‚', 'æ¸›', 'å°', 'è…°', 'å›´', 'ã€‚', 'è¿', 'åŠ¨', 'åˆ·', 'è„‚', 'è¿˜', 'å¯', 'ä»¥', 'ä¿', 'è­·', 'è…°', 'æ¤', 'ã€‚', 'å¡‘', 'å½¢', 'çš„', 'åŒ', 'æ™‚', 'å¸®', 'ä½ ', 'ç®¡', 'æ§', 'å', 'å§¿', 'ã€‚', 'ç¼“', 'è§£', 'ä¸', 'è‰¯', 'ä¹ ', 'æƒ¯', 'å¯¼', 'è‡´', 'çš„', 'è…°', 'éƒ¨', 'ç–²', 'åŠ³', 'ã€‚', 'æœ‰', 'æ•ˆ', 'çš„', 'çŸ«', 'æ­£', 'é©¼', 'èƒŒ', 'ä»¥', 'åŠ', 'è„Š', 'æ¤', 'å¼¯', 'æ›²', 'ã€‚', 'ä¸', 'ä»…', 'ä»…', 'æ˜¯', 'ä¸€', 'ä»¶', 'æŸ', 'è…°', 'å“¦', 'ã€‚', 'ä¹…', 'å', 'äºº', 'ç¾¤', 'éƒ½', 'ä¸', 'ä¼š', 'éš¾', 'å—', 'çš„', 'ä¸€', 'æ¬¾', 'æŸ', 'è…°'], [], ['ä½ ', 'å½’', 'æ¥', 'æ˜¯', 'è¯—', 'ï¼Œ', 'ç¦»', 'å»', 'æˆ', 'è¯', 'ï¼Œ', 'ä¸”', 'ç¬‘', 'é£', 'å°˜', 'ä¸', 'æ•¢', 'é€ ', 'æ¬¡', 'ã€‚'], ['è¿™', 'æ³¢', 'å‰§', 'ä¹Ÿ', 'å¤ª', 'å¤ª', 'å¤ª', 'å¤ª', 'å¥½', 'çœ‹', 'äº†', 'å§', 'ã€‚', 'æœ‰', 'ä½ ', 'ä»¬', 'å–œ', 'æ¬¢', 'çš„', 'å—', 'ã€‚', 'åƒ', 'çœ‹', 'ä»€', 'ä¹ˆ', 'å£', 'çº¸', 'æˆ–', 'ç”µ', 'è§†', 'å‰§', 'è¯„', 'è®º', 'åŒº', 'å‘Š', 'è¯‰', 'æˆ‘', '[UNK]', '#', 'å®˜', 'æ–¹', 'å¤§', 'å¤§', 'æˆ‘', 'è¦', 'ä¸Š', 'çƒ­', '#'], ['é‚£', 'æ˜¯', 'é›¨', 'å', 'çš„', 'å‚', 'æ™š', '[UNK]', 'æˆ‘', 'æœ‰', 'å¹¸', 'è§', 'åˆ°', 'äº†', 'è¿™', 'å¤©', 'è±¡', 'ï¼š', 'äº‘', 'æ¢…', 'åƒ', 'ç¾', 'ä¸½', 'çš„', 'æ¢…', 'èŠ±', 'åœ¨', 'å¤©', 'ä¸Š', 'é£', 'æ¥', 'é£', 'å»', 'ï¼Œ', 'ç¬', 'é—´', 'å˜', 'åŒ–', 'æ— ', 'ç©·', 'ï¼Œ', 'ç¾', 'è½®', 'ç¾', '[UNK]', 'ï¼'], ['#', 'æ²ˆ', 'é˜³', 'äºŒ', 'æ‰‹', 'è½¦', '#', '[UNK]', 'æ–°', 'åˆ°', '12', 'å¹´', 'ç°', 'ä»£', 'ç´¢', 'çº³', 'å¡”', 'å…«', 'ï¼Œ', 'è‡ª', 'åŠ¨', '2', '.', '0', 'æœ€', 'é«˜', 'é…', 'ï¼Œ', 'çœŸ', 'çš®', 'åº§', 'æ¤…', 'å¤§', 'å¤©', 'çª—', 'ï¼Œ', 'å¯¼', 'èˆª', 'å€’', 'è½¦', 'å½±', 'åƒ', 'ï¼Œ', 'å·¡', 'èˆª', 'å®š', 'é€Ÿ', 'ã€‚', 'ä¸€', 'æ‰‹', 'è½¦', 'ï¼Œ', 'å…¨', 'è½¦', 'åŸ', 'ç‰ˆ', 'ã€‚', '8', 'ä¸‡', 'å…¬', 'é‡Œ', 'ï¼Œ', 'è´¹', 'ç”¨', 'å…¨', 'å¹´', 'ï¼Œ', 'é¦–', 'ä»˜', '8000', 'è½¦', 'æ¬¾', '[UNK]'], ['æ¸…', 'ç†', 'ç©º', 'ç½®', 'æˆ¿', 'é™¢', 'å­', 'é‡Œ', 'çš„', 'æ‚', 'è‰'], ['å¥½', 'æœ‹', 'å‹', 'çš„', 'å¥³', 'å„¿', 'ç¡®', 'è¯Š', 'ç™½', 'è¡€', 'ç—…', 'ï¼Œ', 'æ‰', '8', 'å²', 'ï¼Œ', 'ä¸º', 'äº†', 'é¼“', 'åŠ±', 'å¥¹', 'ï¼Œ', 'æˆ‘', 'å’Œ', 'å¥¹', 'çˆ¸', 'çˆ¸', 'éƒ½', 'å‰ƒ', 'äº†', 'å…‰', 'å¤´', 'ï¼Œ', 'é™ª', 'å¥¹', 'ä¸€', 'èµ·', 'åŠ ', 'æ²¹', 'ï¼'], ['ä»Š', 'æ—¥', 'ä»½', 'èƒŒ', 'æ™¯', 'å›¾', 'ï¼š', 'ã€‚', '[UNK]', 'æ•', 'å¤´', 'åœ°', 'ä¸‹', 'ä¸€', 'é¢—', 'ç³–', 'ï¼Œ', 'åš', 'ä¸€', 'ä¸ª', 'ç”œ', 'ç”œ', 'çš„', 'æ¢¦', '.', '[UNK]', 'ã€‚', '#', 'æœ‰', 'ä¸ª', 'æ‹', 'çˆ±', 'æƒ³', 'å’Œ', 'ä½ ', 'è°ˆ', '#', '#', 'æœ‹', 'å‹', 'åœˆ', 'èƒŒ', 'æ™¯', 'å›¾', '#', '#', 'ins', 'é£', '#', '#', 'æ— ', 'æ°´', 'å°', '#', '#', 'å°‘', 'å¥³', 'å¿ƒ', '#'], ['è¿™', 'ä¸ª', 'ç”»', 'é£', 'æˆ‘', 'çˆ±', 'äº†', '(', '[UNK]', ')', '-', 'â™¡', 'ã€‚', 't', '##wi', ':', 'sa', '##kus', '##ya', '##2', '##hon', '##da', 'ã€‚', '@', 'MT', 'æƒ…', 'æŠ¥', 'å±€', '#', 'åŠ¨', 'æ¼«', 'å›¾', '#', '#', 'å¥½', 'çœ‹', 'çš„', 'åŠ¨', 'æ¼«', 'å›¾', '#', '#', 'å£', 'çº¸', '#'], ['å‘µ', 'å‘µ', '[UNK]', 'æ¶ˆ', 'ç£¨', 'æ—¶', 'é—´', 'ç©', 'ä¸€', 'ç©'], ['[UNK]', 'ä½ ', 'å¿…', 'ä¹°', 'çš„', 'ä¼˜', 'æ¢µ', 'æº¶', 'è„‚', 'éœœ', '[UNK]', '[UNK]', 'ã€‚', 'å¾ˆ', 'éš¾', 'è¿‡', 'è¦', 'è®©', 'ä½ ', 'å’Œ', 'ä½ ', 'å¤š', 'å¹´', 'çš„', 'å°', 'ç²—', 'è…¿', 'è¯´', 'ç™½', 'ç™½', 'äº†', '[UNK]', 'éš', 'æ—¶', 'éš', 'åœ°', 'æ»š', 'ä¸€', 'æ»š', '[UNK]', 'ä¸€', '[UNK]', 'å’Œ', 'å¤§', 'è±¡', 'è…¿', 'æ°´', 'æ¡¶', 'è…°', 'ä¸', 'å¤', 'å†', 'ç›¸', 'è§', 'è®©', 'æƒ…', 'æ•Œ', 'çœ‹', 'è§', 'è¢«', 'æ°”', 'æ­»', 'è®©', 'å‰', 'ä»»', 'çœ‹', 'è§', 'ä¼š', 'å', 'æ‚”', 'çš„', 'ç˜¦', 'èº«', 'ç¥', 'å™¨', 'å…¨', 'èº«', 'éƒ½', 'èƒ½', 'ç˜¦', 'ç´§', 'è‡´', 'æº¶', 'è„‚', 'å»', 'è‚Œ', 'è‚‰', 'è…¿', 'è¿˜', 'èƒ½', 'ç¾', 'ç™½', 'æœ‰', 'ä¸€', 'ç‚¹', 'è¾£', 'è¾£', 'çš„', 'ä½†', 'æ˜¯', 'ä¸', 'åˆº', 'æ¿€', 'çš„', 'æ•', 'æ„Ÿ', 'è‚Œ', 'å¯', 'ç”¨'], ['ç‚', 'ç‚', 'å¤', 'æ—¥', 'æ¥', 'ä¸€', 'æ¯', 'åŠ ', 'å†°', 'çš„', 'è‹¹', 'æœ', 'æ±', 'å­', '[UNK]'], ['æ²¡', 'æœ‰', 'å†°', 'å¥¶', 'èŒ¶', 'çš„', 'å¤', 'å¤©', 'ä¸', 'å®Œ', 'ç¾', '.', '.', '.', '.', '.'], ['#', 'ä¹¦', 'ç±', 'å®‰', 'åˆ©', '#', 'ã€Š', 'ä¸', 'è€', 'å¦ˆ', 'çš„', 'æ—¥', 'å¸¸', 'ã€‹', 'ã€‚', 'é‡Œ', 'é¢', 'æ˜¯', 'ä¸€', 'å', 'å', 'çš„', 'å°', 'æ¼«', 'ç”»', 'ï¼Œ', 'å', 'å', 'å…¥', 'å¿ƒ', '[UNK]', 'ã€‚', 'éš', 'æ‰‹', 'æ‘˜', 'æŠ„', '[UNK]', 'åˆ†', 'äº«', 'ä¸­', '[UNK]', '[UNK]', '[UNK]'], ['#', 'æ‰‹', 'ç»˜', '#', 'ä¸€', 'å¤©', 'ä¸€', 'ç”»', 'ï¼Œ', 'è´µ', 'åœ¨', 'åš', 'æŒ', '[UNK]', 'æœ›', 'å‹', 'å‹', 'ä»¬', 'ç‚¹', 'èµ', 'è¯„', 'è®º', '[UNK]'], ['é’', 'è—', 'ç‰¹', 'äº§', 'çº¢', '[UNK]', 'æ', 'ï¼Œ', 'é»‘', '[UNK]', 'æ', 'ï¼Œ', 'è—', 'çº¢', 'èŠ±', 'ï¼Œ', 'çº¯', 'å¤©', 'ç„¶', 'äº§', 'åœ°', 'æ­£', 'å®—', 'ï¼Œ', 'æŠ¢', 'è´­', 'çƒ­', 'çº¿', ':', '1383', '##47', '##0', '##86', '##11'], ['ç¾', 'è¡£', 'é­…', 'åŠ›', 'ï¼Œ', 'è°', 'ç©¿', 'è°', 'ç¾', 'ä¸½', '[UNK]'], ['ç»™', 'å¤§', 'å®¶', 'æ¨', 'è', 'ä¸€', 'éƒ¨', 'ç½‘', 'å‰§', 'ã€‚', 'å«', 'çµ', 'é­‚', 'æ‘†', 'æ¸¡', 'äºº', 'ã€‚', 'è¿™', 'éƒ¨', 'ç½‘', 'ç»œ', 'å‰§', 'æ€»', 'å…±', 'æœ‰', '3', 'éƒ¨', 'ã€‚', 'è¿™', 'ä¸ª', 'æ˜¯', '1', '-', '2', 'é›†', 'ä¸€', 'ä¸ª', 'æ•…', 'äº‹', 'ã€‚', 'è·Ÿ', 'é¬¼', 'ç‰‡', 'æ˜¯', 'ä¸€', 'æ ·', 'çš„', 'ã€‚', 'è¿™', 'éƒ¨', 'ç½‘', 'ç»œ', 'å‰§', 'å', 'åº”', 'äº†', 'ç°', 'å®', 'ä¸­', 'å¾ˆ', 'å¤š', 'å­˜', 'åœ¨', 'çš„', 'ä¸œ', 'è¥¿', 'ã€‚', 'è¯¥', 'å‰§', 'è®²', 'è¿°', 'äº†', 'å¤', 'å†¬', 'é’', 'å’Œ', 'é¬¼', 'å·®', 'èµµ', 'å', 'ã€', 'ä¹', 'å¤©', 'ç„', 'å¥³', 'ç‹', 'å°', 'äºš', 'ä¸‰', 'äºº', 'åœ¨', 'å¤„', 'ç†', 'ä¸€', 'ä»¶', 'ä»¶', 'æœ‰', 'å…³', 'é¬¼', 'é­‚', 'å’Œ', 'çµ', 'é­‚', 'çš„', 'æ•…', 'äº‹', 'ä¸­', 'æ„Ÿ', 'æ‚Ÿ', 'äºº', 'ç”Ÿ', 'çš„', 'æ•…', 'äº‹', 'ã€‚', 'çœŸ', 'çš„', 'è¶…', 'çº§', 'å¥½', 'çœ‹', 'ã€‚', 'æˆ‘', 'éƒ½', 'çœ‹', 'äº†'], ['å¥½', 'å¤š', 'é', 'äº†', 'ã€‚', 'ç‰¹', 'åˆ«', 'å–œ', 'æ¬¢', 'é‡Œ', 'é¢', 'æ‘†', 'æ¸¡', 'äºº', 'èµµ', 'å', 'ã€‚', 'è¿˜', 'æœ‰', 'é‡Œ', 'é¢', 'çš„', 'å†¥', 'ç‹', 'èŒ¶', 'èŒ¶', 'ã€‚', 'çœŸ', 'çš„', 'è¶…', 'çº§', 'æ¨', 'è', 'ã€‚'], ['åŒ…', 'è£…', 'å¥½', 'çœ‹', 'é¢œ', 'è‰²', 'ä¹Ÿ', 'å¥½', 'çœ‹', 'ã€‚', 'æ˜¾', 'è‰²', 'ä¸', 'é£', 'ç²‰', 'ï¼Œ', 'å“‘', 'å…‰', 'ç ', 'å…‰', 'å…±', '12', 'è‰²', 'ï¼Œ', 'æ‰“', 'é€ ', 'å„', 'ç§', 'å¦†', 'å®¹', 'è°', 'ç”¨', 'è°', 'çˆ±', '#', 'æ¬§', 'æŸ', '#'], ['#', 'ä½ ', 'å¥½', 'å¤', 'å¤©', '#', 'ã€‚', 'ä¸Š', 'ç­', 'é«˜', 'å³°', 'ã€‚', 'èƒ¡', 'å­', 'éƒ½', 'æ²¡', 'åˆ®', '[UNK]'], ['ä½™', 'å…‰', 'ä¸­', 'å…ˆ', 'ç”Ÿ', 'è¯´', ':', '[UNK]', 'æœˆ', 'è‰²', 'ä¸', 'é›ª', 'è‰²', 'ä¹‹', 'é—´', 'ï¼Œ', 'ä½ ', 'æ˜¯', 'ç¬¬', 'ä¸‰', 'ç§', 'ç»', 'è‰²', '.', '[UNK]', '#', 'èƒŒ', 'å½±', 'æ€', '#'], ['æ–°', 'æ¬¾', 'ç‚¹', 'ç‚¹', 'è£…', 'ï½', 'æ¸…', 'æ–°', 'çš„', 'åƒ', 'ç¾', 'å°‘', 'å¥³', 'ï½'], ['æˆ‘', 'çš„', 'å“ˆ', 'å¯†', 'æ´', 'ï¼', '[UNK]'], ['è¯·', 'å¿½', 'ç•¥', 'ç²—', 'ç•¥', 'çš„', 'èƒŒ', 'æ™¯', 'ï½', 'è¿˜', 'æœ‰', 'èƒ–', 'èƒ–', 'çš„', 'è…¿', 'è…¿', '(', 'ã€ƒ', '[UNK]', ')', 'ã€‚', 'è¿™', 'ä¸ª', 'é˜²', 'æ™’', 'å–·', 'é›¾', 'çœŸ', 'çš„', 'è¶…', 'çº§', 'å¥½', 'ç”¨', 'ï½', 'ä¸€', 'ç‚¹', 'ä¸', 'æ²¹', 'ï½', 'è¶…', 'çº§', 'èˆ’', 'æœ', 'ï¼Œ', 'å‘³', 'é“', 'ä¹Ÿ', 'å¾ˆ', 'å¥½', 'é—»', 'ï½'], ['#', 'å¦‚', 'æœ', 'æœ‰', 'ä¸€', 'å¤©', 'æˆ‘', 'æ¶ˆ', 'å¤±', 'äº†', '#', 'ä½ ', 'ä¼š', 'æ€', 'æ ·'], ['ä¾„', 'å­™', 'å¥³', 'å„¿', 'å…·', 'ç„¶', 'å’Œ', 'æˆ‘', 'ä¸€', 'å¤©', 'ç”Ÿ', 'æ—¥', 'ï¼', 'å¯', 'å–œ', 'å¯', 'è´º', 'ï¼', 'åŒ', 'å–œ', 'åŒ', 'ä¹', 'å•Š', 'ï¼', '[UNK]'], ['åŒ', 'äº‹', 'ç›¸', 'èš', 'ï¼Œ', 'ç¾', 'é£Ÿ', 'å°‘', 'ä¸', 'äº†', 'ã€‚'], ['[UNK]'], ['è¶…', 'çº§', 'çƒ­', 'çš„', 'å¤©', 'æ°”', 'å–', 'æ¯', 'å†°', 'èŒ¶', 'è§£', 'æš‘', 'å•¦', 'å•¦', 'å•¦', '[UNK]', '^', 'V', '^'], ['ç¾', 'æœ¯', 'æœŸ', 'æœ«', 'è€ƒ', 'è¯•', 'over', '##ï½', 'ã€‚', 'æ„Ÿ', 'è§‰', 'è‡ª', 'å·±', 'åª', 'æœ‰', 'b', '+', '[UNK]', '#', 'ç¾', 'æœ¯', '#', '#', 'æœŸ', 'æœ«', '#', '#', 'è€ƒ', 'è¯•', '#', '#', 'é«˜', 'ä¸€', '#', '#', 'é•¿', 'é£', 'è®¡', 'åˆ’', '#']]
    tuple1, tuple2 = zip(
            *[tokenize_list_with_cand_indexes(w, max_length, tokenizer) for w in lword if w])


def compare_time_tokenize():
    vocab_file = '../src/BERT/models/multi_cased_L-12_H-768_A-12/vocab.txt'
    do_lower_case = True
    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)

    infile = '../Test_cases/bad_cases.txt'
    with open(infile, 'r', encoding='utf8') as f:
        raw_data = f.readlines()

    tx = [x for x in raw_data if x.strip()]
    st = time.time()
    t1 = [tokenizer.tokenize(t) if len(tokenizer.tokenize(t))>0 else ['[UNK]'] for t in tx]
    print(time.time()-st)

    print(t1)


def test_make_dict_feature_vec():
    sentence = 'åŒ—äº¬å¤§å­¦ç”Ÿå­¦ä¼šå¾ˆä¼šæƒ¹äº‹'
    word_dict = ['åŒ—äº¬', 'åŒ—äº¬å¤§å­¦']

    #for max_gram in range(2, 5):
    max_gram = 5
    print(make_dict_feature_vec(sentence, word_dict, max_gram))


def test_read_dict():
    infile = '../resource/dict.txt'
    dict_o = read_dict(infile)
    print(dict_o)


def gen_tuples(bEmpty=True):
    inputs = np.random.randint(5, size=(5, 3)).tolist()
    labels = np.random.randint(2, size=(5, 1)).tolist()
    wd_fvs = []

    if not bEmpty:
        wd_fvs = np.random.randint(10, size=(5, 2)).tolist()

    print(inputs)
    print(labels)
    print(wd_fvs)
    return tuple(inputs + labels + wd_fvs)


def test_tuples():
    vv = gen_tuples()
    print(vv)


if __name__ == '__main__':
    #test_BertCRF_constructor()
    #test_BasicTokenizer()
    #test_pandas_drop()
    #test_pandas_drop_syn()
    #test_metrics()
    #test_CWS_Dict()

    #test_pkuseg()
    #test_FullTokenizer()
    #check_english('candidate defence')
    #check_english('å°åŒ—candidate defence')

    #test_parse_one2BERTformat()

    #test_load_model()

    #test_dataloader()

    #test_decode()
    #test_split()

    #test_write()

    #test_construct_pos_tags()

    #test_outputPOSFscoreUsedBIO()
    #check_time()

    #test_restore_unknown()
    #test_tokenize_list_with_cand_indexes()

    #compare_time_tokenize()

    #test_make_dict_feature_vec()

    #test_read_dict()

    test_tuples()

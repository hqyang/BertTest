#!/anaconda3/envs/haiqin370/bin/ python3
# -*- coding: utf-8 -*-
"""
Created on at 11:21 2019-01-30 
@author: haiqinyang

Feature: 

Scenario: 
"""
from sklearn.preprocessing import LabelEncoder
from src.utilis import save_model
from src.config import args, segType
from src.utilis import get_dataset_and_dataloader, restore_unknown_tokens_without_unused_with_pos
from src.preprocess import CWS_BMEO, tokenize_list_with_cand_indexes
from src.BERT import BertTokenizer
from tqdm import tqdm
import time
import torch


def test_BertCRF_constructor():
    from src.BERT.modeling import BertCRF
    from collections import namedtuple

    test_input_args = namedtuple("test_input_args", "bert_model cache_dir")
    test_input_args.bert_model = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese.tar.gz'
    test_input_args.cache_dir = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/'

    model = BertCRF(test_input_args, 4)


def test_BasicTokenizer():
    from src.tokenization import BasicTokenizer
    # prove processing English and Chinese characters
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    text = 'beauty‰∏ÄÁôæÂàÜ\n Beauty ‰∏ÄÁôæÂàÜ!!'
    print(basic_tokenizer.tokenize(text))


def test_FullTokenizer():
    from src.tokenization import FullTokenizer, BasicTokenizer

    vocab_file = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/models.txt'
    full_tokenizer = FullTokenizer(vocab_file, do_lower_case=True)

    text = '‰ªªÂ§©Â†ÇÊ∏∏ÊàèÂïÜÂ∫óÁöÑÂä†ÂÖ•Ë¢´‰∏öÁïåËßÜ‰∏∫androidÁöÑÈù©ÂëΩ„ÄÇ'
    print(full_tokenizer.tokenize(text))

    text = 'Âè∞Êπæ!!. ÊØîËµõ„ÄÇ‰ªäÂ§©ÔºåÂºÄÂßãÂêóÔºü  ÔºüÔºüÔºÅÂí≥Âí≥Ôø£ Ôø£)œÉÁ¨¨‰∏ÄÊ¨°Á©øÊ±âÊúçÂá∫Èó®üéÄüíûÂºÄÂøÉLaughing'
    print(full_tokenizer.tokenize(text))

    text = 'Âè∞ÊπæÁöÑÂÖ¨ËßÜ‰ªäÂ§©‰∏ªÂäûÁöÑÂè∞ÂåóÂ∏ÇÈïø Candidate  Defence  Ôºå'
    print(full_tokenizer.tokenize(text))
    #['Âè∞', 'Êπæ', 'ÁöÑ', 'ÂÖ¨', 'ËßÜ', '‰ªä', 'Â§©', '‰∏ª', 'Âäû', 'ÁöÑ', 'Âè∞', 'Âåó', 'Â∏Ç', 'Èïø', 'can', '##di', '##da', '##te', 'de', '##fe', '##nce', 'Ôºå']

    text = 'Candidate'
    print(full_tokenizer.tokenize(text))
    # ['can', '##di', '##da', '##te']

    text = '  Defence  Ôºå'
    print(full_tokenizer.tokenize(text))

    text1 = '''Ê§çÁâ©Á†îÁ©∂ÊâÄÊâÄÈï∑Âë®ÊòåÂºòÂÖàÁîüÁï∂ÈÅ∏Á¨¨‰∏â‰∏ñÁïåÁßëÂ≠∏Èô¢ÔºàÔº¥ÔΩàÔΩÖ„ÄÄÔº¥ÔΩàÔΩâÔΩíÔΩÑ„ÄÄÔº∑ÔΩèÔΩíÔΩåÔΩÑ„ÄÄÔº°ÔΩÉÔΩÅÔΩÑÔΩÖÔΩçÔΩô„ÄÄÔΩèÔΩÜ„ÄÄÔº≥ÔΩÉÔΩâÔΩÖÔΩéÔΩÉÔΩÖÔΩìÔºåÁ∞°Á®±Ôº¥Ôº∑Ôº°Ôº≥Ôºâ
    Èô¢Â£´„ÄÇÔº¥Ôº∑Ôº°Ôº≥‰øÇ‰∏Ä‰πùÂÖ´‰∏âÂπ¥Áî±Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄÔº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄÔº≥ÔΩÅÔΩåÔΩÅÔΩçÔºàÂ∑¥Âü∫ÊñØÂù¶Á±çÔºåÊõæÁç≤Ë´æË≤ùÁàæÁçéÔºâÁôºËµ∑ÊàêÁ´ãÔºåÊúÉÂì°ÈÅç‰ΩàÔºñÔºìÂÄãÂúãÂÆ∂ÔºåÁõÆÂâçÁî±ÔºíÔºìÔºí‰ΩçÈô¢Â£´
    ÔºàÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÂèäÔº¶ÔΩèÔΩïÔΩéÔΩÑÔΩâÔΩéÔΩá„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâÔºåÔºñÔºñ‰ΩçÂçîÈô¢Â£´ÔºàÔº°ÔΩìÔΩìÔΩèÔΩÉÔΩâÔΩÅÔΩîÔΩÖ„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâÔºíÔºî‰ΩçÈÄö‰ø°Èô¢Â£´ÔºàÔº£ÔΩèÔΩíÔΩíÔΩÖÔΩìÔΩêÔΩèÔΩéÔΩÑÔΩâÔΩéÔΩá„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâ
    „ÄÄÂèäÔºí‰ΩçÈÄö‰ø°ÂçîÈô¢Â£´ÔºàÔº£ÔΩèÔΩíÔΩíÔΩÖÔΩìÔΩêÔΩèÔΩéÔΩÑÔΩâÔΩéÔΩá„ÄÄÔº°ÔΩìÔΩìÔΩèÔΩÉÔΩâÔΩÅÔΩîÔΩÖ„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩóÔºâÁµÑÊàêÔºà‰∏çÂåÖÊã¨‰∏Ä‰πù‰πùÂõõÂπ¥Áï∂ÈÅ∏ËÄÖÔºâÔºåÊùéÊîøÈÅì„ÄÅÊ•äÊåØÂØß„ÄÅ‰∏ÅËÇá‰∏≠„ÄÅ
    ÊùéÈÅ†Âì≤„ÄÅÈô≥ÁúÅË∫´„ÄÅÂê≥ÂÅ•ÈõÑ„ÄÅÊú±Á∂ìÊ≠¶„ÄÅËî°ÂçóÊµ∑Á≠âÈô¢Â£´ÂùáÁÇ∫Ë©≤Èô¢Ôº°ÔΩìÔΩìÔΩèÔΩÉÔΩâÔΩÅÔΩîÔΩÖ„ÄÄÔº¶ÔΩÖÔΩåÔΩåÔΩèÔΩó„ÄÇÊú¨Èô¢Êï∏ÁêÜÁµÑÈô¢Â£´„ÄÅÂìà‰ΩõÂ§ßÂ≠∏Êï∏Â≠∏Á≥ªÊïôÊéà‰∏òÊàêÊ°êÔºåÁ∂ìÁëûÂÖ∏ÁöáÂÆ∂ÁßëÂ≠∏Èô¢Ë©ïÂÆöÁÇ∫
    ‰∏Ä‰πù‰πùÂõõÂπ¥ÂÖãÂàó‰ΩõÔºàÔº£ÔΩíÔΩÅÔΩÜÔΩèÔΩè„ÄÄÔº∞ÔΩíÔΩâÔΩöÔΩÖÔºâÁçéÂæó‰∏ªÔºåËóâ‰ª•Ë°®ÂΩ∞ÂÖ∂Âú®ÂæÆÂàÜÂπæ‰ΩïÈ†òÂüüÂΩ±ÈüøÊ∑±ÈÅ†‰πãË≤¢Áçª„ÄÇ'''
    print(full_tokenizer.tokenize(text1))

    text = 'Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄÔº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄÔº≥ÔΩÅÔΩåÔΩÅÔΩç'
    print(full_tokenizer.tokenize(text))

    text = ' Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄÔº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄÔº≥ÔΩÅÔΩåÔΩÅÔΩç '
    print(full_tokenizer.tokenize(text))

    text = '( Ôº∞ÔΩíÔΩèÔΩÜ„ÄÄ Ôº°ÔΩÑÔΩÇÔΩïÔΩì„ÄÄ Ôº≥ÔΩÅÔΩåÔΩÅÔΩç  ) '
    print(full_tokenizer.tokenize(text))

    text = 'ÔºñÔºìÂÄãÂúãÂÆ∂'
    print(full_tokenizer.tokenize(text))

    text = 'ÔºñÔºì ÂÄãÂúãÂÆ∂'
    print(full_tokenizer.tokenize(text))

    text = 'ÔºíÔºîÔºíÔºî‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = 'ÔºíÔºîÔºíÔºî ‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = '2424 ‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = '2424‰ΩçÈÄö‰ø°Èô¢Â£´'
    print(full_tokenizer.tokenize(text))

    text = '‰∏ã‰∏ÄÊ≥¢DVDÂèäÁî®‰∫éÂèØÊê∫ÂºèÂ∞èÂûãËµÑËÆØÁî®ÂìÅÁöÑÂæÆÂûãÂÖâÁ¢üÔºàMinidiskÔºâÔºå‰πüÂ∑≤Ëø´‰∏çÂèäÂæÖÂú∞Á≠âÁùÄÊï≤ÂºÄÊ∂àË¥πËÄÖÁöÑËç∑ÂåÖ„ÄÇ'
    print(full_tokenizer.tokenize(text))

    test_text = 'Á¨¨‰∏â‰∏ñÁïåÁßëÂ≠∏Èô¢ÔºàÔº¥ÔΩàÔΩÖ„ÄÄÔº¥ÔΩàÔΩâÔΩíÔΩÑ„ÄÄÔº∑ÔΩèÔΩíÔΩåÔΩÑ„ÄÄÔº°ÔΩÉÔΩÅÔΩÑÔΩÖÔΩçÔΩô„ÄÄÔΩèÔΩÜ„ÄÄÔº≥ÔΩÉÔΩâÔΩÖÔΩéÔΩÉÔΩÖÔΩìÔºåÁ∞°Á®±Ôº¥Ôº∑Ôº°Ôº≥Ôºâ'
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    sep_tokens = basic_tokenizer.tokenize(test_text)
    print('Basic:')
    for tt in sep_tokens:
        if basic_tokenizer._is_chinese_char(ord(tt[0])):
            wt = 'C'
        elif basic_tokenizer._is_english_char(ord(tt[0])):
            wt = 'E'
        else:
            wt = 'O'

        print(tt+' '+wt)

def check_english(w):
    import re

    english_check = re.compile(r'[a-z]')

    if english_check.match(w):
        print("english", w)
    else:
        print("other:", w)

def test_pandas_drop():
    import pandas as pd
    import os

    data_dir = '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data'
    df = pd.read_csv(os.path.join(data_dir, "test_code.tsv"), sep='\t')

    # full_pos (chunk), ner, seg, text
    df.drop(['full_pos', 'ner'], axis=1)

    # change name to tag for consistently processing
    df.rename(columns={'seg': 'label'}, inplace=True)

    print(len(df.full_pos))

def test_pandas_drop_syn():
    import pandas as pd
    import numpy as np

    df = pd.DataFrame(np.arange(12).reshape(3,4), columns=['A', 'B', 'C', 'D'])
    print(df)

    # need parameter inplace=True
    df.drop(['B', 'C'], axis=1, inplace=True)
    print(df)

def test_metrics():
    from src.metrics import get_ner_BMES, get_ner_BIO, get_ner_fmeasure, reverse_style
    label_list_BMES = "O O O O O O O O O O O O B-PER E-PER O O O O O O O O O O O O O O O O O O O O O O O"
    label_list_BIO = "O B-PER I-PER O O O B-PER O O B-ORG I-ORG I-ORG"

    label_list_BMES = label_list_BMES.split()
    #get_ner_BMES(label_list_BMES)

    #label_list_BIO = label_list_BIO.split()
    ll_BIO1 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG']
    ll_BIO2 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']

    print(get_ner_BIO(ll_BIO1))
    print(get_ner_BIO(ll_BIO2))

def test_CWS_Dict():
    from src.utilis import CWS_Dict
    cws_dict = CWS_Dict()

    sent = 'ËøàÂêë  ÂÖÖÊª°  Â∏åÊúõ  ÁöÑ  Êñ∞  ‰∏ñÁ∫™  ‚Äî‚Äî  ‰∏Ä‰πù‰πùÂÖ´Âπ¥  Êñ∞Âπ¥  ËÆ≤ËØù  Ôºà  ÈôÑ  ÂõæÁâá  Ôºë  Âº†  Ôºâ\n  ' \
           '‰∏≠ÂÖ±‰∏≠Â§Æ  ÊÄª‰π¶ËÆ∞  „ÄÅ  ÂõΩÂÆ∂  ‰∏ªÂ∏≠  Ê±ü  Ê≥ΩÊ∞ë\n' \
           'Ôºà  ‰∏Ä‰πù‰πù‰∏ÉÂπ¥  ÂçÅ‰∫åÊúà  ‰∏âÂçÅ‰∏ÄÊó•  Ôºâ \n ' \
           'ÔºëÔºíÊúà  ÔºìÔºëÊó•  Ôºå  ‰∏≠ÂÖ±‰∏≠Â§Æ  ÊÄª‰π¶ËÆ∞  „ÄÅ  ÂõΩÂÆ∂  ‰∏ªÂ∏≠  Ê±ü  Ê≥ΩÊ∞ë  ÂèëË°®  ÔºëÔºôÔºôÔºòÂπ¥  Êñ∞Âπ¥  ËÆ≤ËØù  ' \
           '„Ää  ËøàÂêë  ÂÖÖÊª°  Â∏åÊúõ  ÁöÑ  Êñ∞  ‰∏ñÁ∫™  „Äã  „ÄÇ  Ôºà  Êñ∞ÂçéÁ§æ  ËÆ∞ËÄÖ  ÂÖ∞  Á∫¢ÂÖâ  ÊëÑ  Ôºâ\n' \
           'ÂêåËÉû  ‰ª¨  „ÄÅ  ÊúãÂèã  ‰ª¨  „ÄÅ  Â•≥Â£´  ‰ª¨  „ÄÅ  ÂÖàÁîü  ‰ª¨  Ôºö\n ' \
           'Âú®  ÔºëÔºôÔºôÔºòÂπ¥  Êù•‰∏¥  ‰πãÈôÖ  Ôºå  Êàë  ÂçÅÂàÜ  È´òÂÖ¥  Âú∞  ÈÄöËøá  ‰∏≠Â§Æ  ‰∫∫Ê∞ë  ÂπøÊí≠  ÁîµÂè∞  „ÄÅ' \
           '  ‰∏≠ÂõΩ  ÂõΩÈôÖ  ÂπøÊí≠  ÁîµÂè∞  Âíå  ‰∏≠Â§Æ  ÁîµËßÜÂè∞  Ôºå  Âêë  ÂÖ®ÂõΩ  ÂêÑÊóè  ‰∫∫Ê∞ë  Ôºå  Âêë  È¶ôÊ∏Ø  ÁâπÂà´  Ë°åÊîøÂå∫  ' \
           'ÂêåËÉû  „ÄÅ  Êæ≥Èó®  Âíå  Âè∞Êπæ  ÂêåËÉû  „ÄÅ  Êµ∑Â§ñ  ‰æ®ËÉû  Ôºå  Âêë  ‰∏ñÁïå  ÂêÑÂõΩ  ÁöÑ  ÊúãÂèã  ‰ª¨  Ôºå  Ëá¥‰ª•  ËØöÊåö  ÁöÑ  ' \
           'ÈóÆÂÄô  Âíå  ËâØÂ•Ω  ÁöÑ  Á•ùÊÑø  ÔºÅ  '

    q_num = cws_dict._findNum(sent)
    print(list(q_num.queue))

    sent = '‰∫∫ÊÇ¨ÊåÇÂú®ÂçäÁ©∫‰∏≠Â≠§Á´ãÊó†Êè¥ÔºåËÄå‰ªñÁöÑËÑö‰∏ãÂ∞±ÊòØ‰∏á‰∏àÊ∑±Ê∏ä„ÄÇ\n' \
           '‰ΩÜÁî±‰∫éÊãÖÂøÉÂç∑Ëµ∑ÁöÑÊ∞îÊµ™‰ºöÊääÈ©¨‰øÆËøû‰∫∫Â∏¶‰ºûÂêπËêΩÊÇ¨Â¥ñÔºåÁõ¥ÂçáÊú∫Êó†Ê≥ïÁõ¥Êé•ÂÆûÊñΩÁ©∫‰∏≠ÊïëÊè¥„ÄÇ\n' \
           'ÊïëÊè¥‰∫∫ÂëòÂè™ËÉΩÂÄüÂä©Áõ¥ÂçáÊú∫Áôª‰∏äÊÇ¨Â¥ñÈ°∂Á´ØÔºåÂ∞ÜÁª≥Á¥¢ÊâîÁªôÈ©¨‰øÆËøõË°åËê•Êïë„ÄÇ\n' \
           'Âú®ÁªèÂéÜ‰∫Ü‰∏ÄÁï™Âë®Êäò‰πãÂêéÔºåÈ©¨‰øÆÁªà‰∫éË¢´ÊïëÊè¥‰∫∫ÂëòÊãâ‰∏ä‰∫ÜÊÇ¨Â¥ñÔºå' \
           'Âπ∏ËøêÁöÑÊòØÁî±‰∫éËê•ÊïëÂèäÊó∂ÔºåÈ©¨‰øÆÊú¨‰∫∫Âπ∂Êó†Â§ßÁ¢ç„ÄÇ‰∏≠Â§ÆÂè∞ÁºñËØëÊä•ÈÅì„ÄÇ\n' \
           'Â•ΩÔºåËøôÊ¨°‰∏≠ÂõΩÊñ∞ÈóªËäÇÁõÆÂ∞±Âà∞ËøôÔºåÊàëÊòØÂæê‰øêÔºåË∞¢Ë∞¢Â§ßÂÆ∂„ÄÇ\n' \
           'Êé•‰∏ãÊù•ÊòØÁî±Áéã‰∏ñÊûó‰∏ªÊåÅÁöÑ‰ªäÊó•ÂÖ≥Ê≥®ËäÇÁõÆ„ÄÇ\n' \
           'ÂêÑ‰ΩçËßÇ‰ºóÔºåÂÜçËßÅ„ÄÇ\n' \
           ' Ôº•Ôº≠Ôº∞Ôº¥Ôºπ'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))

    # some problems are here
    sent = 'Ôº•Ôº≠Ôº∞Ôº¥Ôºπ'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))


def test_pkuseg():
    from src.metrics import getChunks, getFscore
    tag_list = ['BBIBBIIBIIIB', 'BBBBIBBBIIIB']
    #tag_list = ['S,BI,S,BII,BIII,S,S,S,S,BI,S']

    tmp_list = [','.join(tag_list[i]) for i in range(len(tag_list))]
    print(getChunks(tmp_list)) # ['B*1*0,B*2*1,B*1*3,B*3*4,B*4*7,B*1*11,', 'B*1*0,B*1*1,B*1*2,B*2*3,B*1*5,B*1*6,B*4*7,B*1*11,']


    tag_to_idx = {'B': 0, 'I': 1, 'O': 2}
    idx_to_chunk_tag = {}

    '''
    for tag, idx in tag_to_idx.items():
        if tag.startswith("I"):
            tag = "I"
        if tag.startswith("O"):
            tag = "O"
        idx_to_chunk_tag[idx] = tag
    '''

    idx_to_chunk_tag = {}
    tag_to_idx = {'B': 0, 'M': 1, 'E': 2, 'S': 3, '[START]': 4, '[END]': 5}
    BIO_tag_to_idx = {'B': 0, 'I': 1, 'O': 2, '[START]': 3, '[END]': 4}
    token_list = [''.join(str(BIO_tag_to_idx[item])+',') for i in range(len(tag_list)) for item in tag_list[i] ]

    idx_to_chunk_tag = idx_to_tag(BIO_tag_to_idx)

    token_list = []
    for i in range(len(tag_list)):
        t = ''
        for item in tag_list[i]:
            t += ''.join(str(BIO_tag_to_idx[item])+',')
        token_list.append(t)


    goldTagList = [token_list[0]]
    resTagList = [token_list[1]]

    scoreList, infoList = getFscore(goldTagList, resTagList, idx_to_chunk_tag)
    # ValueError: invalid literal for int() with base 10: 'B'
    print(scoreList)
    print(infoList)


def calSize(H, vs, mpe, L):
    for l in L:
        # embedding: (vs+mpe)*H; # Query, Key, value: 3*H*H; Intermediate: 4*H *H; Pooler: H*H
        sz = (vs+mpe)*H + ((3+4)*H*H)*l + H*H
        print('# layer: '+str(l)+', #para: '+str(sz))


def verifyModelSize():
    H = 768
    vs = 21128
    mpe = 512
    L = [3, 6, 12]

    num_model_para = calSize(H, vs, mpe, L)

'''
def test_parse_one2BERTformat():
    from OntoNotes.f6_generate_training_data import parse_one2BERT2Dict
    s = '(NP (CP (IP (NP (DNP (NER-GPE (NR Taiwan)) (DEG ÁöÑ)) (NER-ORG (NR ÂÖ¨ËßÜ))) (VP (NT ‰ªäÂ§©) (VV ‰∏ªÂäû))) (DEC ÁöÑ)) (NP-m (NP (NR Âè∞Âåó) (NN Â∏ÇÈïø)) (NP-m (NP (NN candidate) (NN defence)) (PU Ôºå))))'
    out_dict = parse_one2BERT2Dict(s)
    print('src_seg:'+out_dict['src_seg'])
    print('src_ner:'+out_dict['src_ner'])
    print('full_pos:'+out_dict['full_pos'])
    print('text:'+out_dict['text'])
    print('text_seg:'+out_dict['text_seg'])
    print('bert_seg:'+out_dict['bert_seg'])
    print('bert_ner:'+out_dict['bert_ner'])
'''

def set_local_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/4ner_data/',
            #'bert_model_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data/eval/2019_3_12/models/',
            'vocab_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/models.txt',
            'bert_config_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/bert_config.json',
            'output_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/rs/nhl3/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 64,
            'num_hidden_layers': 3,
            'init_checkpoint': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/',
            'bert_model': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/models/nhl3/weights_epoch03.pt',
            'override_output': True,
            'tensorboardWriter': False
            }


def test_load_model():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    label_list = ['B', 'M', 'E', 'S', '[START]', '[END]']
    model, device = load_model(label_list, args)

    save_model(model, args.output_dir + 'tmp.tsv')


def test_dataloader():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    processors = {
        "ontonotes_cws": lambda: CWS_BMEO(nopunc=args.nopunc),
    }

    task_name = args.task_name.lower()

    # Prepare models
    processor = processors[task_name]()
    train_dataset, train_dataloader = get_dataset_and_dataloader(processor, args, training=False, type = 'tmp_test')

    eval_dataloaders = get_ontonotes_eval_dataloaders(processor, args)

    for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
        input_ids, segment_ids, input_mask = batch[:3]
        label_ids = batch[3:] if len(batch[3:])>1 else batch[3]


def decode_iter(logits, attention_mask):
    mask = attention_mask.byte()
    batch_size, seq_length = mask.shape

    best_tags_list = []
    for idx in range(batch_size):
        # Find the tag which maximizes the score at the last timestep; this is our best tag
        # for the last timestep
        best_tags = []
        for iseq in range(seq_length):
            if mask[idx, iseq]:
                _, best_selected_tag = logits[idx, iseq].max(dim=0)
                best_tags.append(best_selected_tag.item())

        best_tags_list.append(best_tags)
    return best_tags_list


def decode_batch(logits, attention_mask):
    mask = attention_mask.byte()
    batch_size, seq_length = mask.shape

    _, best_selected_tag = logits.max(dim=2)

    best_tags_list = []
    for n in range(batch_size):
        selected_tag = torch.masked_select(best_selected_tag[n, :], mask[n, :])
        best_tags_list.append(selected_tag.tolist())

    return best_tags_list


def test_decode():
    n_sample = 32
    n_len = 8
    n_tag = 6

    logits = torch.rand((n_sample, n_len, n_tag))

    mask = [1]*(n_len//2) + [0]*(n_len//2)
    attention_mask = torch.ByteTensor([mask]*n_sample)

    tm = time.time()
    lit = decode_iter(logits, attention_mask)
    print('time: ' + str(time.time()-tm))
    print(lit)

    tm = time.time()
    lbt = decode_batch(logits, attention_mask)
    print('time: ' + str(time.time()-tm))
    print(lbt)


def test_split():
    import re

    text = '‰ΩÜÊòØÔºåËßÑÊ®°Â§ß‰∏çÁ≠â‰∫éËßÑÊ®°ÁªèÊµé„ÄÇËøôÂèØ‰ª•‰ªé‰∏â‰∏™ÊñπÈù¢ËÄÉÂØüÔºöÔºàÔºëÔºâÁîü‰∫ßËÉΩÂäõÁöÑÈôêÂ∫¶„ÄÇÊäïÂÖ•Â¢ûÂä†Ë∂ÖËøá‰∏ÄÂÆöÁÇπÔºå‰∫ßÂá∫ÁöÑÂ¢ûÈáèÊàñËæπÈôÖ‰∫ßÂá∫Â∞Ü‰ºöÂáèÂ∞ëÔºåÂá∫Áé∞ËßÑÊ®°Êä•ÈÖ¨ÈÄíÂáèÁé∞Ë±°„ÄÇÔºàÔºíÔºâ‰∫§ÊòìÊàêÊú¨ÁöÑÈôêÂ∫¶Ôºå‰∏ªË¶ÅÊòØ‰ºÅ‰∏öÂÜÖÈÉ®‰∫§ÊòìÊàêÊú¨‚Äî‚Äî‚ÄîÈÄöÂ∏∏Áß∞‰∏∫ÁÆ°ÁêÜÊàêÊú¨‚Äî‚Äî‚ÄîÈôêÂà∂„ÄÇ‰ºÅ‰∏ö‰πãÊâÄ‰ª•Êõø‰ª£Â∏ÇÂú∫Â≠òÂú®ÔºåÊòØÂõ†‰∏∫ÈÄöËøáÂ∏ÇÂú∫‰∫§ÊòìÊòØÈúÄË¶ÅÊàêÊú¨ÁöÑÔºåÂ¶ÇÊêúÂØªÂêàÈÄÇ‰∫ßÂìÅ„ÄÅË∞àÂà§„ÄÅÁ≠æÁ∫¶„ÄÅÁõëÁù£ÊâßË°åÁ≠âÔºåÈÉΩÈúÄË¶ÅËä±Ë¥πÊàêÊú¨ÔºåÂú®‰∏Ä‰∫õÊÉÖÂÜµ‰∏ãÔºå‰ºÅ‰∏öÂ∞Ü‰∏Ä‰∫õÁªèÊµéÊ¥ªÂä®ÂÜÖÈÉ®ÂåñÔºåÈÄöËøáË°åÊîøÊùÉÂ®ÅÂä†‰ª•ÁªÑÁªáÔºåËÉΩÂ§üËäÇÁ∫¶Â∏ÇÂú∫‰∏äÁöÑ‰∫§ÊòìÊàêÊú¨„ÄÇ‰ºÅ‰∏öÂÜÖÈÉ®ÂçèË∞É‰∏ÄËà¨ÈÄöËøáÂ±ÇÁ∫ßÂà∂ÁªìÊûÑËøõË°åÔºå‰πüÈúÄË¶Å‰∏ÄÂÆöÁöÑË¥πÁî®ÔºåËøôÁßçË¥πÁî®‰πÉÊòØ‰ºÅ‰∏öÂÜÖÈÉ®ÂèëÁîüÁöÑ‰∫§ÊòìÊàêÊú¨„ÄÇÂ¶ÇÊûúÁÆ°ÁêÜÂπÖÂ∫¶ËøáÂ§ßÔºåÊàñËÄÖÂ±ÇÊ¨°Â§™Â§öÔºå‰ªéÂü∫Â±ÇÂà∞‰∏≠ÂøÉÂÜ≥Á≠ñËÄÖÁöÑ‰ø°ÊÅØ‰º†ÈÄíÈÄüÂ∫¶Â∞±‰ºöÂèòÊÖ¢ÔºåÁîöËá≥‰ø°Âè∑Â§±ÁúüÔºåËá¥‰Ωø‰ºÅ‰∏öÊïàÁéáÈôç‰ΩéÔºåÂá∫Áé∞ËßÑÊ®°‰∏çÁªèÊµé„ÄÇÁªÑÁªáÁÆ°ÁêÜÂΩ¢ÂºèÁöÑÂèòÂä®ÔºåÂ¶ÇÂÆûË°å‰∫ã‰∏öÈÉ®Âà∂Á≠âÔºåËÉΩÂ§üÊîπÂèò‰ø°ÊÅØ‰º†ÈÄíÁöÑÈÄüÂ∫¶Âíå‰ø°ÊÅØË¥®ÈáèÔºåÊîπÂñÑÂÜ≥Á≠ñÊ∞¥Âπ≥Ôºå‰ªéËÄåÊãâÈïøËßÑÊ®°ÁªèÊµéÂ≠òÂú®ÁöÑÊó∂Èó¥Ë∑®Â∫¶„ÄÇ‰ΩÜËøô‰∏çÊòØÊ≤°ÊúâÈôêÂ∫¶ÁöÑ„ÄÇÔºàÔºìÔºâÂØπÊäÄÊúØËøõÊ≠•ÁöÑÈôêÂà∂ÔºåËøôÂú®Âá∫Áé∞ÂûÑÊñ≠ÊÉÖÂΩ¢Êó∂Â∞§ÂÖ∂Â¶ÇÊ≠§„ÄÇÈöèÁùÄ‰ºÅ‰∏öËßÑÊ®°Êâ©Â§ßÔºåÂú®Â∏ÇÂú∫‰∏≠ÁöÑÂûÑÊñ≠ÂäõÈáèÁöÑÂ¢ûÂº∫ÔºåÂ∏ÇÂú∫Â∞ÜÂÅèÁ¶ªÂÖÖÂàÜÁ´û‰∫âÊó∂ÁöÑÂùáË°°ÔºåÂûÑÊñ≠ËÄÖÂ∞ÜÈÄöËøáÂûÑÊñ≠ÂÆö‰ª∑ÂíåËøõÂÖ•Â£ÅÂûíÈôêÂà∂Á´û‰∫âËÄÖÔºåËµöÂèñÂûÑÊñ≠Âà©Ê∂¶„ÄÇÊ≠§Êó∂‰ºÅ‰∏öËøΩÊ±ÇÂàõÈÄ†„ÄÅËøΩÊ±ÇÊäÄÊúØËøõÊ≠•ÁöÑÂéãÂäõÂíåÂä®ÂäõÂ∞Ü‰ºöÂáèÂº±„ÄÇËøôÂú®‰∏Ä‰∏™Ë°å‰∏öÂè™Êúâ‰∏Ä‰∏™‰ºÅ‰∏öÁöÑÂÆåÂÖ®ÂûÑÊñ≠ÔºàÁã¨Âç†ÔºâÊÉÖÂΩ¢‰∏≠ÊúÄ‰∏∫ÊòéÊòæ„ÄÇ‰πüÊ≠£Âõ†‰∏∫Ëøô‰∏ÄÁÇπÔºå‰∏ªË¶ÅÂ∏ÇÂú∫ÁªèÊµéÂõΩÂÆ∂ÁöÑÂèçÂûÑÊñ≠Ê≥ïÈÉΩÊûÅÂäõÈôêÂà∂ÂûÑÊñ≠Á®ãÂ∫¶Ôºå‰∏çÂÖÅËÆ∏‰∏Ä‰∏™Ë°å‰∏öÂè™Êúâ‰∏Ä‰∏™‰ºÅ‰∏öÔºàÂéÇÂïÜÔºâ„ÄÇÁâπÂà´ÊòØÔºåÂú®Êñ∞ÁöÑÁßëÊäÄÈù©ÂëΩÈù¢ÂâçÔºåÂ∞è‰ºÅ‰∏ö‰πüÂõ†ÂÖ∂ËÉΩÂ§üÁÅµÊ¥ªÂú∞Èù¢ÂØπÂ∏ÇÂú∫„ÄÅÂØåÊúâÂàõÈÄ†ÂäõËÄåÊòæÁ§∫Âá∫ÁîüÂëΩÂäõÔºåÂ§ß‰ºÅ‰∏öÂèçËÄåÂèØËÉΩÂØπÂ∏ÇÂú∫ÂèòÂåñÂèçÂ∫îËøüÁºìËÄåÂ§Ñ‰∫éÁ´û‰∫âÂä£Âäø„ÄÇÊÄª‰πãÔºåËßÑÊ®°ÁªèÊµéÂåÖÂê´ÁöÑÊòØ‰∏Ä‰∏™ÈÄÇÂ∫¶ËßÑÊ®°„ÄÅÊúâÊïàËßÑÊ®°ÔºåÊó¢‰∏çÊòØË∂äÂ§ßË∂äÁªèÊµéÔºå‰πü‰∏çÊòØË∂äÂ∞èË∂äÁªèÊµé„ÄÇ'
    at = re.split('(„ÄÇ|Ôºå|Ôºö|\n|#)', text)
    print(at)


def test_write():
    with open('tmp.txt', 'w+') as f:
        f.write('abc\n')
        f.write('dd')
        f.write(' ee')
        f.write('\n abc')

    with open('tmp.txt', 'r') as f:
        line = f.read()
        while line != '':
            print(line)
            line = f.read()


def test_construct_pos_tags():
    from src.preprocess import construct_pos_tags

    pos_tags_file = '../resource/pos_tags.txt'
    pos_label_list, pos_label_map, pos_idx_to_label_map = \
        construct_pos_tags(pos_tags_file, mode = 'BIO')

    print(pos_label_list)
    print(pos_label_map)
    print(pos_idx_to_label_map)


def test_outputPOSFscoreUsedBIO():
    goldTagList = [[0, 62, 63, 62, 63, 104, 105, 85, 85, 104, 105, 65, 66, 85, 104, 105, 105, 105, 1, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 85, 65, 66, 66, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 79, 62, 63, 62, 63, 79, 62, 63, 62, 63, 104, 105, 104, 105, 53, 54, 85, 104, 105,
                    79, 68, 69, 62, 63, 22, 65, 66, 65, 66, 62, 63, 62, 63, 62, 63, 85, 4, 79, 68, 69, 55,
                    4, 104, 105, 47, 48, 62, 63, 85, 2, 3, 2, 3, 65, 66, 62, 63, 62, 63, 62, 63, 4, 106,
                    4, 106, 85, 104, 105, 65, 66, 62, 63, 25, 62, 63, 85, 4, 4, 106, 62, 63, 10, 62, 63,
                    106, 79, 47, 48, 25, 62, 63, 62, 63, 55, 85, 47, 48, 48, 25, 62, 63, 62, 63, 13, 62,
                    63, 62, 63, 4, 104, 105, 62, 63, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 65, 66, 65, 66, 62, 63, 63, 62, 63, 62, 63, 71, 72, 62, 63, 62, 63, 62, 63, 85, 79,
                    68, 69, 68, 69, 69, 104, 105, 105, 105, 85, 79, 62, 63, 104, 105, 14, 15, 15, 56, 57,
                    104, 105, 55, 85, 104, 105, 7, 14, 15, 15, 58, 62, 63, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0]]

    input_mask = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]

    preTagList = [[0, 62, 63, 62, 63, 63, 63, 85, 85, 62, 63, 65, 66, 85, 105, 4, 105, 104, 1],
                  [0, 85, 65, 66, 63, 85, 1],
                  [0, 79, 66, 66, 34, 63, 79, 63, 16, 62, 63, 104, 105, 4, 63, 62, 63, 85, 104, 105, 79,
                   16, 69, 65, 69, 25, 62, 66, 66, 63, 62, 63, 62, 63, 62, 63, 85, 104, 105, 68, 69, 63,
                   4, 104, 105, 104, 105, 62, 105, 85, 104, 55, 104, 105, 65, 66, 62, 63, 66, 66, 62, 63,
                   104, 105, 104, 105, 85, 104, 105, 65, 66, 62, 63, 22, 62, 63, 85, 4, 4, 63, 62, 81, 4,
                   62, 63, 105, 79, 2, 16, 22, 104, 104, 62, 63, 55, 85, 62, 63, 63, 25, 62, 63, 104, 105,
                   13, 62, 105, 104, 105, 4, 104, 105, 104, 105, 85, 1],
                  [0, 65, 66, 66, 63, 62, 63, 63, 62, 63, 62, 63, 71, 16, 62, 63, 104, 105, 62, 63, 85, 79,
                   68, 69, 69, 15, 69, 105, 16, 63, 105, 85, 79, 66, 63, 104, 105, 15, 15, 15, 62, 58, 62,
                   63, 105, 85, 104, 105, 7, 14, 15, 15, 63, 63, 63, 85, 1]]

    from src.metrics import outputPOSFscoreUsedBIO
    scoreList, infoList = outputPOSFscoreUsedBIO(goldTagList, preTagList, input_mask)
    print(scoreList)
    print(infoList)


def compare_string_directly(text, tag):
    result_str = ''
    for idx in range(len(tag)):
        tt = text[idx]
        tt = tt.replace('##', '')
        ti = tag[idx]

        if int(ti) == 2:  # 'B'
            result_str += ' ' + tt
        elif int(ti) > 3:  # and (cur_word_is_english)
            # int(ti)>1: tokens of 'E' and 'S'
            # current word is english
            result_str += tt + ' '
        else:
            result_str += tt

    return result_str


def compare_string_reference(text, tag):
    result_str = ''
    for idx in range(len(tag)):
        tt = text[idx]
        tt = tt.replace('##', '')
        ti = tag[idx]

        if int(ti) == segType.BMES_label_map['B']:  # 'B'
            result_str += ' ' + tt
        elif int(ti) > segType.BMES_label_map['M']:  # and (cur_word_is_english)
            # int(ti)>1: tokens of 'E' and 'S'
            # current word is english
            result_str += tt + ' '
        else:
            result_str += tt

    return result_str

def check_time():
    import time

    count = 100000
    text_ele = ['ÁîµÂΩ±È¶ñÂèë„ÄÇ']*count
    text = ''.join(text_ele)

    tag_ele = ['24245']*count
    tag = ''.join(tag_ele)

    st = time.time()
    print(compare_string_directly(text, tag))
    print(time.time()-st)

    st = time.time()
    print(compare_string_reference(text, tag))
    print(time.time()-st)


def test_restore_unknown():
    original_str = '‰∏çÂè™Áà±Áø°Áø†  ‰∏ÄÂàáÁè†ÂÆù ÊñáÁé©ÈÉΩÊòØÊàëÁöÑÊúÄÁà±   ‰∏Ä‰∏™Â•≥ÁîüÊà¥ÁùÄÂ§ßÈáëÂàöÂï•ÁöÑÁ°ÆÂÆûÂæàÈú∏Ê∞î#Áî®Áè†ÂÆùËÆ≤ÊïÖ‰∫ã# ‰∫ÜüòÇüòÇ  ÊîπÂ§©ÊãøÂá∫Êù•ÊôíÊôíÂ§™Èò≥  üòù#ÊàëË¶Å‰∏äÂ¢ô# #Á≤æÂìÅÁø°Áø†#'
    result_str = '‰∏ç Âè™ Áà±  Áø°Áø†  ‰∏ÄÂàá  Áè†ÂÆù ÊñáÁé© ÈÉΩ ÊòØ Êàë ÁöÑ ÊúÄ Áà± ‰∏Ä ‰∏™  Â•≥Áîü Êà¥ ÁùÄ  Â§ßÈáëÂàö [UNK] ÁöÑ  Á°ÆÂÆû Âæà  Èú∏Ê∞î # Áî®  Áè†ÂÆù ËÆ≤  ÊïÖ‰∫ã # ‰∫Ü [UNK]  ÊîπÂ§© Êãø  Âá∫Êù•  ÊôíÊôí  Â§™Èò≥ [UNK] # Êàë Ë¶Å ‰∏ä Â¢ô # #  Á≤æÂìÅ  Áø°Áø† # '
    result_pos = 'AD AD VV NR DT NN NN AD VC PN DEG AD NN CD M NN VV AS NN PU X X AD VA PU P NN VV NN PU AS PU VV VV VV VV NN PU PU PN VV VV NN PU PU NN NN PU '

    seg_ls, pos_ls = restore_unknown_tokens_without_unused_with_pos(original_str, result_str, result_pos)
    print(seg_ls)
    print(pos_ls)


def test_tokenize_list_with_cand_indexes():
    max_length = 128

    vocab_file = '../src/BERT/models/multi_cased_L-12_H-768_A-12/vocab.txt'
    do_lower_case = True
    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)
    lword = [['‰∏ç', 'Âè™', 'Áà±', 'Áø°', 'Áø†', '‰∏Ä', 'Âàá', 'Áè†', 'ÂÆù', 'Êñá', 'Áé©', 'ÈÉΩ', 'ÊòØ', 'Êàë', 'ÁöÑ', 'ÊúÄ', 'Áà±', '‰∏Ä', '‰∏™', 'Â•≥', 'Áîü', 'Êà¥', 'ÁùÄ', 'Â§ß', 'Èáë', 'Âàö', '[UNK]', 'ÁöÑ', 'Á°Æ', 'ÂÆû', 'Âæà', 'Èú∏', 'Ê∞î', '#', 'Áî®', 'Áè†', 'ÂÆù', 'ËÆ≤', 'ÊïÖ', '‰∫ã', '#', '‰∫Ü', '[UNK]', 'Êîπ', 'Â§©', 'Êãø', 'Âá∫', 'Êù•', 'Êôí', 'Êôí', 'Â§™', 'Èò≥', '[UNK]', '#', 'Êàë', 'Ë¶Å', '‰∏ä', 'Â¢ô', '#', '#', 'Á≤æ', 'ÂìÅ', 'Áø°', 'Áø†', '#'], ['#', 'Áªò', 'Áîª', 'Êú∫', 'Âô®', '‰∫∫', '#', '‚äô', '##‚àÄ', '##‚äô', 'ÔºÅ', 'Âì¶', 'Âéü', 'Êù•', 'Ëá™', 'Â∑±', 'Èïø', 'Êàê', 'Ëøô', 'Ê†∑', 'Â≠ê', 'ÁöÑ', 'Âï¶', 'ÔºÅ'], ['#', 'Áæé', 'È£ü', '#', '‰πã', 'Êù•', 'Ëá™', 'Áßë', 'Âçé', 'Ë∑Ø', 'ÁöÑ', '‰∏≠', '‰∏ú', 'Êñô', 'ÁêÜ', 'Ôºå', 'Ëø™', 'Êãú', 'Êñô', 'ÁêÜ', 'Âë≥', 'ÈÅì', 'Áúü', 'ÂøÉ', 'Âæà', 'ËÆ©', '‰∫∫', 'Â§±', 'Êúõ', 'Âìà', '[UNK]', 'Âë≥', 'ÈÅì', 'Â§™', '‰∏Ä', 'Ëà¨', 'Ôºå', 'Êûú', 'ÁÑ∂', 'Âè™', 'ÊòØ', 'Áæé', 'ËÄå', 'Â∑≤', '„ÄÇ', 'Ëøò', 'ÊòØ', 'Âúü', 'ËÄ≥', 'ÂÖ∂', 'Êñô', 'ÁêÜ', 'Â•Ω', 'ÂêÉ'], ['ÈÄÇ', 'Âêà', 'Â§è', 'Â§©', 'ÁöÑ', 'Â∞ë', 'Â•≥', 'ÊÑü', 'È¶ô', 'Ê∞õ', '[UNK]', 'Ë∂Ö', 'Âπ≥', '‰ª∑', '[UNK]', 'Ë∂Ö', 'Êó•', 'Â∏∏', '[UNK]', '„ÄÇ', '[UNK]', 'Âæà', 'Â§ö', 'ÂÆù', 'ÂÆù', 'Âπ≥', 'Êó∂', 'Âë¢', 'Âíå', 'Èáç', 'Ë¶Å', 'ÁöÑ', '‰∫∫', 'Á∫¶', '‰ºö', 'ÈÉΩ', '‰ºö', 'Êúâ', 'Áßç', 'Âñ∑', 'È¶ô', 'Ê∞¥', 'ÁöÑ', '‰π†', 'ÊÉØ', 'Ôºå', '‰ΩÜ', 'ÊòØ', 'Âë¢', 'Ôºå', 'ÂØπ', '‰∫é', 'Â≠¶', 'Áîü', 'ÂÖö', 'Êù•', 'ËØ¥', 'Ôºå', '‰∏Ä', 'Áì∂', 'È¶ô', 'Ê∞¥', 'ÈÉΩ', 'Âæà', 'Ë¥µ', 'Ôºå', 'Âæà', 'Èöæ', 'Èöè', 'ÁùÄ', 'ÂøÉ', 'ÊÉÖ', 'Êç¢', '‰∏™', 'Âë≥', 'ÈÅì', 'Ôºå', 'ËÄå', '‰∏î', 'Êúâ', '‰∫õ', 'È¶ô', 'Ê∞¥', 'Âë≥', 'ÈÅì', 'Áâπ', 'Âà´', 'Êµì', 'Ôºå', 'Âæà', 'Â§ö', 'Áõ¥', 'Áî∑', 'Áôå', '‰ª¨', 'Ëßâ', 'Âæó', '‰Ω†', 'Ë∫´', '‰∏ä', 'ÁöÑ', 'Âë≥', 'ÈÅì', 'Áâπ', 'Âà´', 'Âà∫', 'Èºª', '[UNK]', '„ÄÇ', '[UNK]', 'ÊâÄ', '‰ª•', 'Êàë', 'Áé∞', 'Âú®', 'Âñú', 'Ê¨¢', 'Áî®', '‰Ωì', 'È¶ô', 'Âéü', 'Ê∂≤', 'Ôºå', 'Ê∑°', 'Ê∑°', 'ÁöÑ'], ['Ê∏Ö', 'È¶ô', 'Ôºå', 'Ëøû', 'Âá∫', 'Ê±ó', 'ÈÉΩ', 'ÊòØ', 'Èùû', 'Â∏∏', 'Ëá™', 'ÁÑ∂', 'ÁöÑ', '‰Ωì', 'È¶ô', 'Ôºå', 'Ê†π', 'Êú¨', 'Èóª', '‰∏ç', 'Âà∞', 'Áãê', 'Ëá≠', 'Ê±ó', 'Ëá≠', 'Âïä', 'Ôºå', 'ÊØè', 'Ê¨°', 'Âá∫', 'Èó®', 'ÈÉΩ', 'Êê∫', 'Â∏¶', 'Âú®', 'Ë∫´', '‰∏ä', 'ÂèØ', '‰ª•', 'Èöè', 'Êó∂', 'Ê∂Ç', 'Êäπ', 'Ôºå', 'ÊØè', 'Ê¨°', 'Ê∂Ç', 'Êäπ', 'ÈÉΩ', 'Ë¢´', 'ÈóÆ', 'Êàë', 'Êäπ', '‰∫Ü', '‰ªÄ', '‰πà', 'ÈÇ£', '‰πà', 'È¶ô', 'Ôºå', 'Âìà', 'Âìà', 'Âìà', 'Âìà', '‰∏Ä', 'Áõ¥', 'Ë¢´', 'Â§∏', 'È¶ô', 'È¶ô', 'ÁöÑ', '[UNK]', '#', 'Áãê', 'Ëá≠', '#', '#', '‰Ωì', 'È¶ô', '#'], ['ÊØè', '‰∏™', 'Â•≥', 'Â≠©', 'Â≠ê', 'ÈÉΩ', 'ÂèØ', '‰ª•', 'ÊòØ', 'Ê≥¢', 'Â¶û', 'Ôºå', '‰ΩÜ', 'ÈÅá', 'Âà∞', 'ÁöÑ', '‰∏ç', '‰∏Ä', 'ÂÆö', 'ÊòØ', 'ÂÆó', '‰ªã'], ['Ê≥°', '‰∏Ä', 'Ê≥°', 'Ôºå', 'Êç¢', 'Êñ∞', 'ËΩ¶', 'Âï¶', '#', 'Êàë', 'Áà±', 'ÂÆò', 'Êñπ', 'Êàë', 'Áà±', 'ÁÉ≠', 'Èó®', '#', '#', 'ÈÄÜ', 'Ë¢≠', 'Â∞è', '‰ªô', 'Â•≥', '#', '[UNK]'], ['ÁßÄ', 'ÊÅ©', 'Áà±', 'ÂøÖ', 'Â§á', 'Ë∂Ö', 'ÂèØ', 'Áà±', 'ÁöÑ', 'ÊÉÖ', '‰æ£', 'Â£Å', 'Á∫∏', 'Â§¥', 'ÂÉè', '[UNK]', 'Âõæ', 'Á¥†', 'Êùê', '„ÄÇ', 'Áªô', 'Â§ß', 'ÂÆ∂', 'ÂàÜ', '‰∫´', '‰∏Ä', '‰∫õ', 'Ëá™', 'Â∑±', 'Âæà', 'Âñú', 'Ê¨¢', 'ÁöÑ', 'ÊÉÖ', '‰æ£', 'Â£Å', 'Á∫∏', 'Âéü', 'Âõæ', 'ÂèØ', '‰ª•', 'ÊÄù', 'Êàë', '[UNK]', '„ÄÇ', '[UNK]', '[UNK]', 'Âõæ', '‰øÆ', 'Âõæ', '[UNK]', '„ÄÇ', 'Êâì', 'ÂºÄ', 'pi', '##cs', '##art', 'Â∞±', 'ÂèØ', '‰ª•', '‰ªé', 'Áõ∏', 'ÂÜå', 'ÁÖß', 'Áâá', 'Èáå', '[UNK]', 'Âá∫', '‰∏Ä', 'Âº†', 'Â§ß', 'Â§¥', 'Ë¥¥', 'Ôºå', 'ÁÑ∂', 'Âêé', 'Âú®', 'Ëøô', '‰∫õ', 'Âç°', 'ÈÄö', 'Â£Å', 'Á∫∏', 'ÁöÑ', 'Á©∫', 'ÁôΩ', 'Â§Ñ', 'ÂÜç', 'Ë∞É', 'Êï¥', '‰∏Ä', '‰∏ã', '‰∫Æ', 'Â∫¶', 'ÂØπ', 'ÊØî', 'Â∫¶', 'Ëæπ', 'Ê°Ü', '‰ªÄ', '‰πà', 'ÁöÑ', '‰∏Ä', 'Âº†', 'Ë∂Ö', 'Áîú', 'Ë∂Ö', 'Â∞ë', 'Â•≥', 'ÂøÉ', 'ÁöÑ', 'ÊÉÖ', '‰æ£', 'Â§¥', 'ÂÉè', 'Â£Å', 'Á∫∏', 'ËÉå', 'ÊôØ'], ['Âõæ', 'Â∞±', '[UNK]', 'ÁÇâ', 'Âï¶', '~', '„ÄÇ', 'Ëµ∂', 'Á¥ß', 'Âíå', 'Áî∑', 'Êúã', 'Âèã', '‰∏Ä', 'Ëµ∑', 'Áî®', 'Ëµ∑', 'Êù•', 'Âêß', '~', '[UNK]', '#', 'Êúâ', '‰∏™', 'ÊÅã', 'Áà±', 'ÊÉ≥', 'Âíå', '‰Ω†', 'Ë∞à', '#', '#', 'Â£Å', 'Á∫∏', '#', '#', 'ÊÉÖ', '‰æ£', '[UNK]', 'Âõæ', 'Á¥†', 'Êùê', '#', '#', 'ÊÉÖ', '‰æ£', 'Â§¥', 'ÂÉè', '#'], ['#', 'È≠î', 'Ê≥ï', 'ÁÖß', 'Áâá', '#', '#', 'È≠î', 'Ê≥ï', 'Êòü', '‰∫ë', 'ÁÖß', '#', '„ÄÇ', 'Â∞è', 'Êó∂', 'ÂÄô', 'Ôºå', 'ÊÄª', 'ÊòØ', 'Êª°', 'ÂøÉ', 'Âπª', 'ÊÉ≥', 'ÁùÄ', 'Êã•', 'Êúâ', 'ÂÉè', 'Âìà', 'Âà©', 'Ê≥¢', 'Áâπ', '‰∏Ä', 'Ê†∑', 'ÁöÑ', 'È≠î', 'Ê≥ï', 'Ôºå', 'Â∞è', 'Ê£í', '‰∏Ä', 'Êå•', 'Ôºå', 'ËÆ©', 'Ëá™', 'Â∑±', 'Êã•', 'Êúâ', 'Âæà', 'Â§ö', '‰∏ú', 'Ë•ø', 'Ôºõ', 'Èïø', 'Â§ß', 'Âêé', 'Ôºå', 'Êâç', 'Âèë', 'Áé∞', 'Âéü', 'Êù•', 'Ëá™', 'Â∑±', 'Êâç', 'ÊòØ', 'ÊúÄ', 'ÂÄº', 'Âæó', 'Êã•', 'Êúâ', 'ÁöÑ', '[UNK]'], ['‰Ω†', 'Ëøò', 'ËÆ∞', 'Âæó', 'Êàë', '‰πà', 'ÔΩû', 'Êàë', 'ÂèØ', 'ÊòØ', 'Ê∑±', 'Ê∑±', 'ÁöÑ', 'ËÆ∞', 'Âæó', '‰Ω†', '[UNK]', '[UNK]', 'Âúà', '#', '90', 'Âêé', 'ÂÖª', 'Áîü', 'Êó•', 'Â∏∏', '#', '#', 'Á´•', 'Âπ¥', 'ÂÑø', 'Êó∂', 'ÁöÑ', 'Âõû', 'ÂøÜ', '#', 'Âúà', 'Âá∫', '‰Ω†', 'ÁöÑ', 'ÂÑø', 'Êó∂', 'Â∞è', 'Èõ∂', 'È£ü', '„ÄÅ', 'Âõû', 'ÂøÜ', 'ÊùÄ', '[UNK]'], ['Iris', 'ÁΩë', 'Êãç', 'Ê®°', 'Áâπ', 'Âèç', 'È¶à', '[', 'Âòø', 'Âìà', ']', '„ÄÇ', 'ÁΩë', 'Êãç', 'Â∞±', 'ÊòØ', 'ÈÄÅ', 'Á¶è', 'Âà©', 'ÁöÑ', 'Âú∞', 'Êñπ', 'ÂëÄ', 'ÔºÅ', 'ÂÆÉ', '‰∏ç', 'ÊòØ', 'ÂæÆ', 'ÂïÜ', 'Ë¶Å', '‰Ω†', 'Âçñ', 'Ë¥ß', 'Ôºå', 'ËÄå', 'ÊòØ', 'ÁÆÄ', 'Âçï', 'Êãç', 'ÁÖß', 'ÈÄÅ', '‰Ω†', '‰∏Ä', 'Â†Ü', 'Ë¥ß', 'Âíå', '[UNK]', '„ÄÇ', 'ÂÆÉ', 'Á®≥', 'ÂÆö', 'ÂÆâ', 'ÈÄ∏', 'Ôºå', 'Áî®', 'Èó≤', 'Êó∂', 'Â∞±', 'ÂèØ', '‰ª•', 'Ëµö', '#', 'l', '##ris', 'ÁΩë', 'Êãç', '#'], ['#', 'running', '##man'], ['Â§è', 'Êó•', 'Êòæ', 'ÁôΩ', 'È£é', '„ÄÇ', 'Á∫¢', 'Ëâ≤', 'Êòæ', 'ÁôΩ', 'Ôºå', 'Êê≠', 'ÈÖç', '‰∏ä', 'ÊúÄ', 'Ëøë', 'ÊµÅ', 'Ë°å', 'ÁöÑ', 'Êôï', 'Êüì', '„ÄÇ', 'Â•Ω', 'Áúã', 'Âèà', 'Êúâ', 'Â§è', 'Êó•', 'Ê∞î', 'ÊÅØ'], ['Èó≤', 'ÁöÑ', 'Ê≤°', '‰∫ã', 'Ôºå', 'Êîæ', 'Êùæ', '‰∏ã', 'ÂøÉ', 'ÊÉÖ', 'Âêß', 'ÔºÅ'], ['Êú®', 'Á≥†', 'Â∏É', '‰∏Å', '[UNK]', '‰∏ä', 'Ê¨°', 'Âú®', 'Êæ≥', 'Èó®', 'ÂêÉ', '‰∫Ü', 'Âøµ', 'Âøµ', '‰∏ç', 'Âøò', 'Ôºå', '‰ªä', 'Â§©', 'Ëá™', 'Â∑±', 'Âä®', 'Êâã', 'ÂÅö', '‰∫Ü', '„ÄÇ', 'Ë´ã', 'Áµ¶', '‰∏™', 'Ëµû', '„ÄÇ', '#', 'Ëá™', 'Âà∂', 'Áæé', 'È£ü', '#', '#', 'ÁÉò', 'ÁÑô', 'È£ü', 'Ë∞±', '#', '#', 'ÁÉò', 'ÁÑô', '#', '#', 'Ëá™', 'Â∑±', 'Âä®', 'Êâã', 'Áæé', 'È£ü', '#', '#', 'Ëá™', 'Â∑±', 'Âä®', 'Êâã', '‰∏∞', 'Ë°£', 'Ë∂≥', 'È£ü', '#', '#', 'Â§è', 'Êó•', 'Áîú', 'Èªû', 'Ëá™', 'Â∑±', 'Âãï', 'Êâã', '‰Ωú', '#'], ['Êóó', 'Ë¢ç', '_', '216', 'Êóó', 'Ë¢ç', 'ÁöÑ', 'Âèë', 'Â±ï', 'Âè≤', '-', 'Âè§', 'ÂÖ∏', 'Êóó', 'Ë¢ç', '„ÄÇ', 'Êóó', 'Ë¢ç', ':', 'Áõ∏', 'Á∫¶', 'Ê∏©', 'Â©â', 'Ëç£', 'Âçé', '„ÄÇ', 'ËÆ©', 'Êó∂', 'ÂÖâ', 'Êää', 'Á≤ó', '[UNK]', 'Ë§™', 'Âéª', '„ÄÇ', '#', 'Êóó', 'Ë¢ç', '#', '#', 'Âè§', 'È£é', '#', '#', 'ÂÜô', 'Áúü', '#', '#', 'Âêé', 'Êúü', 'Âà∂', '‰Ωú', '#', '#', 'ÊëÑ', 'ÂΩ±', '#', '„ÄÇ', 'ËÆ©', 'Â≤Å', 'Êúà', 'Êää', 'Ê≤ß', 'Ê°ë', 'Ëûç', 'Âåñ', '„ÄÇ', 'Á©ø', 'Ëøá', '‰∏Ä', '‰∏™', '‰∏ñ', 'Á∫™', 'ÁöÑ', 'ÈÅ•', 'Ëøú', 'Ë∑ù', 'Á¶ª', '„ÄÇ', 'È•±', 'Âê´', 'Â§©', 'Áîü', 'ÁöÑ', '[UNK]', 'Â™ö', 'Á´Ø', 'Â∫Ñ', '„ÄÇ', 'Âíå', '‰Ω†', 'Áõ∏', 'Á∫¶', 'Ê∏©', 'Â©â', 'Ëç£', 'Âçé', '„ÄÇ', 'Á∫§', 'Â∞ò', 'Êäñ', 'ËêΩ', 'Âú®', 'ÂéÜ', 'Âè≤', 'Ëßí', 'ËêΩ', '‰∏≠', '„ÄÇ', 'ÂØÇ', 'Èùô', 'Âú∞', 'ËÆ©', 'Âá∫', 'ÂÖâ', 'ÂΩ©', 'ÁöÑ'], ['ÈÅì', 'Ë∑Ø', '„ÄÇ', 'ËÆ©', 'Êàë', 'ËÑâ', 'ËÑâ', 'Âê´', 'ÊÉÖ', 'ËÄå', 'Êù•', '„ÄÇ', 'Âú®', 'Áôæ', 'Ëä±', 'ÂºÄ', 'Êîæ', 'ÁöÑ', 'Êò•', 'Â§©', '„ÄÇ', 'Âú®', 'ÁÉ≠', 'ÊÉÖ', '‰ºº', 'ÁÅ´', 'ÁöÑ', 'Â§è', 'Êó•', '„ÄÇ', 'Âú®', 'ÊµÅ', 'Ê∞¥', '[UNK]', '[UNK]', 'ÁöÑ', 'Èáë', 'Áßã', '„ÄÇ', 'Âú®', 'ÁôΩ', 'Èõ™', 'È£ò', 'È£û', 'ÁöÑ', 'ÂÜ¨', 'Â≠£', '„ÄÇ', '‰∏Ä', 'Â¶Ç', 'Êó¢', 'ÂæÄ', '„ÄÇ', '‰∏é', '‰Ω†', 'Áõ∏', 'Á∫¶', 'Ê∏©', 'Â©â', 'Ëç£', 'Âçé', '„ÄÇ', '#', 'Êóó', 'Ë¢ç', '#', '#', 'Âè§', 'È£é', '#', '#', 'ÂÜô', 'Áúü', '#', '#', 'Âêé', 'Êúü', 'Âà∂', '‰Ωú', '#', '#', 'ÊëÑ', 'ÂΩ±', '#'], ['Êòü', 'Êòü', 'ÁÅØ', 'ÔºÉ', 'ÂΩì', 'Êàë', 'Áúã', 'ÁùÄ', '‰Ω†', 'ÁöÑ', 'Êó∂', 'ÂÄô', 'Áúº', 'Èáå', '‰πü', 'Êúâ', 'Êòü', 'Êòü', '[UNK]'], ['Ê≤°', 'Êúâ', 'ÈÇ£', 'Êää', 'Ââë', 'Ôºå', 'Êàë', 'ÁÖß', 'Ê†∑', 'ÂèØ', '‰ª•', 'Ê≠º', 'ÁÅ≠', 'Êïå', 'ÂÜõ'], ['Êãç', 'Âá∫', 'ÊôÆ', 'ÈÄö', '‰∫∫', 'ÊúÄ', 'Áæé', 'ÁöÑ', '‰∏Ä', 'Èù¢', 'ÔΩû'], ['Â∞è', 'Âßê', 'Âßê', 'Âèà', 'Âú®', 'Êãç', 'Êãç', 'Êãç', 'Âï¶', '[UNK]', '#', 'ÈÄÜ', 'Ë¢≠', 'Â∞è', '‰ªô', 'Â•≥', '#'], ['Á¨¨', '‰∏Ä', 'Ê¨°', 'ÂÅö', 'ÁöÑ', 'Âõæ', 'Ê°à', 'Âãâ', 'Âº∫', 'ËÉΩ', 'Áúã', '#', '520', 'Èô™', '‰Ω†', 'Êíí', 'Á≥ñ', '#'], ['#', 'Âú∞', 'Áã±', 'Â∞ë', 'Â•≥', '#', '#', 'Èòé', 'È≠î', 'Áà±', '#', '[UNK]'], ['Êú¨', '‰∫∫', 'Âõ†', 'Â∑•', '‰Ωú', 'Âéü', 'Âõ†', 'Ôºå', 'Áªè', 'Â∏∏', 'Âá∫', 'Â∑Æ', 'Ôºå', 'ÂÆ∂', 'Èáå', 'Êúâ', 'Âè™', 'Áãó', 'Áãó', '‰∏ç', 'ËÉΩ', 'ÁÖß', 'È°æ', 'Ôºå', 'ÊÅ≥', 'ËØ∑', 'Êúâ', 'Áà±', 'ÂøÉ', 'ÁöÑ', 'Â•≥', 'ÊÄß', 'Â±±', '‰∏ú', 'Ëèè', 'Ê≥Ω', 'Êú¨', 'Âú∞', 'Êúã', 'Âèã', 'ÂÖç', 'Ë¥π', 'È¢Ü', 'ÂÖª', ':', 'V', '##X', ':', 'j', '##sam', '##ine', '##12', '##12'], ['Âèë', '‰∏Ä', '‰∫õ', 'Ëâ∫', 'ÊúØ', 'Ê∞õ', 'Âõ¥', 'ÁöÑ', 'Âàõ', '‰Ωú', 'Ôºå', 'Áæé', 'Âõæ', '‰∏ä', '99', '%', 'ÊòØ', 'ÂçÉ', 'ÁØá', '‰∏Ä', 'Âæã', 'ÁöÑ', 'Á≥ñ', 'Ê∞¥', 'Áâá', 'Ôºå', '‰∏ç', 'ËÄê', 'Áúã', '„ÄÇ'], ['Èªë', 'Èæô', 'Ê±ü', 'ÊóÖ', 'Ê∏∏', 'Êîª', 'Áï•', '‰πã', '‰∫î', 'Â§ß', 'Ëøû', 'Ê±†', 'ÁÅ´', 'Â±±', 'Êé¢', 'Á¥¢', '‰πã', 'ÊóÖ', '„ÄÇ', '[UNK]', 'Ë°å', 'Á®ã', 'ËÆ°', 'Âàí', '„ÄÇ', 'Day', '##1', ':', '„ÄÇ', 'ÂÖ•', '‰Ωè', '‰∫î', 'Â§ß', 'Ëøû', 'Ê±†', 'È£é', 'ÊôØ', 'Âå∫', '‰∏á', 'Ë±™', 'Âêç', 'Ëãë', 'ÂïÜ', 'Âä°', 'ÈÖí', 'Â∫ó', '[UNK]', '[UNK]', 'Êôö', 'È§ê', 'Áüø', 'Ê≥â', 'ÂÜ∑', 'Ê∞¥', 'È±º', 'È±º', 'ÂÆ¥', '„ÄÇ', 'Day', '##2', ':', '„ÄÇ', 'Êñ∞', 'Êúü', 'ÁÅ´', 'Â±±', 'ËßÇ', 'ÂÖâ', 'Âå∫', 'ËÄÅ', 'Èªë', 'Â±±', '„ÄÅ', 'ÁÅ´', 'ÁÉß', 'Â±±', '[UNK]', '[UNK]', 'Âçà', 'È§ê', 'Áüø', 'Ê≥â', 'Ë±Ü', 'ËÖê', 'ÂÆ¥', '[UNK]', '[UNK]', 'Ê∏©', 'Ê≥ä', '[UNK]', '[UNK]', 'Ê∞¥', 'Êô∂', 'ÂÆ´', 'ÊôØ', 'Âå∫', '[UNK]', '[UNK]', 'Èæô', 'Èó®', 'Áü≥', 'ÂØ®', 'ÊôØ', 'Âå∫', '„ÄÇ', 'Day', '##3', 'Ôºö', '„ÄÇ', 'Âú£', 'Ê∞¥', 'Á•≠', 'ÂÖ∏', '[UNK]', '[UNK]', 'Ëä±'], ['ËΩ¶', 'Â∑°', 'Ê∏∏', '[UNK]', '[UNK]', 'Ê≥•', 'ÊµÜ', 'Â§ß', 'Êàò', '[UNK]', '[UNK]', 'ÁÅ´', 'Â±±', 'Ê∏©', 'Ê≥â', '‰Ωì', 'È™å', '[UNK]', '[UNK]', 'ÂºÄ', 'Âπï', 'Âºè', 'Âèä', 'Ê≠å', 'Ëàû', 'Êôö', '‰ºö', '[UNK]', '[UNK]', 'Áîµ', 'Èü≥', '‰πã', 'Â§ú', '„ÄÇ', 'Day', '##4', ':', '„ÄÇ', 'Ëøî', 'Á®ã', '„ÄÇ', '#', 'Èïø', 'È£é', 'ËÆ°', 'Âàí', '#', '@', 'MT', 'ÊÉÖ', 'Êä•', 'Â±Ä', '@', 'MT', 'Â∞è', 'Âä©', 'Êâã', '@', 'MT', 'Â±Ö', 'Âßî', '‰ºö', '@', 'MT', 'Áé©', 'Âõæ', 'Âêõ'], ['#', 'Â§©', 'Ê¥•', 'Êé¢', 'Â∫ó', '#', '#', 'Â§©', 'Ê¥•', 'Áæé', 'È£ü', '#', '„ÄÇ', 'Â∫ó', 'Âêç', ':', 'ÁæΩ', 'Ê∑±', '„ÄÇ', 'Âõæ', '‰∏Ä', 'ÁÑ¶', 'Á≥ñ', 'ÂÜ∞', 'Ê∑á', 'Ê∑ã', 'Êàë', 'Áà±', '‰∫Ü', '[UNK]', '„ÄÇ', 'È§ê', 'ÂéÖ', 'ÁéØ', 'Â¢É', '‰πü', 'Êå∫', 'Â•Ω', 'ÁöÑ', '„ÄÇ', 'Êé®', 'Ëçê', 'ÁÇπ', 'Áâõ', 'Êéí', '[UNK]', 'Ôºå', 'Ëøò', 'Êúâ', 'Áï™', 'ËåÑ', 'Áâõ', '[UNK]', '„ÄÇ', 'ÈúÄ', 'Ë¶Å', 'Êèê', 'Ââç', 'ËÆ¢', '‰∏Ä', '‰∏ã'], ['[UNK]', 'Ë∂Ö', 'Áæé', 'ËÉå', 'ÊôØ', 'Â¢ô', 'ÈÄÅ', 'Áªô', '‰Ω†', '[UNK]'], ['Èºì', 'Ê•º', 'Âå∫', 'ËÄÅ', 'Âπ¥', 'Â§ß', 'Â≠¶', 'Ê±á', 'Êä•', 'Êºî', 'Âá∫', 'ÂúÜ', 'Êª°', 'Áªì', 'Êùü'], ['[', 'Â•∏', 'Á¨ë', ']', '[', 'Â•∏', 'Á¨ë', ']', 'ËÖÆ', 'Á∫¢', 'Áªß', 'Áª≠', '„ÄÇ', '‰ª•', 'Ââç', 'Êàë', 'ÊÄª', 'Ëßâ', 'Âæó', 'ËÖÆ', 'Á∫¢', 'Áî®', '‰∏ç', 'Áî®', 'Êó†', 'ÊâÄ', 'Ë∞ì', '„ÄÇ', '‰ΩÜ', 'ÊòØ', 'Âêé', 'Êù•', 'Êàë', 'Âèë', 'Áé∞', 'Ôºå', 'Áúº', 'ÂΩ±', 'Ê≤°', 'Êó∂', 'Èó¥', 'ÁªÜ', 'Êèè', 'ÊÖ¢', 'Âåñ', '„ÄÇ', 'Áõ¥', 'Êé•', 'ËÖÆ', 'Á∫¢', '[UNK]', 'Âè£', 'Á∫¢', '„ÄÇ', 'Ê∞î', 'Ëâ≤', 'Áõ¥', 'Êé•', 'Áøª', 'ÂÄç', 'Ôºå', 'Âπ¥', 'ËΩª', '3', '-', '5', 'Â≤Å', '[', 'Êú∫', 'Êô∫', ']'], ['[UNK]', '[', 'Mi', '##cist', '##y', 'ÂØÜ', 'Ê±ê', '[UNK]', 'Ëø™', 'Êùü', 'ËÖ∞', 'Â∏¶', ']', '„ÄÇ', 'Ë∂ä', 'Áæé', 'ÁöÑ', '‰∫∫', 'ÂØπ', 'Ëá™', 'Â∑±', 'ÁöÑ', 'Ë¶Å', 'Ê±Ç', 'ÊÄª', 'ÊòØ', 'Ë∂ä', 'È´ò', '„ÄÇ', '‰∏ç', '‰ªÖ', 'ÂèØ', '‰ª•', 'ËæÖ', 'Âä©', 'ÁáÉ', 'ËÑÇ', 'Ê∏õ', 'Â∞è', 'ËÖ∞', 'Âõ¥', '„ÄÇ', 'Ëøê', 'Âä®', 'Âà∑', 'ËÑÇ', 'Ëøò', 'ÂèØ', '‰ª•', '‰øù', 'Ë≠∑', 'ËÖ∞', 'Ê§é', '„ÄÇ', 'Â°ë', 'ÂΩ¢', 'ÁöÑ', 'Âêå', 'ÊôÇ', 'Â∏Æ', '‰Ω†', 'ÁÆ°', 'Êéß', 'Âùê', 'Âßø', '„ÄÇ', 'Áºì', 'Ëß£', '‰∏ç', 'ËâØ', '‰π†', 'ÊÉØ', 'ÂØº', 'Ëá¥', 'ÁöÑ', 'ËÖ∞', 'ÈÉ®', 'Áñ≤', 'Âä≥', '„ÄÇ', 'Êúâ', 'Êïà', 'ÁöÑ', 'Áü´', 'Ê≠£', 'È©º', 'ËÉå', '‰ª•', 'Âèä', 'ËÑä', 'Ê§é', 'ÂºØ', 'Êõ≤', '„ÄÇ', '‰∏ç', '‰ªÖ', '‰ªÖ', 'ÊòØ', '‰∏Ä', '‰ª∂', 'Êùü', 'ËÖ∞', 'Âì¶', '„ÄÇ', '‰πÖ', 'Âùê', '‰∫∫', 'Áæ§', 'ÈÉΩ', '‰∏ç', '‰ºö', 'Èöæ', 'Âèó', 'ÁöÑ', '‰∏Ä', 'Ê¨æ', 'Êùü', 'ËÖ∞'], [], ['‰Ω†', 'ÂΩí', 'Êù•', 'ÊòØ', 'ËØó', 'Ôºå', 'Á¶ª', 'Âéª', 'Êàê', 'ËØç', 'Ôºå', '‰∏î', 'Á¨ë', 'È£é', 'Â∞ò', '‰∏ç', 'Êï¢', 'ÈÄ†', 'Ê¨°', '„ÄÇ'], ['Ëøô', 'Ê≥¢', 'Ââß', '‰πü', 'Â§™', 'Â§™', 'Â§™', 'Â§™', 'Â•Ω', 'Áúã', '‰∫Ü', 'Âêß', '„ÄÇ', 'Êúâ', '‰Ω†', '‰ª¨', 'Âñú', 'Ê¨¢', 'ÁöÑ', 'Âêó', '„ÄÇ', 'ÂÉè', 'Áúã', '‰ªÄ', '‰πà', 'Â£Å', 'Á∫∏', 'Êàñ', 'Áîµ', 'ËßÜ', 'Ââß', 'ËØÑ', 'ËÆ∫', 'Âå∫', 'Âëä', 'ËØâ', 'Êàë', '[UNK]', '#', 'ÂÆò', 'Êñπ', 'Â§ß', 'Â§ß', 'Êàë', 'Ë¶Å', '‰∏ä', 'ÁÉ≠', '#'], ['ÈÇ£', 'ÊòØ', 'Èõ®', 'Âêé', 'ÁöÑ', 'ÂÇç', 'Êôö', '[UNK]', 'Êàë', 'Êúâ', 'Âπ∏', 'ËßÅ', 'Âà∞', '‰∫Ü', 'Ëøô', 'Â§©', 'Ë±°', 'Ôºö', '‰∫ë', 'Ê¢Ö', 'ÂÉè', 'Áæé', '‰∏Ω', 'ÁöÑ', 'Ê¢Ö', 'Ëä±', 'Âú®', 'Â§©', '‰∏ä', 'È£û', 'Êù•', 'È£û', 'Âéª', 'Ôºå', 'Áû¨', 'Èó¥', 'Âèò', 'Âåñ', 'Êó†', 'Á©∑', 'Ôºå', 'Áæé', 'ËΩÆ', 'Áæé', '[UNK]', 'ÔºÅ'], ['#', 'Ê≤à', 'Èò≥', '‰∫å', 'Êâã', 'ËΩ¶', '#', '[UNK]', 'Êñ∞', 'Âà∞', '12', 'Âπ¥', 'Áé∞', '‰ª£', 'Á¥¢', 'Á∫≥', 'Â°î', 'ÂÖ´', 'Ôºå', 'Ëá™', 'Âä®', '2', '.', '0', 'ÊúÄ', 'È´ò', 'ÈÖç', 'Ôºå', 'Áúü', 'ÁöÆ', 'Â∫ß', 'Ê§Ö', 'Â§ß', 'Â§©', 'Á™ó', 'Ôºå', 'ÂØº', 'Ëà™', 'ÂÄí', 'ËΩ¶', 'ÂΩ±', 'ÂÉè', 'Ôºå', 'Â∑°', 'Ëà™', 'ÂÆö', 'ÈÄü', '„ÄÇ', '‰∏Ä', 'Êâã', 'ËΩ¶', 'Ôºå', 'ÂÖ®', 'ËΩ¶', 'Âéü', 'Áâà', '„ÄÇ', '8', '‰∏á', 'ÂÖ¨', 'Èáå', 'Ôºå', 'Ë¥π', 'Áî®', 'ÂÖ®', 'Âπ¥', 'Ôºå', 'È¶ñ', '‰ªò', '8000', 'ËΩ¶', 'Ê¨æ', '[UNK]'], ['Ê∏Ö', 'ÁêÜ', 'Á©∫', 'ÁΩÆ', 'Êàø', 'Èô¢', 'Â≠ê', 'Èáå', 'ÁöÑ', 'ÊùÇ', 'Ëçâ'], ['Â•Ω', 'Êúã', 'Âèã', 'ÁöÑ', 'Â•≥', 'ÂÑø', 'Á°Æ', 'ËØä', 'ÁôΩ', 'Ë°Ä', 'ÁóÖ', 'Ôºå', 'Êâç', '8', 'Â≤Å', 'Ôºå', '‰∏∫', '‰∫Ü', 'Èºì', 'Âä±', 'Â•π', 'Ôºå', 'Êàë', 'Âíå', 'Â•π', 'Áà∏', 'Áà∏', 'ÈÉΩ', 'ÂâÉ', '‰∫Ü', 'ÂÖâ', 'Â§¥', 'Ôºå', 'Èô™', 'Â•π', '‰∏Ä', 'Ëµ∑', 'Âä†', 'Ê≤π', 'ÔºÅ'], ['‰ªä', 'Êó•', '‰ªΩ', 'ËÉå', 'ÊôØ', 'Âõæ', 'Ôºö', '„ÄÇ', '[UNK]', 'Êûï', 'Â§¥', 'Âú∞', '‰∏ã', '‰∏Ä', 'È¢ó', 'Á≥ñ', 'Ôºå', 'ÂÅö', '‰∏Ä', '‰∏™', 'Áîú', 'Áîú', 'ÁöÑ', 'Ê¢¶', '.', '[UNK]', '„ÄÇ', '#', 'Êúâ', '‰∏™', 'ÊÅã', 'Áà±', 'ÊÉ≥', 'Âíå', '‰Ω†', 'Ë∞à', '#', '#', 'Êúã', 'Âèã', 'Âúà', 'ËÉå', 'ÊôØ', 'Âõæ', '#', '#', 'ins', 'È£é', '#', '#', 'Êó†', 'Ê∞¥', 'Âç∞', '#', '#', 'Â∞ë', 'Â•≥', 'ÂøÉ', '#'], ['Ëøô', '‰∏™', 'Áîª', 'È£é', 'Êàë', 'Áà±', '‰∫Ü', '(', '[UNK]', ')', '-', '‚ô°', '„ÄÇ', 't', '##wi', ':', 'sa', '##kus', '##ya', '##2', '##hon', '##da', '„ÄÇ', '@', 'MT', 'ÊÉÖ', 'Êä•', 'Â±Ä', '#', 'Âä®', 'Êº´', 'Âõæ', '#', '#', 'Â•Ω', 'Áúã', 'ÁöÑ', 'Âä®', 'Êº´', 'Âõæ', '#', '#', 'Â£Å', 'Á∫∏', '#'], ['Âëµ', 'Âëµ', '[UNK]', 'Ê∂à', 'Á£®', 'Êó∂', 'Èó¥', 'Áé©', '‰∏Ä', 'Áé©'], ['[UNK]', '‰Ω†', 'ÂøÖ', '‰π∞', 'ÁöÑ', '‰ºò', 'Ê¢µ', 'Ê∫∂', 'ËÑÇ', 'Èúú', '[UNK]', '[UNK]', '„ÄÇ', 'Âæà', 'Èöæ', 'Ëøá', 'Ë¶Å', 'ËÆ©', '‰Ω†', 'Âíå', '‰Ω†', 'Â§ö', 'Âπ¥', 'ÁöÑ', 'Â∞è', 'Á≤ó', 'ËÖø', 'ËØ¥', 'ÁôΩ', 'ÁôΩ', '‰∫Ü', '[UNK]', 'Èöè', 'Êó∂', 'Èöè', 'Âú∞', 'Êªö', '‰∏Ä', 'Êªö', '[UNK]', '‰∏Ä', '[UNK]', 'Âíå', 'Â§ß', 'Ë±°', 'ËÖø', 'Ê∞¥', 'Ê°∂', 'ËÖ∞', '‰∏ç', 'Â§ç', 'ÂÜç', 'Áõ∏', 'ËßÅ', 'ËÆ©', 'ÊÉÖ', 'Êïå', 'Áúã', 'ËßÅ', 'Ë¢´', 'Ê∞î', 'Ê≠ª', 'ËÆ©', 'Ââç', '‰ªª', 'Áúã', 'ËßÅ', '‰ºö', 'Âêé', 'ÊÇî', 'ÁöÑ', 'Áò¶', 'Ë∫´', 'Á•û', 'Âô®', 'ÂÖ®', 'Ë∫´', 'ÈÉΩ', 'ËÉΩ', 'Áò¶', 'Á¥ß', 'Ëá¥', 'Ê∫∂', 'ËÑÇ', 'Âéª', 'ËÇå', 'ËÇâ', 'ËÖø', 'Ëøò', 'ËÉΩ', 'Áæé', 'ÁôΩ', 'Êúâ', '‰∏Ä', 'ÁÇπ', 'Ëæ£', 'Ëæ£', 'ÁöÑ', '‰ΩÜ', 'ÊòØ', '‰∏ç', 'Âà∫', 'ÊøÄ', 'ÁöÑ', 'Êïè', 'ÊÑü', 'ËÇå', 'ÂèØ', 'Áî®'], ['ÁÇé', 'ÁÇé', 'Â§è', 'Êó•', 'Êù•', '‰∏Ä', 'ÊùØ', 'Âä†', 'ÂÜ∞', 'ÁöÑ', 'Ëãπ', 'Êûú', 'Ê±Å', 'Âè≠', '[UNK]'], ['Ê≤°', 'Êúâ', 'ÂÜ∞', 'Â•∂', 'Ëå∂', 'ÁöÑ', 'Â§è', 'Â§©', '‰∏ç', 'ÂÆå', 'Áæé', '.', '.', '.', '.', '.'], ['#', '‰π¶', 'Á±ç', 'ÂÆâ', 'Âà©', '#', '„Ää', '‰∏é', 'ËÄÅ', 'Â¶à', 'ÁöÑ', 'Êó•', 'Â∏∏', '„Äã', '„ÄÇ', 'Èáå', 'Èù¢', 'ÊòØ', '‰∏Ä', 'ÂÅè', 'ÂÅè', 'ÁöÑ', 'Â∞è', 'Êº´', 'Áîª', 'Ôºå', 'ÂÅè', 'ÂÅè', 'ÂÖ•', 'ÂøÉ', '[UNK]', '„ÄÇ', 'Èöè', 'Êâã', 'Êëò', 'ÊäÑ', '[UNK]', 'ÂàÜ', '‰∫´', '‰∏≠', '[UNK]', '[UNK]', '[UNK]'], ['#', 'Êâã', 'Áªò', '#', '‰∏Ä', 'Â§©', '‰∏Ä', 'Áîª', 'Ôºå', 'Ë¥µ', 'Âú®', 'Âùö', 'ÊåÅ', '[UNK]', 'Êúõ', 'Âèã', 'Âèã', '‰ª¨', 'ÁÇπ', 'Ëµû', 'ËØÑ', 'ËÆ∫', '[UNK]'], ['Èùí', 'Ëóè', 'Áâπ', '‰∫ß', 'Á∫¢', '[UNK]', 'Êùû', 'Ôºå', 'Èªë', '[UNK]', 'Êùû', 'Ôºå', 'Ëóè', 'Á∫¢', 'Ëä±', 'Ôºå', 'Á∫Ø', 'Â§©', 'ÁÑ∂', '‰∫ß', 'Âú∞', 'Ê≠£', 'ÂÆó', 'Ôºå', 'Êä¢', 'Ë¥≠', 'ÁÉ≠', 'Á∫ø', ':', '1383', '##47', '##0', '##86', '##11'], ['Áæé', 'Ë°£', 'È≠Ö', 'Âäõ', 'Ôºå', 'Ë∞Å', 'Á©ø', 'Ë∞Å', 'Áæé', '‰∏Ω', '[UNK]'], ['Áªô', 'Â§ß', 'ÂÆ∂', 'Êé®', 'Ëçê', '‰∏Ä', 'ÈÉ®', 'ÁΩë', 'Ââß', '„ÄÇ', 'Âè´', 'ÁÅµ', 'È≠Ç', 'ÊëÜ', 'Ê∏°', '‰∫∫', '„ÄÇ', 'Ëøô', 'ÈÉ®', 'ÁΩë', 'Áªú', 'Ââß', 'ÊÄª', 'ÂÖ±', 'Êúâ', '3', 'ÈÉ®', '„ÄÇ', 'Ëøô', '‰∏™', 'ÊòØ', '1', '-', '2', 'ÈõÜ', '‰∏Ä', '‰∏™', 'ÊïÖ', '‰∫ã', '„ÄÇ', 'Ë∑ü', 'È¨º', 'Áâá', 'ÊòØ', '‰∏Ä', 'Ê†∑', 'ÁöÑ', '„ÄÇ', 'Ëøô', 'ÈÉ®', 'ÁΩë', 'Áªú', 'Ââß', 'Âèç', 'Â∫î', '‰∫Ü', 'Áé∞', 'ÂÆû', '‰∏≠', 'Âæà', 'Â§ö', 'Â≠ò', 'Âú®', 'ÁöÑ', '‰∏ú', 'Ë•ø', '„ÄÇ', 'ËØ•', 'Ââß', 'ËÆ≤', 'Ëø∞', '‰∫Ü', 'Â§è', 'ÂÜ¨', 'Èùí', 'Âíå', 'È¨º', 'Â∑Æ', 'Ëµµ', 'Âêè', '„ÄÅ', '‰πù', 'Â§©', 'ÁéÑ', 'Â•≥', 'Áéã', 'Â∞è', '‰∫ö', '‰∏â', '‰∫∫', 'Âú®', 'Â§Ñ', 'ÁêÜ', '‰∏Ä', '‰ª∂', '‰ª∂', 'Êúâ', 'ÂÖ≥', 'È¨º', 'È≠Ç', 'Âíå', 'ÁÅµ', 'È≠Ç', 'ÁöÑ', 'ÊïÖ', '‰∫ã', '‰∏≠', 'ÊÑü', 'ÊÇü', '‰∫∫', 'Áîü', 'ÁöÑ', 'ÊïÖ', '‰∫ã', '„ÄÇ', 'Áúü', 'ÁöÑ', 'Ë∂Ö', 'Á∫ß', 'Â•Ω', 'Áúã', '„ÄÇ', 'Êàë', 'ÈÉΩ', 'Áúã', '‰∫Ü'], ['Â•Ω', 'Â§ö', 'ÈÅç', '‰∫Ü', '„ÄÇ', 'Áâπ', 'Âà´', 'Âñú', 'Ê¨¢', 'Èáå', 'Èù¢', 'ÊëÜ', 'Ê∏°', '‰∫∫', 'Ëµµ', 'Âêè', '„ÄÇ', 'Ëøò', 'Êúâ', 'Èáå', 'Èù¢', 'ÁöÑ', 'ÂÜ•', 'Áéã', 'Ëå∂', 'Ëå∂', '„ÄÇ', 'Áúü', 'ÁöÑ', 'Ë∂Ö', 'Á∫ß', 'Êé®', 'Ëçê', '„ÄÇ'], ['ÂåÖ', 'Ë£Ö', 'Â•Ω', 'Áúã', 'È¢ú', 'Ëâ≤', '‰πü', 'Â•Ω', 'Áúã', '„ÄÇ', 'Êòæ', 'Ëâ≤', '‰∏ç', 'È£û', 'Á≤â', 'Ôºå', 'Âìë', 'ÂÖâ', 'Áè†', 'ÂÖâ', 'ÂÖ±', '12', 'Ëâ≤', 'Ôºå', 'Êâì', 'ÈÄ†', 'ÂêÑ', 'Áßç', 'Â¶Ü', 'ÂÆπ', 'Ë∞Å', 'Áî®', 'Ë∞Å', 'Áà±', '#', 'Ê¨ß', 'Êùü', '#'], ['#', '‰Ω†', 'Â•Ω', 'Â§è', 'Â§©', '#', '„ÄÇ', '‰∏ä', 'Áè≠', 'È´ò', 'Â≥∞', '„ÄÇ', 'ËÉ°', 'Â≠ê', 'ÈÉΩ', 'Ê≤°', 'ÂàÆ', '[UNK]'], ['‰Ωô', 'ÂÖâ', '‰∏≠', 'ÂÖà', 'Áîü', 'ËØ¥', ':', '[UNK]', 'Êúà', 'Ëâ≤', '‰∏é', 'Èõ™', 'Ëâ≤', '‰πã', 'Èó¥', 'Ôºå', '‰Ω†', 'ÊòØ', 'Á¨¨', '‰∏â', 'Áßç', 'Áªù', 'Ëâ≤', '.', '[UNK]', '#', 'ËÉå', 'ÂΩ±', 'ÊùÄ', '#'], ['Êñ∞', 'Ê¨æ', 'ÁÇπ', 'ÁÇπ', 'Ë£Ö', 'ÔΩû', 'Ê∏Ö', 'Êñ∞', 'ÁöÑ', 'ÂÉè', 'Áæé', 'Â∞ë', 'Â•≥', 'ÔΩû'], ['Êàë', 'ÁöÑ', 'Âìà', 'ÂØÜ', 'Ê¥û', 'ÔºÅ', '[UNK]'], ['ËØ∑', 'ÂøΩ', 'Áï•', 'Á≤ó', 'Áï•', 'ÁöÑ', 'ËÉå', 'ÊôØ', 'ÔΩû', 'Ëøò', 'Êúâ', 'ËÉñ', 'ËÉñ', 'ÁöÑ', 'ËÖø', 'ËÖø', '(', '„ÄÉ', '[UNK]', ')', '„ÄÇ', 'Ëøô', '‰∏™', 'Èò≤', 'Êôí', 'Âñ∑', 'Èõæ', 'Áúü', 'ÁöÑ', 'Ë∂Ö', 'Á∫ß', 'Â•Ω', 'Áî®', 'ÔΩû', '‰∏Ä', 'ÁÇπ', '‰∏ç', 'Ê≤π', 'ÔΩû', 'Ë∂Ö', 'Á∫ß', 'Ëàí', 'Êúç', 'Ôºå', 'Âë≥', 'ÈÅì', '‰πü', 'Âæà', 'Â•Ω', 'Èóª', 'ÔΩû'], ['#', 'Â¶Ç', 'Êûú', 'Êúâ', '‰∏Ä', 'Â§©', 'Êàë', 'Ê∂à', 'Â§±', '‰∫Ü', '#', '‰Ω†', '‰ºö', 'ÊÄé', 'Ê†∑'], ['‰æÑ', 'Â≠ô', 'Â•≥', 'ÂÑø', 'ÂÖ∑', 'ÁÑ∂', 'Âíå', 'Êàë', '‰∏Ä', 'Â§©', 'Áîü', 'Êó•', 'ÔºÅ', 'ÂèØ', 'Âñú', 'ÂèØ', 'Ë¥∫', 'ÔºÅ', 'Âêå', 'Âñú', 'Âêå', '‰πê', 'Âïä', 'ÔºÅ', '[UNK]'], ['Âêå', '‰∫ã', 'Áõ∏', 'ËÅö', 'Ôºå', 'Áæé', 'È£ü', 'Â∞ë', '‰∏ç', '‰∫Ü', '„ÄÇ'], ['[UNK]'], ['Ë∂Ö', 'Á∫ß', 'ÁÉ≠', 'ÁöÑ', 'Â§©', 'Ê∞î', 'Âñù', 'ÊùØ', 'ÂÜ∞', 'Ëå∂', 'Ëß£', 'Êöë', 'Âï¶', 'Âï¶', 'Âï¶', '[UNK]', '^', 'V', '^'], ['Áæé', 'ÊúØ', 'Êúü', 'Êú´', 'ËÄÉ', 'ËØï', 'over', '##ÔΩû', '„ÄÇ', 'ÊÑü', 'Ëßâ', 'Ëá™', 'Â∑±', 'Âè™', 'Êúâ', 'b', '+', '[UNK]', '#', 'Áæé', 'ÊúØ', '#', '#', 'Êúü', 'Êú´', '#', '#', 'ËÄÉ', 'ËØï', '#', '#', 'È´ò', '‰∏Ä', '#', '#', 'Èïø', 'È£é', 'ËÆ°', 'Âàí', '#']]
    tuple1, tuple2 = zip(
            *[tokenize_list_with_cand_indexes(w, max_length, tokenizer) for w in lword if w])


def compare_time_tokenize():
    vocab_file = '../src/BERT/models/multi_cased_L-12_H-768_A-12/vocab.txt'
    do_lower_case = True
    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)

    infile = '../Test_cases/bad_cases.txt'
    with open(infile, 'r', encoding='utf8') as f:
        raw_data = f.readlines()

    tx = [x for x in raw_data if x.strip()]
    st = time.time()
    t1 = [tokenizer.tokenize(t) if len(tokenizer.tokenize(t))>0 else ['[UNK]'] for t in tx]
    print(time.time()-st)

    print(t1)


if __name__ == '__main__':
    #test_BertCRF_constructor()
    #test_BasicTokenizer()
    #test_pandas_drop()
    #test_pandas_drop_syn()
    #test_metrics()
    #test_CWS_Dict()

    #test_pkuseg()
    #test_FullTokenizer()
    #check_english('candidate defence')
    #check_english('Âè∞Âåócandidate defence')

    #test_parse_one2BERTformat()

    #test_load_model()

    #test_dataloader()

    #test_decode()
    #test_split()

    #test_write()

    #test_construct_pos_tags()

    #test_outputPOSFscoreUsedBIO()
    #check_time()

    #test_restore_unknown()
    #test_tokenize_list_with_cand_indexes()

    compare_time_tokenize()



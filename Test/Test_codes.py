#!/anaconda3/envs/haiqin370/bin/ python3
# -*- coding: utf-8 -*-
"""
Created on at 11:21 2019-01-30 
@author: haiqinyang

Feature: 

Scenario: 
"""
from sklearn.preprocessing import LabelEncoder
import sys
sys.path.append('../src')
from src.utilis import save_model
from src.config import args
from src.utilis import get_dataset_and_dataloader, get_ontonotes_eval_dataloaders
from src.preprocess import CWS_BMEO
from tqdm import tqdm

def test_BertCRF_constructor():
    from src.BERT.modeling import BertCRF
    from collections import namedtuple

    test_input_args = namedtuple("test_input_args", "bert_model cache_dir")
    test_input_args.bert_model = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese.tar.gz'
    test_input_args.cache_dir = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/'

    model = BertCRF(test_input_args, 4)


def test_BasicTokenizer():
    from src.tokenization import BasicTokenizer
    # prove processing English and Chinese characters
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    text = 'beautyä¸€ç™¾åˆ†\n Beauty ä¸€ç™¾åˆ†!!'
    print(basic_tokenizer.tokenize(text))


def test_FullTokenizer():
    from src.tokenization import FullTokenizer, BasicTokenizer

    vocab_file = '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/vocab.txt'
    full_tokenizer = FullTokenizer(vocab_file, do_lower_case=True)

    text = 'ä»»å¤©å ‚æ¸¸æˆå•†åº—çš„åŠ å…¥è¢«ä¸šç•Œè§†ä¸ºandroidçš„é©å‘½ã€‚'
    print(full_tokenizer.tokenize(text))

    text = 'å°æ¹¾!!. æ¯”èµ›ã€‚ä»Šå¤©ï¼Œå¼€å§‹å—ï¼Ÿ  ï¼Ÿï¼Ÿï¼å’³å’³ï¿£ ï¿£)Ïƒç¬¬ä¸€æ¬¡ç©¿æ±‰æœå‡ºé—¨ğŸ€ğŸ’å¼€å¿ƒLaughing'
    print(full_tokenizer.tokenize(text))

    text = 'å°æ¹¾çš„å…¬è§†ä»Šå¤©ä¸»åŠçš„å°åŒ—å¸‚é•¿ Candidate  Defence  ï¼Œ'
    print(full_tokenizer.tokenize(text))
    #['å°', 'æ¹¾', 'çš„', 'å…¬', 'è§†', 'ä»Š', 'å¤©', 'ä¸»', 'åŠ', 'çš„', 'å°', 'åŒ—', 'å¸‚', 'é•¿', 'can', '##di', '##da', '##te', 'de', '##fe', '##nce', 'ï¼Œ']

    text = 'Candidate'
    print(full_tokenizer.tokenize(text))
    # ['can', '##di', '##da', '##te']

    text = '  Defence  ï¼Œ'
    print(full_tokenizer.tokenize(text))

    text1 = '''æ¤ç‰©ç ”ç©¶æ‰€æ‰€é•·å‘¨æ˜Œå¼˜å…ˆç”Ÿç•¶é¸ç¬¬ä¸‰ä¸–ç•Œç§‘å­¸é™¢ï¼ˆï¼´ï½ˆï½…ã€€ï¼´ï½ˆï½‰ï½’ï½„ã€€ï¼·ï½ï½’ï½Œï½„ã€€ï¼¡ï½ƒï½ï½„ï½…ï½ï½™ã€€ï½ï½†ã€€ï¼³ï½ƒï½‰ï½…ï½ï½ƒï½…ï½“ï¼Œç°¡ç¨±ï¼´ï¼·ï¼¡ï¼³ï¼‰
    é™¢å£«ã€‚ï¼´ï¼·ï¼¡ï¼³ä¿‚ä¸€ä¹å…«ä¸‰å¹´ç”±ï¼°ï½’ï½ï½†ã€€ï¼¡ï½„ï½‚ï½•ï½“ã€€ï¼³ï½ï½Œï½ï½ï¼ˆå·´åŸºæ–¯å¦ç±ï¼Œæ›¾ç²è«¾è²çˆ¾çï¼‰ç™¼èµ·æˆç«‹ï¼Œæœƒå“¡éä½ˆï¼–ï¼“å€‹åœ‹å®¶ï¼Œç›®å‰ç”±ï¼’ï¼“ï¼’ä½é™¢å£«
    ï¼ˆï¼¦ï½…ï½Œï½Œï½ï½—åŠï¼¦ï½ï½•ï½ï½„ï½‰ï½ï½‡ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰ï¼Œï¼–ï¼–ä½å”é™¢å£«ï¼ˆï¼¡ï½“ï½“ï½ï½ƒï½‰ï½ï½”ï½…ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰ï¼’ï¼”ä½é€šä¿¡é™¢å£«ï¼ˆï¼£ï½ï½’ï½’ï½…ï½“ï½ï½ï½ï½„ï½‰ï½ï½‡ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰
    ã€€åŠï¼’ä½é€šä¿¡å”é™¢å£«ï¼ˆï¼£ï½ï½’ï½’ï½…ï½“ï½ï½ï½ï½„ï½‰ï½ï½‡ã€€ï¼¡ï½“ï½“ï½ï½ƒï½‰ï½ï½”ï½…ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ï¼‰çµ„æˆï¼ˆä¸åŒ…æ‹¬ä¸€ä¹ä¹å››å¹´ç•¶é¸è€…ï¼‰ï¼Œææ”¿é“ã€æ¥ŠæŒ¯å¯§ã€ä¸è‚‡ä¸­ã€
    æé å“²ã€é™³çœèº«ã€å³å¥é›„ã€æœ±ç¶“æ­¦ã€è”¡å—æµ·ç­‰é™¢å£«å‡ç‚ºè©²é™¢ï¼¡ï½“ï½“ï½ï½ƒï½‰ï½ï½”ï½…ã€€ï¼¦ï½…ï½Œï½Œï½ï½—ã€‚æœ¬é™¢æ•¸ç†çµ„é™¢å£«ã€å“ˆä½›å¤§å­¸æ•¸å­¸ç³»æ•™æˆä¸˜æˆæ¡ï¼Œç¶“ç‘å…¸çš‡å®¶ç§‘å­¸é™¢è©•å®šç‚º
    ä¸€ä¹ä¹å››å¹´å…‹åˆ—ä½›ï¼ˆï¼£ï½’ï½ï½†ï½ï½ã€€ï¼°ï½’ï½‰ï½šï½…ï¼‰çå¾—ä¸»ï¼Œè—‰ä»¥è¡¨å½°å…¶åœ¨å¾®åˆ†å¹¾ä½•é ˜åŸŸå½±éŸ¿æ·±é ä¹‹è²¢ç»ã€‚'''
    print(full_tokenizer.tokenize(text1))

    text = 'ï¼°ï½’ï½ï½†ã€€ï¼¡ï½„ï½‚ï½•ï½“ã€€ï¼³ï½ï½Œï½ï½'
    print(full_tokenizer.tokenize(text))

    text = ' ï¼°ï½’ï½ï½†ã€€ï¼¡ï½„ï½‚ï½•ï½“ã€€ï¼³ï½ï½Œï½ï½ '
    print(full_tokenizer.tokenize(text))

    text = '( ï¼°ï½’ï½ï½†ã€€ ï¼¡ï½„ï½‚ï½•ï½“ã€€ ï¼³ï½ï½Œï½ï½  ) '
    print(full_tokenizer.tokenize(text))

    text = 'ï¼–ï¼“å€‹åœ‹å®¶'
    print(full_tokenizer.tokenize(text))

    text = 'ï¼–ï¼“ å€‹åœ‹å®¶'
    print(full_tokenizer.tokenize(text))

    text = 'ï¼’ï¼”ï¼’ï¼”ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = 'ï¼’ï¼”ï¼’ï¼” ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = '2424 ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = '2424ä½é€šä¿¡é™¢å£«'
    print(full_tokenizer.tokenize(text))

    text = 'ä¸‹ä¸€æ³¢DVDåŠç”¨äºå¯æºå¼å°å‹èµ„è®¯ç”¨å“çš„å¾®å‹å…‰ç¢Ÿï¼ˆMinidiskï¼‰ï¼Œä¹Ÿå·²è¿«ä¸åŠå¾…åœ°ç­‰ç€æ•²å¼€æ¶ˆè´¹è€…çš„è·åŒ…ã€‚'
    print(full_tokenizer.tokenize(text))

    test_text = 'ç¬¬ä¸‰ä¸–ç•Œç§‘å­¸é™¢ï¼ˆï¼´ï½ˆï½…ã€€ï¼´ï½ˆï½‰ï½’ï½„ã€€ï¼·ï½ï½’ï½Œï½„ã€€ï¼¡ï½ƒï½ï½„ï½…ï½ï½™ã€€ï½ï½†ã€€ï¼³ï½ƒï½‰ï½…ï½ï½ƒï½…ï½“ï¼Œç°¡ç¨±ï¼´ï¼·ï¼¡ï¼³ï¼‰'
    basic_tokenizer = BasicTokenizer(do_lower_case=True)
    sep_tokens = basic_tokenizer.tokenize(test_text)
    print('Basic:')
    for tt in sep_tokens:
        if basic_tokenizer._is_chinese_char(ord(tt[0])):
            wt = 'C'
        elif basic_tokenizer._is_english_char(ord(tt[0])):
            wt = 'E'
        else:
            wt = 'O'

        print(tt+' '+wt)

def check_english(w):
    import re

    english_check = re.compile(r'[a-z]')

    if english_check.match(w):
        print("english", w)
    else:
        print("other:", w)

def test_pandas_drop():
    import pandas as pd
    import os

    data_dir = '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data'
    df = pd.read_csv(os.path.join(data_dir, "test_code.tsv"), sep='\t')

    # full_pos (chunk), ner, seg, text
    df.drop(['full_pos', 'ner'], axis=1)

    # change name to tag for consistently processing
    df.rename(columns={'seg': 'label'}, inplace=True)

    print(len(df.full_pos))

def test_pandas_drop_syn():
    import pandas as pd
    import numpy as np

    df = pd.DataFrame(np.arange(12).reshape(3,4), columns=['A', 'B', 'C', 'D'])
    print(df)

    # need parameter inplace=True
    df.drop(['B', 'C'], axis=1, inplace=True)
    print(df)

def test_metrics():
    from src.metrics import get_ner_BMES, get_ner_BIO, get_ner_fmeasure, reverse_style
    label_list_BMES = "O O O O O O O O O O O O B-PER E-PER O O O O O O O O O O O O O O O O O O O O O O O"
    label_list_BIO = "O B-PER I-PER O O O B-PER O O B-ORG I-ORG I-ORG"

    label_list_BMES = label_list_BMES.split()
    #get_ner_BMES(label_list_BMES)

    #label_list_BIO = label_list_BIO.split()
    ll_BIO1 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG']
    ll_BIO2 = ['O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']

    print(get_ner_BIO(ll_BIO1))
    print(get_ner_BIO(ll_BIO2))

def test_CWS_Dict():
    from src.utilis import CWS_Dict
    cws_dict = CWS_Dict()

    sent = 'è¿ˆå‘  å……æ»¡  å¸Œæœ›  çš„  æ–°  ä¸–çºª  â€”â€”  ä¸€ä¹ä¹å…«å¹´  æ–°å¹´  è®²è¯  ï¼ˆ  é™„  å›¾ç‰‡  ï¼‘  å¼   ï¼‰\n  ' \
           'ä¸­å…±ä¸­å¤®  æ€»ä¹¦è®°  ã€  å›½å®¶  ä¸»å¸­  æ±Ÿ  æ³½æ°‘\n' \
           'ï¼ˆ  ä¸€ä¹ä¹ä¸ƒå¹´  åäºŒæœˆ  ä¸‰åä¸€æ—¥  ï¼‰ \n ' \
           'ï¼‘ï¼’æœˆ  ï¼“ï¼‘æ—¥  ï¼Œ  ä¸­å…±ä¸­å¤®  æ€»ä¹¦è®°  ã€  å›½å®¶  ä¸»å¸­  æ±Ÿ  æ³½æ°‘  å‘è¡¨  ï¼‘ï¼™ï¼™ï¼˜å¹´  æ–°å¹´  è®²è¯  ' \
           'ã€Š  è¿ˆå‘  å……æ»¡  å¸Œæœ›  çš„  æ–°  ä¸–çºª  ã€‹  ã€‚  ï¼ˆ  æ–°åç¤¾  è®°è€…  å…°  çº¢å…‰  æ‘„  ï¼‰\n' \
           'åŒèƒ  ä»¬  ã€  æœ‹å‹  ä»¬  ã€  å¥³å£«  ä»¬  ã€  å…ˆç”Ÿ  ä»¬  ï¼š\n ' \
           'åœ¨  ï¼‘ï¼™ï¼™ï¼˜å¹´  æ¥ä¸´  ä¹‹é™…  ï¼Œ  æˆ‘  ååˆ†  é«˜å…´  åœ°  é€šè¿‡  ä¸­å¤®  äººæ°‘  å¹¿æ’­  ç”µå°  ã€' \
           '  ä¸­å›½  å›½é™…  å¹¿æ’­  ç”µå°  å’Œ  ä¸­å¤®  ç”µè§†å°  ï¼Œ  å‘  å…¨å›½  å„æ—  äººæ°‘  ï¼Œ  å‘  é¦™æ¸¯  ç‰¹åˆ«  è¡Œæ”¿åŒº  ' \
           'åŒèƒ  ã€  æ¾³é—¨  å’Œ  å°æ¹¾  åŒèƒ  ã€  æµ·å¤–  ä¾¨èƒ  ï¼Œ  å‘  ä¸–ç•Œ  å„å›½  çš„  æœ‹å‹  ä»¬  ï¼Œ  è‡´ä»¥  è¯šæŒš  çš„  ' \
           'é—®å€™  å’Œ  è‰¯å¥½  çš„  ç¥æ„¿  ï¼  '

    q_num = cws_dict._findNum(sent)
    print(list(q_num.queue))

    sent = 'äººæ‚¬æŒ‚åœ¨åŠç©ºä¸­å­¤ç«‹æ— æ´ï¼Œè€Œä»–çš„è„šä¸‹å°±æ˜¯ä¸‡ä¸ˆæ·±æ¸Šã€‚\n' \
           'ä½†ç”±äºæ‹…å¿ƒå·èµ·çš„æ°”æµªä¼šæŠŠé©¬ä¿®è¿äººå¸¦ä¼å¹è½æ‚¬å´–ï¼Œç›´å‡æœºæ— æ³•ç›´æ¥å®æ–½ç©ºä¸­æ•‘æ´ã€‚\n' \
           'æ•‘æ´äººå‘˜åªèƒ½å€ŸåŠ©ç›´å‡æœºç™»ä¸Šæ‚¬å´–é¡¶ç«¯ï¼Œå°†ç»³ç´¢æ‰”ç»™é©¬ä¿®è¿›è¡Œè¥æ•‘ã€‚\n' \
           'åœ¨ç»å†äº†ä¸€ç•ªå‘¨æŠ˜ä¹‹åï¼Œé©¬ä¿®ç»ˆäºè¢«æ•‘æ´äººå‘˜æ‹‰ä¸Šäº†æ‚¬å´–ï¼Œ' \
           'å¹¸è¿çš„æ˜¯ç”±äºè¥æ•‘åŠæ—¶ï¼Œé©¬ä¿®æœ¬äººå¹¶æ— å¤§ç¢ã€‚ä¸­å¤®å°ç¼–è¯‘æŠ¥é“ã€‚\n' \
           'å¥½ï¼Œè¿™æ¬¡ä¸­å›½æ–°é—»èŠ‚ç›®å°±åˆ°è¿™ï¼Œæˆ‘æ˜¯å¾ä¿ï¼Œè°¢è°¢å¤§å®¶ã€‚\n' \
           'æ¥ä¸‹æ¥æ˜¯ç”±ç‹ä¸–æ—ä¸»æŒçš„ä»Šæ—¥å…³æ³¨èŠ‚ç›®ã€‚\n' \
           'å„ä½è§‚ä¼—ï¼Œå†è§ã€‚\n' \
           ' ï¼¥ï¼­ï¼°ï¼´ï¼¹'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))

    # some problems are here
    sent = 'ï¼¥ï¼­ï¼°ï¼´ï¼¹'
    q_eng = cws_dict._findEng(sent)
    print(list(q_eng.queue))


def test_pkuseg():
    from src.metrics import getChunks, getFscore
    tag_list = ['BBIBBIIBIIIB', 'BBBBIBBBIIIB']
    #tag_list = ['S,BI,S,BII,BIII,S,S,S,S,BI,S']

    tmp_list = [','.join(tag_list[i]) for i in range(len(tag_list))]
    print(getChunks(tmp_list)) # ['B*1*0,B*2*1,B*1*3,B*3*4,B*4*7,B*1*11,', 'B*1*0,B*1*1,B*1*2,B*2*3,B*1*5,B*1*6,B*4*7,B*1*11,']


    tag_to_idx = {'B': 0, 'I': 1, 'O': 2}
    idx_to_chunk_tag = {}

    '''
    for tag, idx in tag_to_idx.items():
        if tag.startswith("I"):
            tag = "I"
        if tag.startswith("O"):
            tag = "O"
        idx_to_chunk_tag[idx] = tag
    '''

    idx_to_chunk_tag = {}
    tag_to_idx = {'B': 0, 'M': 1, 'E': 2, 'S': 3, '[START]': 4, '[END]': 5}
    BIO_tag_to_idx = {'B': 0, 'I': 1, 'O': 2, '[START]': 3, '[END]': 4}
    token_list = [''.join(str(BIO_tag_to_idx[item])+',') for i in range(len(tag_list)) for item in tag_list[i] ]

    idx_to_chunk_tag = idx_to_tag(BIO_tag_to_idx)

    token_list = []
    for i in range(len(tag_list)):
        t = ''
        for item in tag_list[i]:
            t += ''.join(str(BIO_tag_to_idx[item])+',')
        token_list.append(t)


    goldTagList = [token_list[0]]
    resTagList = [token_list[1]]

    scoreList, infoList = getFscore(goldTagList, resTagList, idx_to_chunk_tag)
    # ValueError: invalid literal for int() with base 10: 'B'
    print(scoreList)
    print(infoList)


def calSize(H, vs, mpe, L):
    for l in L:
        # embedding: (vs+mpe)*H; # Query, Key, value: 3*H*H; Intermediate: 4*H *H; Pooler: H*H
        sz = (vs+mpe)*H + ((3+4)*H*H)*l + H*H
        print('# layer: '+str(l)+', #para: '+str(sz))


def verifyModelSize():
    H = 768
    vs = 21128
    mpe = 512
    L = [3, 6, 12]

    num_model_para = calSize(H, vs, mpe, L)

'''
def test_parse_one2BERTformat():
    from OntoNotes.f6_generate_training_data import parse_one2BERT2Dict
    s = '(NP (CP (IP (NP (DNP (NER-GPE (NR Taiwan)) (DEG çš„)) (NER-ORG (NR å…¬è§†))) (VP (NT ä»Šå¤©) (VV ä¸»åŠ))) (DEC çš„)) (NP-m (NP (NR å°åŒ—) (NN å¸‚é•¿)) (NP-m (NP (NN candidate) (NN defence)) (PU ï¼Œ))))'
    out_dict = parse_one2BERT2Dict(s)
    print('src_seg:'+out_dict['src_seg'])
    print('src_ner:'+out_dict['src_ner'])
    print('full_pos:'+out_dict['full_pos'])
    print('text:'+out_dict['text'])
    print('text_seg:'+out_dict['text_seg'])
    print('bert_seg:'+out_dict['bert_seg'])
    print('bert_ner:'+out_dict['bert_ner'])
'''

def set_local_eval_param():
    return {'task_name': 'ontonotes_CWS',
            'model_type': 'sequencelabeling',
            'data_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/4ner_data/',
            #'bert_model_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/final_data/eval/2019_3_12/models/',
            'vocab_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/vocab.txt',
            'bert_config_file': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/bert_config.json',
            'output_dir': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/rs/nhl3/',
            'do_lower_case': True,
            'train_batch_size': 128,
            'max_seq_length': 64,
            'num_hidden_layers': 3,
            'init_checkpoint': '/Users/haiqinyang/Downloads/codes/pytorch-pretrained-BERT-master/models/bert-base-chinese/',
            'bert_model': '/Users/haiqinyang/Downloads/datasets/ontonotes-release-5.0/ontonote_data/proc_data/eval/2019_3_12/models/nhl3/weights_epoch03.pt',
            'override_output': True,
            'tensorboardWriter': False
            }


def test_load_model():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    label_list = ['B', 'M', 'E', 'S', '[START]', '[END]']
    model, device = load_model(label_list, args)

    save_model(model, args.output_dir + 'tmp.tsv')


def test_dataloader():
    kwargs = set_local_eval_param()
    args._parse(kwargs)

    processors = {
        "ontonotes_cws": lambda: CWS_BMEO(nopunc=args.nopunc),
    }

    task_name = args.task_name.lower()

    # Prepare model
    processor = processors[task_name]()
    train_dataset, train_dataloader = get_dataset_and_dataloader(processor, args, training=False, type = 'tmp_test')

    eval_dataloaders = get_ontonotes_eval_dataloaders(processor, args)

    for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
        input_ids, segment_ids, input_mask = batch[:3]
        label_ids = batch[3:] if len(batch[3:])>1 else batch[3]



if __name__ == '__main__':
    #test_BertCRF_constructor()
    #test_BasicTokenizer()
    #test_pandas_drop()
    #test_pandas_drop_syn()
    #test_metrics()
    #test_CWS_Dict()

    #test_pkuseg()
    test_FullTokenizer()
    #check_english('candidate defence')
    #check_english('å°åŒ—candidate defence')

    #test_parse_one2BERTformat()

    #test_load_model()

    #test_dataloader()
